[
    {
        "url": "https://www.nationalgeographic.com/science/article/are-we-living-inside-black-hole-universe",
        "text": "Mathematical quirks of our universe have led some cosmologists to wonder whether the cosmos was actually born in a black hole. Gazing up at the stars at night, it’s easy to imagine that space goes on forever. But cosmologists know that the universe actually has limits. First, their best models indicate that space and time had a beginning, a subatomic point called a singularity. This point of intense heat and density rapidly ballooned outward during the big bang. And second, the visible universe is circumscribed by what’s known as an event horizon, a precipice beyond which nothing can be observed because the cosmos has expanded faster than the speed of light, leaving parts of it too far for even our best telescopes to see. These two elements—a singularity and an event horizon—are also important features of black holes. Such gravitational monsters lurk all over the cosmos, gobbling up gas, dust and light. Like the universe, black holes are restricted by their own event horizons, a boundary past which nothing can be observed, that is believed to contain a singularity. Perhaps that’s why a couple of recent scientific papers have suggested that our entire universe could exist inside of a black hole.   While it rests somewhat outside the realm of ordinary cosmology, the mind-bending possibility of living in a black hole isn’t just for contemplative college stoners up too late. “It's certainly a reasonable idea,” says Niayesh Afshordi, an astrophysicist at the Perimeter Institute for Theoretical Physics in Waterloo, Canada. “It’s just making the details actually work.”  (How a stranger’s kindness during WWII helped give us the big bang theory.) The mathematics underpinning our understanding of the universe are quite similar to those describing black holes. Both stem from Albert Einstein’s theory of general relativity. That’s the idea that objects in space create curves in the fabric of space-time that govern their movements and explain how gravity works. Coincidentally, the radius of the observable universe happens to be the same as it would for a black hole with the mass of our cosmos.  Over the years, that has led some researchers to raise the possibility that the universe is inside of a black hole. Two of the first to work out the particulars were theoretical physicist Raj Kumar Pathria and, concurrently, mathematician I. J. Good in the 1970s. About 20 years later, physicist Lee Smolin took things a step further with a theory that every black hole that forms in our universe produces a new universe inside of it with slightly different physics than ours. Thus, universes bud from one another, mutating and ‘evolving’ as they create daughter universes. He called it cosmological natural selection. (What is the multiverse—and is there any evidence it really exists?) Though none of these ideas have gone particularly mainstream, many physicists still recognize the conceptual connection between black holes and the universe. “Mathematically, they're very related,” says Ghazal Geshnizjani, a theoretical physicist also at the Perimeter Institute. “They are kind of like the opposite of each other.”  Our cosmos is believed to have begun with a singularity, that point of infinite density that preceded the big bang. Black holes, by contrast, end in a singularity, an itty-bitty garbage disposal dot where everything is crushed beyond meaning. A black hole’s event horizon—the spherical borderline around the singularity—is also its point of no return. While often seen as cosmic vacuum cleaners in pop culture, black holes are actually relatively placid objects. A spaceship can enter a stable orbit around one and then escape unless, that is, it slips past the event horizon, beyond which nothing can ever come back. Our universe’s constant expansion drives a similar phenomenon. When we look out into the universe with telescopes, we see further objects moving away from us faster than closer ones. At large enough distances, the expansion happens quicker than the speed of light, shuffling stars and galaxies away so swiftly that they disappear beyond an edge, the cosmic horizon. It’s almost like those stars and galaxies are disappearing down the maw of an inside out black hole. Whew. Is your head hurting yet? Don’t worry. The main point for scientists is that these superficial connections between black holes and the universe don’t necessarily imply that one is the other. In order to take that leap, physicists would need to know what observable results such an idea might have. “We have theories and they have consequences,” says Alex Lupsasca, a physicist at Vanderbilt University in Nashville, Tennessee. “If the implications of the theory are ruled out by an experiment, then we could say the assumptions are inconsistent or wrong.” So what would be the observable consequences if our universe was, in fact, inside of a black hole? For one, there would be a kind of natural direction or orientation to the cosmos—galaxies spinning in a favored direction or a subtle axis in the leftover heat from the big bang that fills the universe. “You would expect some sort of gradient across our universe,” says Afshordi. “One direction would be towards the center of the black hole, another towards the outside of the black hole.” Yet our best measurements show that, at the largest scales, the cosmos appears to be quite repetitive. Physicists refer to this as the cosmological principle. It states that the universe has no special direction, and it’s the same pretty much everywhere. How such uniformity could arise from the birth of a black hole is a challenge for anyone claiming the cosmos is inside of one. Black holes are born from dying stars—a process that’s messy, chaotic, and far from uniform.   There is also the problem of the black hole’s singularity. That infinitesimal point is a fated final moment for anyone or anything that falls inside a black hole, kind of the opposite of a rapidly expanding cosmos. (You’ve heard of the big bang. But what is the big crunch?) Getting a better handle on these issues would require physicists to figure out how to combine the two most successful theories of the 20th century: general relativity, which applies to the largest objects, and quantum mechanics, which governs the smallest. Since a singularity is a microscopic point with enormous mass, it cannot be handled by either theory alone and requires some kind of synthesis between the two. Despite many efforts, such a theory of quantum gravity has so far eluded scientists. For the same reason, we can’t determine what precisely happens inside a black hole or before the big bang. Nevertheless, cosmologists agree that exploring some of these avenues is both a fascinating exercise and could potentially lead to new discoveries. Perhaps they might even find reason to reconsider their cosmic models and find that the universe really is inside a black hole. Copyright © 1996-2015 National Geographic SocietyCopyright © 2015-2025 National Geographic Partners, LLC. All rights reserved",
        "onlytext": "Mathematical quirks of our universe have led some cosmologists to wonder whether the cosmos was actually born in a black hole. Gazing up at the stars at night, it’s easy to imagine that space goes on forever. But cosmologists know that the universe actually has limits. First, their best models indicate that space and time had a beginning, a subatomic point called a singularity. This point of intense heat and density rapidly ballooned outward during the big bang. And second, the visible universe is circumscribed by what’s known as an event horizon, a precipice beyond which nothing can be observed because the cosmos has expanded faster than the speed of light, leaving parts of it too far for even our best telescopes to see.\n\nThese two elements—a singularity and an event horizon—are also important features of black holes. Such gravitational monsters lurk all over the cosmos, gobbling up gas, dust and light. Like the universe, black holes are restricted by their own event horizons, a boundary past which nothing can be observed, that is believed to contain a singularity. Perhaps that’s why a couple of recent scientific papers have suggested that our entire universe could exist inside of a black hole.\n\nWhile it rests somewhat outside the realm of ordinary cosmology, the mind-bending possibility of living in a black hole isn’t just for contemplative college stoners up too late. “It's certainly a reasonable idea,” says Niayesh Afshordi, an astrophysicist at the Perimeter Institute for Theoretical Physics in Waterloo, Canada. “It’s just making the details actually work.” \n\nThe mathematics underpinning our understanding of the universe are quite similar to those describing black holes. Both stem from Albert Einstein’s theory of general relativity. That’s the idea that objects in space create curves in the fabric of space-time that govern their movements and explain how gravity works. Coincidentally, the radius of the observable universe happens to be the same as it would for a black hole with the mass of our cosmos.\n\nOver the years, that has led some researchers to raise the possibility that the universe is inside of a black hole. Two of the first to work out the particulars were theoretical physicist Raj Kumar Pathria and, concurrently, mathematician I. J. Good in the 1970s. About 20 years later, physicist Lee Smolin took things a step further with a theory that every black hole that forms in our universe produces a new universe inside of it with slightly different physics than ours. Thus, universes bud from one another, mutating and ‘evolving’ as they create daughter universes. He called it cosmological natural selection.\n\nThough none of these ideas have gone particularly mainstream, many physicists still recognize the conceptual connection between black holes and the universe. “Mathematically, they're very related,” says Ghazal Geshnizjani, a theoretical physicist also at the Perimeter Institute. “They are kind of like the opposite of each other.” \n\nOur cosmos is believed to have begun with a singularity, that point of infinite density that preceded the big bang. Black holes, by contrast, end in a singularity, an itty-bitty garbage disposal dot where everything is crushed beyond meaning. A black hole’s event horizon—the spherical borderline around the singularity—is also its point of no return. While often seen as cosmic vacuum cleaners in pop culture, black holes are actually relatively placid objects. A spaceship can enter a stable orbit around one and then escape unless, that is, it slips past the event horizon, beyond which nothing can ever come back.\n\nOur universe’s constant expansion drives a similar phenomenon. When we look out into the universe with telescopes, we see further objects moving away from us faster than closer ones. At large enough distances, the expansion happens quicker than the speed of light, shuffling stars and galaxies away so swiftly that they disappear beyond an edge, the cosmic horizon. It’s almost like those stars and galaxies are disappearing down the maw of an inside out black hole.\n\nThe main point for scientists is that these superficial connections between black holes and the universe don’t necessarily imply that one is the other. In order to take that leap, physicists would need to know what observable results such an idea might have. “We have theories and they have consequences,” says Alex Lupsasca, a physicist at Vanderbilt University in Nashville, Tennessee. “If the implications of the theory are ruled out by an experiment, then we could say the assumptions are inconsistent or wrong.”\n\nSo what would be the observable consequences if our universe was, in fact, inside of a black hole? For one, there would be a kind of natural direction or orientation to the cosmos—galaxies spinning in a favored direction or a subtle axis in the leftover heat from the big bang that fills the universe. “You would expect some sort of gradient across our universe,” says Afshordi. “One direction would be towards the center of the black hole, another towards the outside of the black hole.” Yet our best measurements show that, at the largest scales, the cosmos appears to be quite repetitive. Physicists refer to this as the cosmological principle. It states that the universe has no special direction, and it’s the same pretty much everywhere.\n\nHow such uniformity could arise from the birth of a black hole is a challenge for anyone claiming the cosmos is inside of one. Black holes are born from dying stars—a process that’s messy, chaotic, and far from uniform.\n\nThere is also the problem of the black hole’s singularity. That infinitesimal point is a fated final moment for anyone or anything that falls inside a black hole, kind of the opposite of a rapidly expanding cosmos. Getting a better handle on these issues would require physicists to figure out how to combine the two most successful theories of the 20th century: general relativity, which applies to the largest objects, and quantum mechanics, which governs the smallest. Since a singularity is a microscopic point with enormous mass, it cannot be handled by either theory alone and requires some kind of synthesis between the two. Despite many efforts, such a theory of quantum gravity has so far eluded scientists. For the same reason, we can’t determine what precisely happens inside a black hole or before the big bang. Nevertheless, cosmologists agree that exploring some of these avenues is both a fascinating exercise and could potentially lead to new discoveries. Perhaps they might even find reason to reconsider their cosmic models and find that the universe really is inside a black hole."
    },
    {
        "url": "https://www.nationalgeographic.com/premium/article/supermassive-black-hole-was-formed-when-the-universe-was-a-toddler",
        "text": "A recently spotted black hole existed about 570 million years after the big bang—and may help us understand the evolution of the universe. A giant black hole, roughly nine million times the mass of the sun, may have been discovered from the early ages of the universe. Detected by the James Webb Space Telescope (JWST), it had already formed in the center of a galaxy only about 570 million years after the big bang. The discovery—and others like it that researchers say are likely to emerge soon—may one day help solve the mystery of how these behemoths could have emerged so early in cosmic history. (Every Black Hole Contains Another Universe?) Over the decades, scientists have detected two kinds of black holes: stellar-mass black holes and supermassive black holes. Stellar-mass black holes are typically five to 10 times the sun's mass and are thought to arise when giant stars die and collapse in on themselves, then exploding in supernovae and leaving black holes behind. Supermassive black holes, on the other hand, are millions to billions of times the sun's mass and form the hearts of most, if not all, large galaxies. (Mini Black Holes Zip Through Earth Every Day?) One might naturally assume that supermassive black holes were born from stellar-mass black holes that ate and ate until they grew huge. However, astronomers have detected black holes that are more than one billion times the mass of the sun from less than one billion years after the big bang, says Steven Finkelstein, an astrophysicist at the University of Texas at Austin. It remains an enigma how they could have possibly bloated to such enormous sizes in such a brief time. (Black Hole Hosts Universe's Most Massive Water Cloud) \"This is one of the major problems in modern astronomy,\" says Masafusa Onoue, an astronomer at the Kavli Institute for Astronomy and Astrophysics at Peking University in Beijing. To solve this puzzle, scientists peer at light from long ago in the universe's 13.7-billion-year history. After traveling for so long, dust can obscure much of this light, making these early black holes extraordinarily faint. The newfound black hole, found within a galaxy dubbed CEERS 1019, was discovered as part of a deep galaxy survey known as the Cosmic Evolution Early Release Science (CEERS). Along with two other early supermassive black holes, the object was recently reported in a paper reported accepted for publication in the Astrophysical Journal Letters. As light travels from distant galaxies to Earth, it slowly gets distorted to longer wavelengths and shifted toward the infrared part of the spectrum. The CEERS survey analyzed data from JWST’s highly sensitive infrared instruments to spot ancient black holes too dim for prior telescopes to see. The astronomers detected CEERS 1019's black hole by looking for extraordinarily bright galactic cores. Previous research has suggested that these so-called active galactic nuclei (AGN) are likely black holes that unleash vast amounts of energy as matter falls or accretes onto them. Given how rare scientists had thought early black holes were, \"it was truly a surprise to me that the first-year JWST observations discovered a number of active black holes … within the first billion years of the universe,\" says Onoue, who did not take part in the new research. (Scientists record a black hole collision they weren’t sure was possible.) CEERS 1019's black hole may not only be the earliest one detected so far, but also the smallest yet identified in the early universe. Most black holes spotted from around this era are more than one billion times the sun's mass. In contrast, CEERS 1019's black hole is more like the black hole at the center of our Milky Way galaxy, which is only a few million times the sun's mass. \"It was thought that lower-mass black holes had to exist in early galaxies, but I didn't think we could find them with these observations,\" says Dale Kocevski, a CEERS team member and an astrophysicist at Colby College in Waterville, Maine. \"The most surprising thing for me is that JWST has proven to be more sensitive than we could have hoped for.\" The CEERS survey also detected two other black holes that existed one billion and 1.1 billion years after the big bang, within the galaxies CEERS 746 and CEERS 2782, respectively. These are also relative lightweights at about 10 million solar masses. CEERS 2782's black hole was relatively easy to spot, as there was no dust obscuring JWST's view of it, while the bright accretion disk encircling CEERS 746's black hole was partially clouded by dust, suggesting it might lie within a galaxy that is also furiously pumping out stars, Kocevski explains. In CEERS 1019, the team detected extremely fast-moving gas, which they believe can only come from the region surrounding a supermassive black hole. \"The signature of fast-moving gas we see isn't extremely strong. We did extensive simulations to quantify our uncertainties, and arrived at a 95 percent confidence that we detect this feature,\" he says. \"It will be quite easy to increase the significance of this signal with future observations.\" Some scientists suspect the supermassive black holes in the early universe grew from the mergers of smaller constituent parts, and these newfound objects fit that prediction. Still, CEERS 1019's black hole is big enough already to prove difficult to explain. One possibility is that the seeds of supermassive black holes were born from the collapse of enormous gas clouds, 10,000 to one million times the mass of the sun, that existed before galaxies did, says Finkelstein, who led the CEERS survey. Another is that they were born from the supernovae of giant stars, he says, ones up to roughly 100 times the sun's mass, and then grew at surprisingly fast rates. \"Either case tells us something we didn't know before,\" Kocevski says. \"If the former is true, we've learned something about what came before galaxies. If the latter is correct, it tells us there's something we still don't understand about how black holes grow.\" These two models predict different numbers of black holes that should have existed within the first 600 million years after the big bang, says Zoltan Haiman, an astrophysicist at Columbia University in New York, who did not take part in this research. Discovering more could help solve the puzzle. “What is exciting is that there are certainly more such objects out there to be found, and both increasing the sample size and pushing to earlier times will allow us to hone in on which of these theories are correct,” Finkelstein says. Other hints of early black holes have begun to crop up as well. One team may have identified an AGN in the galaxy UHZ1 dating back about 450 million years after the big bang, while the JWST Advanced Deep Extragalactic Survey (JADES) may have discovered an active galactic nucleus within the galaxy GN-z11 dating to roughly 430 million years after the big bang, says Kevin Hainline, an astronomer at the University of Arizona in Tucson and a member of the JADES team. \"These records won't stand for very long. Which is good!\" Hainline says. \"We're only seeing the beginning of what is a great wealth of actively growing supermassive black holes in the early universe.\" CEERS also unexpectedly identified 11 remote candidate galaxies that existed when the universe was 470 million to 675 million years old; scientists had theorized that JWST would detect fewer galaxies at such distances. The newfound galaxies are rapidly forming stars and may shed light on galactic evolution throughout cosmic history, the researchers say. Intriguingly, the early supermassive black holes that prior work spotted all outshone their host galaxies. In contrast, these newfound lightweights were discovered in previously detected galaxies, Haiman says. \"This means that for some reason the situation is reversed—the small black holes are fainter than their host galaxies, whereas by the time they evolve into the bigger ones, they become much brighter than their hosts,\" Haiman says. \"This requires that the host galaxies grow slower than the black holes and gives us new understanding into the evolution of these objects.\" Copyright © 1996-2015 National Geographic SocietyCopyright © 2015-2025 National Geographic Partners, LLC. All rights reserved",
        "onlytext": "A recently spotted black hole existed about 570 million years after the big bang—and may help us understand the evolution of the universe. A giant black hole, roughly nine million times the mass of the sun, may have been discovered from the early ages of the universe. Detected by the James Webb Space Telescope (JWST), it had already formed in the center of a galaxy only about 570 million years after the big bang. The discovery—and others like it that researchers say are likely to emerge soon—may one day help solve the mystery of how these behemoths could have emerged so early in cosmic history.\n\nOver the decades, scientists have detected two kinds of black holes: stellar-mass black holes and supermassive black holes. Stellar-mass black holes are typically five to 10 times the sun's mass and are thought to arise when giant stars die and collapse in on themselves, then exploding in supernovae and leaving black holes behind. Supermassive black holes, on the other hand, are millions to billions of times the sun's mass and form the hearts of most, if not all, large galaxies.\n\nOne might naturally assume that supermassive black holes were born from stellar-mass black holes that ate and ate until they grew huge. However, astronomers have detected black holes that are more than one billion times the mass of the sun from less than one billion years after the big bang, says Steven Finkelstein, an astrophysicist at the University of Texas at Austin. It remains an enigma how they could have possibly bloated to such enormous sizes in such a brief time.\n\n\"This is one of the major problems in modern astronomy,\" says Masafusa Onoue, an astronomer at the Kavli Institute for Astronomy and Astrophysics at Peking University in Beijing. To solve this puzzle, scientists peer at light from long ago in the universe's 13.7-billion-year history. After traveling for so long, dust can obscure much of this light, making these early black holes extraordinarily faint.\n\nThe newfound black hole, found within a galaxy dubbed CEERS 1019, was discovered as part of a deep galaxy survey known as the Cosmic Evolution Early Release Science (CEERS). Along with two other early supermassive black holes, the object was recently reported in a paper reported accepted for publication in the Astrophysical Journal Letters. As light travels from distant galaxies to Earth, it slowly gets distorted to longer wavelengths and shifted toward the infrared part of the spectrum. The CEERS survey analyzed data from JWST’s highly sensitive infrared instruments to spot ancient black holes too dim for prior telescopes to see.\n\nThe astronomers detected CEERS 1019's black hole by looking for extraordinarily bright galactic cores. Previous research has suggested that these so-called active galactic nuclei (AGN) are likely black holes that unleash vast amounts of energy as matter falls or accretes onto them. Given how rare scientists had thought early black holes were, \"it was truly a surprise to me that the first-year JWST observations discovered a number of active black holes … within the first billion years of the universe,\" says Onoue, who did not take part in the new research.\n\nCEERS 1019's black hole may not only be the earliest one detected so far, but also the smallest yet identified in the early universe. Most black holes spotted from around this era are more than one billion times the sun's mass. In contrast, CEERS 1019's black hole is more like the black hole at the center of our Milky Way galaxy, which is only a few million times the sun's mass. \"It was thought that lower-mass black holes had to exist in early galaxies, but I didn't think we could find them with these observations,\" says Dale Kocevski, a CEERS team member and an astrophysicist at Colby College in Waterville, Maine. \"The most surprising thing for me is that JWST has proven to be more sensitive than we could have hoped for.\"\n\nThe CEERS survey also detected two other black holes that existed one billion and 1.1 billion years after the big bang, within the galaxies CEERS 746 and CEERS 2782, respectively. These are also relative lightweights at about 10 million solar masses. CEERS 2782's black hole was relatively easy to spot, as there was no dust obscuring JWST's view of it, while the bright accretion disk encircling CEERS 746's black hole was partially clouded by dust, suggesting it might lie within a galaxy that is also furiously pumping out stars, Kocevski explains.\n\nIn CEERS 1019, the team detected extremely fast-moving gas, which they believe can only come from the region surrounding a supermassive black hole. \"The signature of fast-moving gas we see isn't extremely strong. We did extensive simulations to quantify our uncertainties, and arrived at a 95 percent confidence that we detect this feature,\" he says. \"It will be quite easy to increase the significance of this signal with future observations.\"\n\nSome scientists suspect the supermassive black holes in the early universe grew from the mergers of smaller constituent parts, and these newfound objects fit that prediction. Still, CEERS 1019's black hole is big enough already to prove difficult to explain. One possibility is that the seeds of supermassive black holes were born from the collapse of enormous gas clouds, 10,000 to one million times the mass of the sun, that existed before galaxies did, says Finkelstein, who led the CEERS survey. Another is that they were born from the supernovae of giant stars, he says, ones up to roughly 100 times the sun's mass, and then grew at surprisingly fast rates. \"Either case tells us something we didn't know before,\" Kocevski says. \"If the former is true, we've learned something about what came before galaxies. If the latter is correct, it tells us there's something we still don't understand about how black holes grow.\"\n\nThese two models predict different numbers of black holes that should have existed within the first 600 million years after the big bang, says Zoltan Haiman, an astrophysicist at Columbia University in New York, who did not take part in this research. Discovering more could help solve the puzzle. “What is exciting is that there are certainly more such objects out there to be found, and both increasing the sample size and pushing to earlier times will allow us to hone in on which of these theories are correct,” Finkelstein says.\n\nOther hints of early black holes have begun to crop up as well. One team may have identified an AGN in the galaxy UHZ1 dating back about 450 million years after the big bang, while the JWST Advanced Deep Extragalactic Survey (JADES) may have discovered an active galactic nucleus within the galaxy GN-z11 dating to roughly 430 million years after the big bang, says Kevin Hainline, an astronomer at the University of Arizona in Tucson and a member of the JADES team. \"These records won't stand for very long. Which is good!\" Hainline says. \"We're only seeing the beginning of what is a great wealth of actively growing supermassive black holes in the early universe.\"\n\nCEERS also unexpectedly identified 11 remote candidate galaxies that existed when the universe was 470 million to 675 million years old; scientists had theorized that JWST would detect fewer galaxies at such distances. The newfound galaxies are rapidly forming stars and may shed light on galactic evolution throughout cosmic history, the researchers say.\n\nIntriguingly, the early supermassive black holes that prior work spotted all outshone their host galaxies. In contrast, these newfound lightweights were discovered in previously detected galaxies, Haiman says. \"This means that for some reason the situation is reversed—the small black holes are fainter than their host galaxies, whereas by the time they evolve into the bigger ones, they become much brighter than their hosts,\" Haiman says. \"This requires that the host galaxies grow slower than the black holes and gives us new understanding into the evolution of these objects.\""
    },
    {
        "url": "https://www.nationalgeographic.com/premium/article/are-new-morning-sickness-drugs-on-the-way",
        "text": "The debilitating, sometimes fatal condition called hyperemesis gravidarum has received scant funding and little acknowledgment, but new research may soon yield drugs to treat it. During her first pregnancy, Marlena Fejzo experienced nausea and vomiting so severe it landed her in the emergency room twice before giving birth. But her second pregnancy “was so much worse. I didn’t even think it could be worse, but it was,” recalls Fejzo, who is now a women’s health researcher at the Keck School of Medicine of USC. During the second pregnancy, Fejzo was given IV fluids, seven different medications, and placed on a feeding tube. Nothing worked. At points she was so weak that she couldn’t speak, was bedridden, and needed round-the-clock care. Fejzo’s doctor told her he thought she was just trying to get attention from her husband. At 15 weeks, she miscarried. Fejzo suffered from hyperemesis gravidarum (HG), a condition experienced by around 2 percent of pregnant individuals and characterized by severe, persistent nausea and vomiting that can be life-threatening. Despite that, HG research is consistently underfunded and those experiencing it are often dismissed. Fejzo’s miscarriage was in 1999. Shortly after, she returned to her postdoctoral researcher position at UCLA motivated to learn everything she could about HG. Last month, Fejzo and her colleagues published breakthrough work on how the hormone GDF15 impacts a mother’s risk of developing HG. The work could lead to several effective treatments whose availability, some researchers say, feels imminent. But lack of awareness and acknowledgment of the severity of HG could stand in the way. Morning sickness is an unpleasant pregnancy experience, but when HG—a far more extreme condition—is lumped together with morning sickness, the women suffering from it feel gaslit, says Kimber Wakefield MacGibbon, one of the study authors and the co-founder and executive director of the Hyperemesis Education and Research (HER) Foundation. HG feels like food poisoning, but with a very important difference: vomiting does not lead to relief. “It’s a continuous feeling that something's in your stomach that shouldn't be there,\" says MacGibbon, a registered nurse who experienced HG in both of her pregnancies. Dehydration and weight loss are common symptoms of HG, but the most severe cases can lead to miscarriage and conditions in the mother such as Wernicke’s encephalopathy, a neurological disorder caused by vitamin B1(thiamine) deficiency that can be fatal. A number of studies have shown that babies born to mothers with HG are at increased risk for preterm birth, low birth weight, and neurodevelopmental disorders including speech and language delay. “It really is a dangerous exposure in pregnancy, and it should be considered that,” says Fejzo. “Unfortunately, it just isn’t.” First-line treatments for HG, including anti-vomiting and nausea medications, are not effective for many women, says physician Jone Trovik, a professor in the department of clinical science at the University of Bergen who was not involved in Fejzo and MacGibbon’s study. And, even if a patient is given intravenous fluids to help relieve dehydration and electrolyte depletion or—in the most dire circumstances—is hooked up to a feeding tube, they may still need to terminate their pregnancy to survive. “As a doctor I feel very incompetent when I do not manage to help these women avoid termination in an otherwise wanted pregnancy,” says Trovik. Despite its severity, HG is overlooked, even by the medical community. Obstetrics and gynecology physician and HER Foundation medical advisor Aimee Brecht-Doscher will never forget the American College of Obstetricians and Gynecologists annual meeting she attended alongside thousands of other physicians in 2017, where only two presentations were given about HG, one by Fejzo. And as Brecht-Doscher and a few other attendees sat discussing the neglected condition, a male physician joined the conversation and announced: ‘I know what causes hyperemesis: it's hysteria.’ “And if you believe that,” says Brecht-Doscher, “then you don't believe that you really need to do anything to treat people.” Brecht-Doscher, who also was not involved in Fejzo and MacGibbon’s study, suffered from HG in two pregnancies, one of which led to a miscarriage. “The knee jerk reaction as a physician—especially to women who don't respond to standard therapies—is to assume that there's a psychological component and that's why they're not responding,” she says. “And I had learned that bias myself as a physician prior to having hyperemesis.” Brecht-Doscher says that, once she got HG, “I realized there was really nothing I could do to make myself better.” Following her miscarriage, one of the first things Fejzo did was create an online survey to get a sense of HG’s prevalence and the variables that influenced it. She was shocked by how many responses she received, including one from MacGibbon, who Fejzo remembers writing, “after I’m done with this pregnancy, I’m going to make a website on hyperemesis because there’s nothing out there.” What began as a website became the HER Foundation in 2002, a non-profit that collaborates with universities and research studies; offers support to families; and provides resources on HG to patients and providers—such as information on medications and management strategies. MacGibbon says she has spoken to around 10,000 families across the globe since she launched the foundation. Fejzo’s survey was soon posted on the HER website and, with that data, she, MacGibbon, and colleagues showed that HG was likely heritable. Fejzo then applied for NIH funding to study which gene(s) may be responsible but was denied. In 2010, her brother gave her a 23andMe genetic testing kit as a birthday gift. In addition to providing genetic information, 23andMe customers have the option of filling out health surveys. Fejzo had an idea. “I contacted them and asked them if they could include questions about hyperemesis, which they did.” In 2018, using genetic and health survey data from 23andMe participants, Fejzo, MacGibbon, and colleagues were the first to show a link between hyperemesis and a hormone called GDF15. GDF15 levels were already known to increase in the first two trimesters of pregnancy and to be a driver of cachexia, a wasting syndrome often seen in cancer patients. Around the same time, studies showed that GDF15 binds to cells in the brain stem, a structure responsible for basic functions like breathing and consciousness as well as vomiting, which reinforced its likely role in HG. But Fejzo was still perplexed as to why some people have HG in one pregnancy and not another. In the recent study, Fejzo and colleagues discovered that the majority of the GDF15 hormone comes from the baby, not the mother, and the amount produced can change from one pregnancy to the next, depending on the genetics of the baby, which is why mothers don’t always experience HG with all pregnancies. In addition, a mother’s level of nausea and vomiting during pregnancy is determined by her sensitivity to GDF15. The researchers found that women who produce below average amounts of GDF15 before becoming pregnant are at higher risk for developing HG because they are hypersensitive to the typical rise of the GDF15 protein in early pregnancy. By comparison, women who produce high levels of GDF15 before becoming pregnant report very little nausea or vomiting. To test the hypothesis that sensitivity to GDF15 influences risk of HG, researchers exposed mice to either a small dose of GDF15 followed by a high dose of GDF15—comparable to levels in women with HG—or to only a single high dose of GDF15. Mice given only one high dose began eating less and lost weight; by contrast, the mice given a small dose of GDF15 first, and thereby desensitized, were not impacted when given the larger dose. Brecht-Doscher believes these findings will soon lead to treatments. But, she says, there is still valid concern about giving drugs to pregnant women. “There's a lot of history there, with other medications that were used specifically for nausea and pregnancy that did cause harm.” One of those was thalidomide which, in the early 1960s, was found to cause severe limb deformities in the children of mothers who took it to relieve nausea during pregnancy. But Fejzo and others are optimistic because drugs that appear promising are already being tested, albeit for other conditions. Fejzo is hoping to evaluate drugs that increase GDF15 levels prior to pregnancy, preventing HG, as well as drugs that decrease GDF15 during pregnancy, further staving off or mitigating symptoms. Fejzo is currently applying for a grant to test the diabetes drug metformin, which increases levels of GDF15 in the blood and is already used to increase fertility in patients with polycystic ovary syndrome (PCOS) and in some cases of gestational diabetes. There are also GDF15 blocking drugs in clinical trials for cancer patients with cachexia. Fejzo hopes that, once those drugs are shown to be safe in those trials as well as in pregnant animal models, they can be tested in pregnant women as well. On January 9, San Francisco biotech company NGM Bio announced that they are talking with the FDA about beginning clinical trials in HG patients with their GDF15-blocking drug, NGM120. Fejzo will serve as an advisor to NGM Bio in the process. Copyright © 1996-2015 National Geographic SocietyCopyright © 2015-2025 National Geographic Partners, LLC. All rights reserved",
        "onlytext": "During her first pregnancy, Marlena Fejzo experienced nausea and vomiting so severe it landed her in the emergency room twice before giving birth. But her second pregnancy “was so much worse. I didn’t even think it could be worse, but it was,” recalls Fejzo, who is now a women’s health researcher at the Keck School of Medicine of USC. During the second pregnancy, Fejzo was given IV fluids, seven different medications, and placed on a feeding tube. Nothing worked. At points she was so weak that she couldn’t speak, was bedridden, and needed round-the-clock care. Fejzo’s doctor told her he thought she was just trying to get attention from her husband. At 15 weeks, she miscarried.\n\nFejzo suffered from hyperemesis gravidarum (HG), a condition experienced by around 2 percent of pregnant individuals and characterized by severe, persistent nausea and vomiting that can be life-threatening. Despite that, HG research is consistently underfunded and those experiencing it are often dismissed.\n\nFejzo’s miscarriage was in 1999. Shortly after, she returned to her postdoctoral researcher position at UCLA motivated to learn everything she could about HG.\n\nLast month, Fejzo and her colleagues published breakthrough work on how the hormone GDF15 impacts a mother’s risk of developing HG. The work could lead to several effective treatments whose availability, some researchers say, feels imminent. But lack of awareness and acknowledgment of the severity of HG could stand in the way.\n\nMorning sickness is an unpleasant pregnancy experience, but when HG—a far more extreme condition—is lumped together with morning sickness, the women suffering from it feel gaslit, says Kimber Wakefield MacGibbon, one of the study authors and the co-founder and executive director of the Hyperemesis Education and Research (HER) Foundation.\n\nHG feels like food poisoning, but with a very important difference: vomiting does not lead to relief. “It’s a continuous feeling that something's in your stomach that shouldn't be there,\" says MacGibbon, a registered nurse who experienced HG in both of her pregnancies.\n\nDehydration and weight loss are common symptoms of HG, but the most severe cases can lead to miscarriage and conditions in the mother such as Wernicke’s encephalopathy, a neurological disorder caused by vitamin B1(thiamine) deficiency that can be fatal. A number of studies have shown that babies born to mothers with HG are at increased risk for preterm birth, low birth weight, and neurodevelopmental disorders including speech and language delay.\n\n“Unfortunately, it just isn’t.” First-line treatments for HG, including anti-vomiting and nausea medications, are not effective for many women, says physician Jone Trovik, a professor in the department of clinical science at the University of Bergen who was not involved in Fejzo and MacGibbon’s study.\n\nAnd, even if a patient is given intravenous fluids to help relieve dehydration and electrolyte depletion or—in the most dire circumstances—is hooked up to a feeding tube, they may still need to terminate their pregnancy to survive.\n\nDespite its severity, HG is overlooked, even by the medical community. Obstetrics and gynecology physician and HER Foundation medical advisor Aimee Brecht-Doscher will never forget the American College of Obstetricians and Gynecologists annual meeting she attended alongside thousands of other physicians in 2017, where only two presentations were given about HG, one by Fejzo.\n\nAnd as Brecht-Doscher and a few other attendees sat discussing the neglected condition, a male physician joined the conversation and announced: ‘I know what causes hyperemesis: it's hysteria.’\n\nThe knee jerk reaction as a physician—especially to women who don't respond to standard therapies—is to assume that there's a psychological component and that's why they're not responding,” she says. “And I had learned that bias myself as a physician prior to having hyperemesis.”\n\nBrecht-Doscher says that, once she got HG, “I realized there was really nothing I could do to make myself better.”\n\nFollowing her miscarriage, one of the first things Fejzo did was create an online survey to get a sense of HG’s prevalence and the variables that influenced it.\n\nFejzo then applied for NIH funding to study which gene(s) may be responsible but was denied. In 2010, her brother gave her a 23andMe genetic testing kit as a birthday gift.\n\nIn 2018, using genetic and health survey data from 23andMe participants, Fejzo, MacGibbon, and colleagues were the first to show a link between hyperemesis and a hormone called GDF15. GDF15 levels were already known to increase in the first two trimesters of pregnancy and to be a driver of cachexia, a wasting syndrome often seen in cancer patients.\n\nAround the same time, studies showed that GDF15 binds to cells in the brain stem, a structure responsible for basic functions like breathing and consciousness as well as vomiting, which reinforced its likely role in HG.\n\nBut Fejzo was still perplexed as to why some people have HG in one pregnancy and not another. In the recent study, Fejzo and colleagues discovered that the majority of the GDF15 hormone comes from the baby, not the mother, and the amount produced can change from one pregnancy to the next, depending on the genetics of the baby, which is why mothers don’t always experience HG with all pregnancies.\n\nIn addition, a mother’s level of nausea and vomiting during pregnancy is determined by her sensitivity to GDF15. The researchers found that women who produce below average amounts of GDF15 before becoming pregnant are at higher risk for developing HG because they are hypersensitive to the typical rise of the GDF15 protein in early pregnancy.\n\nBy comparison, women who produce high levels of GDF15 before becoming pregnant report very little nausea or vomiting.\n\nTo test the hypothesis that sensitivity to GDF15 influences risk of HG, researchers exposed mice to either a small dose of GDF15 followed by a high dose of GDF15—comparable to levels in women with HG—or to only a single high dose of GDF15.\n\nBrecht-Doscher believes these findings will soon lead to treatments. But, she says, there is still valid concern about giving drugs to pregnant women.\n\nBut Fejzo and others are optimistic because drugs that appear promising are already being tested, albeit for other conditions.\n\nFejzo is hoping to evaluate drugs that increase GDF15 levels prior to pregnancy, preventing HG, as well as drugs that decrease GDF15 during pregnancy, further staving off or mitigating symptoms.\n\nFejzo is currently applying for a grant to test the diabetes drug metformin, which increases levels of GDF15 in the blood and is already used to increase fertility in patients with polycystic ovary syndrome (PCOS) and in some cases of gestational diabetes.\n\nThere are also GDF15 blocking drugs in clinical trials for cancer patients with cachexia. Fejzo hopes that, once those drugs are shown to be safe in those trials as well as in pregnant animal models, they can be tested in pregnant women as well.\n\nOn January 9, San Francisco biotech company NGM Bio announced that they are talking with the FDA about beginning clinical trials in HG patients with their GDF15-blocking drug, NGM120. Fejzo will serve as an advisor to NGM Bio in the process."
    },
    {
        "url": "https://www.nationalgeographic.com/science/article/dna-data-storage-biotechnology",
        "text": "The Age of AI will rely on massive volumes of data that can be easily stored and retrieved—and bioscience may have an ingenious solution. Shakespeare’s entire catalog of sonnets and eight of his tragedies, all of Wikipedia’s English-language pages, and one of the first movies ever made: scientists have been able to fit the contents of all these works in a space smaller than a tiny test tube. They didn’t somehow miniaturize them, though. Instead, they used DNA—the building block of all life—to encode the information in these creative works and store it at a microscopic scale. As humans adopt advanced tools like artificial intelligence, tomorrow’s currency will be data. Already, tech giants like Microsoft are raising billions of dollars to construct data centers for AI. And there’s a very real “Storage Wars” scramble underway right now to figure out how to preserve and safeguard exponentially increasing amounts of data. Football field-size, gigawatt energy-sucking data centers are one option. Or DNA storage could be an energy-efficient, compact solution.  (Ancient DNA, from Neanderthals to the Black Plague, is transforming archaeology) We typically think of DNA as a blueprint or instruction booklet—its sequences of As, Ts, Cs, and Gs tell molecular machines how to build the fabric of our very beings. DNA storage flips this paradigm on its head. Computer data make up the inputs, and DNA is the end product. A handful of start-ups are working to perfect the conversion of binary computer code into physical DNA strands, and in doing so, take a shot at disrupting the multibillion-dollar storage industry. Here’s how they plan to move the industry away from microfilm, microfiche, disks, and servers. Traditional data storage relies on constant migration to prevent old data from degrading or the technology it’s stored in from becoming obsolete. Varun Mehta, CEO of Atlas Data Storage, compares long-term data storage to painting the Golden Gate bridge—by the time you’ve gone from one end to the other, the first end is rusting and you have to start all over again. “The same thing happens with long-term data storage,” he says. “You’re always moving from your old tape to your new tape.” He predicts that “people who want to get off that treadmill will be the first to move to DNA.” In practice, DNA storage involves several steps: deciding on a code, making the DNA using a process called synthesis, and storing the resulting DNA strands. DNA storage methods also include ways to categorize the stored strands and convert nucleotide sequences back into information that may be compatible with computers or accessible in some other way. Though industry members formed the DNA Data Storage Alliance in 2020 in part to set standards, companies in the DNA storage space still approach each of these steps in slightly different ways. (This archaeologist hunts DNA from prehistoric diseases) First, to store information as DNA, scientists have to determine how the data will be translated. DNA is a base 4 system; in contrast, computers store and process information in binary. Instead of assigning a “1” or a “0” to each DNA nucleotide—an A, C, T, or G—you could instead assign a particular combination of two digits to each base—so an A might stand in for “00,” C “01,” T “10,” and G “11.” Theoretically, this means every DNA nucleotide can encode up to 2 unique bits. In practice, the system isn’t as efficient as that (there are certain combinations of DNA nucleotides that are less stable or otherwise undesirable, and different chemistry protocols exist for turning bits into DNA bases). Catalog, one DNA storage company, announced in 2022 that it had encoded eight of William Shakespeare’s tragedies into a single test tube. To do this, scientists had to translate about 207,000 words into strings of nucleotides using a class of enzymes called recombinases. They claimed their DNA-building machine, Shannon, encoded the plays into millions of nucleotides in a matter of minutes. “To each of those words, you associate a random bit vector. A bit vector is just a sequences of ones and zeroes of a fixed length,” explains Catalog’s head of DNA Computing, Swapnil Bhatia, in a video for the company. The word “rose” might have a random bit vector stretching 1,000 numbers long, and different companies will have different ciphers for translating words into 1s, 0s, and nucleotides. DNA synthesis—the step of actually creating custom DNA strands—is another place where companies diverge in their methods. Catalog uses the principles of inkjet printing to exude tiny droplets containing premade DNA fragments. In each droplet, hundreds of thousands of chemical reactions take place per second to elongate the DNA strands. Atlas Data Storage, meanwhile, relies on semiconductor chips and silicon wafers as the environment for assembling strands of synthetic DNA. “Once those strands are assembled, we harvest them from our chip,” Mehta says. “These DNA strands really are like corn stalks growing in a field on this chip and once they've gotten to the height that we want—to the number of bases—then we harvest them.”  (Dog DNA tests are on the rise. But are they reliable?) Storing and preserving these synthetic strands presents another set of hurdles. Catalog and Atlas store DNA samples inside metal capsules, where the strands are not exposed to the elements and degraded. To convert DNA back into bit form, one can sequence it—using the same technology that powers genetic testing like 23andMe. This method can’t be done indefinitely; eventually, the sample will need to be copied over again to restore it. To create longer-lasting, accessible storage, some groups are working on fluorescent tags. Shining a light on the samples can tell researchers information about a given sample at a glance, the same way metadata can help us organize computer files without having to open them. If companies are able to surmount these challenges, a DNA storage system would take up a fraction of the space of traditional storage methods. “The theoretical limit is astounding,” Mehta says. “You could fill 50 petabytes worth of data in in a Tylenol-sized capsule”—or roughly 50,000 times as much data as an iPhone can store. Storing information in such a small physical package raises philosophical questions about the purpose of storage. Could a storage device itself serve a purpose? Scientists have theorized and created proofs-of-concept of fabrics and everyday items like glasses that contain DNA-stored information. The company Catalog has a branch dedicated to “DNA computing” to search and analyze synthetic DNA without first converting the information encoded in it back into bits. There could be some advantages to working with data in DNA form—rather than moving from one end to another, like a computer processor does, working with the data can occur in many places at once in parallel. DNA’s status as the basic building block of life may someday make it one of our most durable technologies, Mehta says, because it means it isn’t going anywhere. “One thousand years from now, there probably will not be any DVD players. In fact, it's hard to find a VHS tape player anymore. But that's never going to happen with DNA, because we need it for our own health,” he says. “We'll always have that technology available.”     Copyright © 1996-2015 National Geographic SocietyCopyright © 2015-2025 National Geographic Partners, LLC. All rights reserved",
        "onlytext": "The Age of AI will rely on massive volumes of data that can be easily stored and retrieved—and bioscience may have an ingenious solution. Shakespeare’s entire catalog of sonnets and eight of his tragedies, all of Wikipedia’s English-language pages, and one of the first movies ever made: scientists have been able to fit the contents of all these works in a space smaller than a tiny test tube. They didn’t somehow miniaturize them, though. Instead, they used DNA—the building block of all life—to encode the information in these creative works and store it at a microscopic scale.\n\nAs humans adopt advanced tools like artificial intelligence, tomorrow’s currency will be data. Already, tech giants like Microsoft are raising billions of dollars to construct data centers for AI. And there’s a very real “Storage Wars” scramble underway right now to figure out how to preserve and safeguard exponentially increasing amounts of data. Football field-size, gigawatt energy-sucking data centers are one option. Or DNA storage could be an energy-efficient, compact solution.\n\nWe typically think of DNA as a blueprint or instruction booklet—its sequences of As, Ts, Cs, and Gs tell molecular machines how to build the fabric of our very beings. DNA storage flips this paradigm on its head. Computer data make up the inputs, and DNA is the end product. A handful of start-ups are working to perfect the conversion of binary computer code into physical DNA strands, and in doing so, take a shot at disrupting the multibillion-dollar storage industry.\n\nHere’s how they plan to move the industry away from microfilm, microfiche, disks, and servers. Traditional data storage relies on constant migration to prevent old data from degrading or the technology it’s stored in from becoming obsolete. Varun Mehta, CEO of Atlas Data Storage, compares long-term data storage to painting the Golden Gate bridge—by the time you’ve gone from one end to the other, the first end is rusting and you have to start all over again. “The same thing happens with long-term data storage,” he says. “You’re always moving from your old tape to your new tape.” He predicts that “people who want to get off that treadmill will be the first to move to DNA.”\n\nIn practice, DNA storage involves several steps: deciding on a code, making the DNA using a process called synthesis, and storing the resulting DNA strands. DNA storage methods also include ways to categorize the stored strands and convert nucleotide sequences back into information that may be compatible with computers or accessible in some other way. Though industry members formed the DNA Data Storage Alliance in 2020 in part to set standards, companies in the DNA storage space still approach each of these steps in slightly different ways.\n\nFirst, to store information as DNA, scientists have to determine how the data will be translated. DNA is a base 4 system; in contrast, computers store and process information in binary. Instead of assigning a “1” or a “0” to each DNA nucleotide—an A, C, T, or G—you could instead assign a particular combination of two digits to each base—so an A might stand in for “00,” C “01,” T “10,” and G “11.” Theoretically, this means every DNA nucleotide can encode up to 2 unique bits. In practice, the system isn’t as efficient as that (there are certain combinations of DNA nucleotides that are less stable or otherwise undesirable, and different chemistry protocols exist for turning bits into DNA bases).\n\nCatalog, one DNA storage company, announced in 2022 that it had encoded eight of William Shakespeare’s tragedies into a single test tube. To do this, scientists had to translate about 207,000 words into strings of nucleotides using a class of enzymes called recombinases. They claimed their DNA-building machine, Shannon, encoded the plays into millions of nucleotides in a matter of minutes. “To each of those words, you associate a random bit vector. A bit vector is just a sequences of ones and zeroes of a fixed length,” explains Catalog’s head of DNA Computing, Swapnil Bhatia, in a video for the company. The word “rose” might have a random bit vector stretching 1,000 numbers long, and different companies will have different ciphers for translating words into 1s, 0s, and nucleotides.\n\nDNA synthesis—the step of actually creating custom DNA strands—is another place where companies diverge in their methods. Catalog uses the principles of inkjet printing to exude tiny droplets containing premade DNA fragments. In each droplet, hundreds of thousands of chemical reactions take place per second to elongate the DNA strands. Atlas Data Storage, meanwhile, relies on semiconductor chips and silicon wafers as the environment for assembling strands of synthetic DNA. “Once those strands are assembled, we harvest them from our chip,” Mehta says. “These DNA strands really are like corn stalks growing in a field on this chip and once they've gotten to the height that we want—to the number of bases—then we harvest them.”\n\nStoring and preserving these synthetic strands presents another set of hurdles. Catalog and Atlas store DNA samples inside metal capsules, where the strands are not exposed to the elements and degraded. To convert DNA back into bit form, one can sequence it—using the same technology that powers genetic testing like 23andMe. This method can’t be done indefinitely; eventually, the sample will need to be copied over again to restore it. To create longer-lasting, accessible storage, some groups are working on fluorescent tags. Shining a light on the samples can tell researchers information about a given sample at a glance, the same way metadata can help us organize computer files without having to open them.\n\nIf companies are able to surmount these challenges, a DNA storage system would take up a fraction of the space of traditional storage methods. “The theoretical limit is astounding,” Mehta says. “You could fill 50 petabytes worth of data in in a Tylenol-sized capsule”—or roughly 50,000 times as much data as an iPhone can store.\n\nStoring information in such a small physical package raises philosophical questions about the purpose of storage. Could a storage device itself serve a purpose? Scientists have theorized and created proofs-of-concept of fabrics and everyday items like glasses that contain DNA-stored information. The company Catalog has a branch dedicated to “DNA computing” to search and analyze synthetic DNA without first converting the information encoded in it back into bits. There could be some advantages to working with data in DNA form—rather than moving from one end to another, like a computer processor does, working with the data can occur in many places at once in parallel.\n\nDNA’s status as the basic building block of life may someday make it one of our most durable technologies, Mehta says, because it means it isn’t going anywhere. “One thousand years from now, there probably will not be any DVD players. In fact, it's hard to find a VHS tape player anymore. But that's never going to happen with DNA, because we need it for our own health,” he says. “We'll always have that technology available.”"
    },
    {
        "url": "https://www.nationalgeographic.com/history/article/louis-braille-writing-system-creator",
        "text": "Two hundred years ago, the son of a saddler from a rural French village devised a groundbreaking tactile writing method of raised dots for blind people at the mere age of 15. Where would we be without writing? From its origins over 5,000 years ago in ancient Mesopotamia, the history of writing echoes the history of humanity. The Greeks and Romans developed unique alphabets, the Chinese evolved complex characters, and today we read novels, newspapers, and social media. A bedrock of human civilization, writing is fundamental for the rule of law and the accumulation of knowledge and culture. Yet it was not until the 19th century that blind people had access to writing. Between 1824 and 1825, Louis Braille created a system of raised dot letters that could be read with the hands. Initially ignored, his invention would be universally adopted by the 20th century, opening a new world of learning for the visually impaired. In a speech at the Sorbonne on the centennial of Braille’s death, Helen Keller said, “We, the blind, are as indebted to Louis Braille as mankind is to Gutenberg.” (How to make travel more accessible to the blind.) The youngest of four children, Braille was born in 1809 in the village of Coupvray, 22 miles east of Paris. His father, Simon-René, worked as a saddler, a trade that was always in demand. The family lived comfortably, also cultivating vines for winemaking. Luxuries, such as a bread oven, can be seen today in the house, transformed into the Louis Braille Museum in the 1950s. The centerpiece exhibit is the re-created leather workshop where Braille suffered the accident that would lead to his loss of sight, changing his destiny—and the course of history. As a curious three-year-old, Braille snuck into the shop when no one was around and played with the tools he often watched his father use. When he tried to punch a hole in the leather with an awl, the tool slipped and pierced his eye. This horrific injury led to an infection that spread to both eyes, leaving him blind by the age of five, as antibiotics were not yet discovered. His distraught parents did not want their son’s fate sealed in an era when the visually impaired were treated as subhuman, often ridiculed for their disability. On French city streets, blind people were paraded in silly outfits or resigned to begging. Public school education was not yet mandatory in France, but Braille’s parents understood the importance of literacy. To aid his son, Simon-René hammered nails into the shape of the alphabet’s letters on panels, and a priest named Abbé Jacques Palluy began to instruct Braille. By the age of seven, he attended the local school, where he was the only blind student. His teacher was struck by his raw intelligence and happy demeanor—traits that were admired by Braille’s friends over the course of his life. A few years later, a scholarship was secured for him to continue his studies at the Royal Institute for Blind Youth, the world’s first such school and one that’s still in existence, now called the National Institute for Blind Youth, or INJA. At 10 years old, he would be its youngest ever student. Most astonishing of all was his close-knit family’s consent in allowing him to leave home. “His mother and father could’ve just as easily kept him in the village,” explained Farida Saïdi-Hamid, the curator of the Louis Braille Museum. “They are going to write his destiny without knowing it.” This familial support would prove a constant for Braille, and he would continue to return to Coupvray to rest and recharge throughout his life. (These scientists set out to end blindness.) Founded by pioneering educator Valentin Haüy, the institute was groundbreaking in its methodology and approach. The students learned a variety of academic subjects and a manual trade. Haüy had devised a means of embossing books with raised letters, which the children could read with their fingertips, albeit with great difficulty. The school would bring Braille’s salvation and his demise, because it’s likely where he caught the tuberculosis that would kill him. The building, situated in the longtime student hub of Paris’s Latin Quarter, was filthy, damp, and run-down. It had even served as a prison during the French Revolution. But despite the noxious conditions, and the sometimes severe punishment doled out for rule-breaking kids, Braille thrived, making friends and excelling at his studies. Teachers noted his remarkable smarts and spiritual quality. His friend Hippolyte Coltat would later write, “Friendship with him was a conscientious duty as well as a tender sentiment. He would have sacrificed everything for it, his time, his health, his possessions.” The catalyst for Braille’s invention came in 1821. Capt. Charles Barbier, an artillery officer, had devised a means of “night writing” for the French Army to transmit and carry out orders under the cover of darkness. Convinced of its merit for blind people, Barbier transformed this dot-and-dash code into a phonetics-based system he presented to the students. There were linguistic flaws—sonography reduced language to sounds, so spelling was inaccurate and punctuation nonexistent—but Braille had an epiphany. A dot system could provide an easy and efficient method for the visually impaired to read and write. He spent the next four years working to devise such a code. At the institute, he’d pull all-nighters after his classes had finished. Even on vacations home to Coupvray, villagers would describe seeing the boy sitting on a hill with stylus and paper in hand. At the age of 15, he succeeded in creating what would become known as braille writing. The basis was cells of six dots arranged in two columns and three rows. Each combination of raised dots represented a letter of the alphabet. It was elegant in its simplicity and logic. The school’s students quickly embraced its use—allowed in an unofficial capacity by Director François-René Pignier. Braille humbly acknowledged his indebtedness to Barbier in his 1829 book Method for Writing Words, Music, and Plainsong by Means of Dots for the Use of the Blind: “If we have pointed out the advantages of our method over his, we must say in his honor that his method gave us the first idea of our own.” Despite Pignier’s promotion of braille and letters to the government, the system was not immediately accepted. The established order, dictated by the sighted, was resistant to change and favored the uniform use of one writing system. Braille became a teacher at the institute at the age of 19. By 26, he was diagnosed with tuberculosis, leading to long stretches of convalescence at home in Coupvray. Political machinations at the school led to the ousting of Pignier, whose replacement, Pierre-Armand Dufau, flatly rejected the use of braille. He even burned books and punished students caught using it. Gracefully, Braille persisted in his fight for the acceptance of his new writing system. A letter that he wrote in 1840 to Johann Wilhelm Klein, the founder of a school for blind people in Vienna, shows his humble efforts of persuasion when describing yet another invention, decapoint, a means for blind and sighted people to communicate: “I will be happy if my little methods can be useful for your students, and if this specimen is in your eyes the proof of the high consideration with which I have the honor to be, sir, your respectful and very humble servant, Braille.” A moment of recognition finally came in 1844, at the inauguration of the school’s new premises on the Boulevard des Invalides. By this time, Dufau had changed his mind about braille, thanks to the insistence of assistant director Joseph Guadet. Following a speech about the raised-dot system, students demonstrated its use by transcribing and reading verse. Guadet later wrote: “Braille was modest, too modest ... those around him did not appreciate him ... We were perhaps the first to give him his proper place in the eyes of the public, either in spreading his system more widely in our musical instruction or in making known the full significance of his invention.” Louis Braille did not live to see the universal adoption of braille. He died on January 6, 1852, surrounded by his brother and friends. Not a single newspaper carried a death notice for the man called “the apostle of light” by Jean Roblin, the first curator of the Louis Braille Museum. Students raised money for Parisian sculptor François Jouffroy to create a marble bust based on Braille’s death mask. In 1878 in Paris, a global congress for deaf and blind people proposed an inter- national braille standard. Braille was officially adopted by English speakers in 1932, and postwar UNESCO efforts unified adaptations in India, Africa, and the Middle East. Braille’s profound legacy cannot be overstated. On the centennial of his death, Braille’s accomplishments were finally celebrated in a national homage. His body was exhumed from the Coupvray cemetery and transferred to Paris’s Panthéon, the resting place of France’s great citizens. (His hands remained in an urn decorated with ceramic flowers at the Coupvray grave.) The parade through the streets of Paris included hundreds of blind people, elbows linked, some wearing dark sunglasses, tapping white canes on the cobblestones. Yet 200 years after the invention of braille writing, the fight continues. It is a fight to preserve not only the memory of Louis Braille, the subject of surprisingly few biographies, but also the use of his system in the digital age. Increasingly, visually impaired children are learning via screens and audio programs. But neuroscientists argue that writing is essential for thinking, brain connectivity, and learning. The cognitive benefits of writing are fundamentally important. Studies have shown that when a blind person reads braille through touch, the visual cortex is illuminated. With a shortage of braille teachers worldwide, braille literacy has plummeted, and its very future is in peril. Saïdi-Hamid, the curator of the Louis Braille Museum for nearly 17 years, equates her fight to defend braille as a “combat to defend intelligence itself.” Noting Braille’s “extraordinary personality,” Saïdi-Hamid said, “he always perceived his disability as a strength and not as a limitation.” As Braille fought during his lifetime, the fight must go on. (How the wheelchair opened up the world to millions of people.)  Copyright © 1996-2015 National Geographic SocietyCopyright © 2015-2025 National Geographic Partners, LLC. All rights reserved",
        "onlytext": "Two hundred years ago, the son of a saddler from a rural French village devised a groundbreaking tactile writing method of raised dots for blind people at the mere age of 15. Where would we be without writing? From its origins over 5,000 years ago in ancient Mesopotamia, the history of writing echoes the history of humanity. The Greeks and Romans developed unique alphabets, the Chinese evolved complex characters, and today we read novels, newspapers, and social media. A bedrock of human civilization, writing is fundamental for the rule of law and the accumulation of knowledge and culture. Yet it was not until the 19th century that blind people had access to writing.\n\nBetween 1824 and 1825, Louis Braille created a system of raised dot letters that could be read with the hands. Initially ignored, his invention would be universally adopted by the 20th century, opening a new world of learning for the visually impaired. In a speech at the Sorbonne on the centennial of Braille’s death, Helen Keller said, “We, the blind, are as indebted to Louis Braille as mankind is to Gutenberg.” \n\nThe youngest of four children, Braille was born in 1809 in the village of Coupvray, 22 miles east of Paris. His father, Simon-René, worked as a saddler, a trade that was always in demand. The family lived comfortably, also cultivating vines for winemaking. Luxuries, such as a bread oven, can be seen today in the house, transformed into the Louis Braille Museum in the 1950s. The centerpiece exhibit is the re-created leather workshop where Braille suffered the accident that would lead to his loss of sight, changing his destiny—and the course of history.\n\nAs a curious three-year-old, Braille snuck into the shop when no one was around and played with the tools he often watched his father use. When he tried to punch a hole in the leather with an awl, the tool slipped and pierced his eye. This horrific injury led to an infection that spread to both eyes, leaving him blind by the age of five, as antibiotics were not yet discovered. His distraught parents did not want their son’s fate sealed in an era when the visually impaired were treated as subhuman, often ridiculed for their disability.\n\nOn French city streets, blind people were paraded in silly outfits or resigned to begging. Public school education was not yet mandatory in France, but Braille’s parents understood the importance of literacy. To aid his son, Simon-René hammered nails into the shape of the alphabet’s letters on panels, and a priest named Abbé Jacques Palluy began to instruct Braille. By the age of seven, he attended the local school, where he was the only blind student. His teacher was struck by his raw intelligence and happy demeanor—traits that were admired by Braille’s friends over the course of his life.\n\nA few years later, a scholarship was secured for him to continue his studies at the Royal Institute for Blind Youth, the world’s first such school and one that’s still in existence, now called the National Institute for Blind Youth, or INJA. At 10 years old, he would be its youngest ever student. Most astonishing of all was his close-knit family’s consent in allowing him to leave home. “His mother and father could’ve just as easily kept him in the village,” explained Farida Saïdi-Hamid, the curator of the Louis Braille Museum. “They are going to write his destiny without knowing it.” \n\nThis familial support would prove a constant for Braille, and he would continue to return to Coupvray to rest and recharge throughout his life. Founded by pioneering educator Valentin Haüy, the institute was groundbreaking in its methodology and approach. The students learned a variety of academic subjects and a manual trade. Haüy had devised a means of embossing books with raised letters, which the children could read with their fingertips, albeit with great difficulty.\n\nThe school would bring Braille’s salvation and his demise, because it’s likely where he caught the tuberculosis that would kill him. The building, situated in the longtime student hub of Paris’s Latin Quarter, was filthy, damp, and run-down. It had even served as a prison during the French Revolution. But despite the noxious conditions, and the sometimes severe punishment doled out for rule-breaking kids, Braille thrived, making friends and excelling at his studies.\n\nTeachers noted his remarkable smarts and spiritual quality. His friend Hippolyte Coltat would later write, “Friendship with him was a conscientious duty as well as a tender sentiment. He would have sacrificed everything for it, his time, his health, his possessions.” The catalyst for Braille’s invention came in 1821. Capt. Charles Barbier, an artillery officer, had devised a means of “night writing” for the French Army to transmit and carry out orders under the cover of darkness.\n\nConvinced of its merit for blind people, Barbier transformed this dot-and-dash code into a phonetics-based system he presented to the students. There were linguistic flaws—sonography reduced language to sounds, so spelling was inaccurate and punctuation nonexistent—but Braille had an epiphany. A dot system could provide an easy and efficient method for the visually impaired to read and write.\n\nHe spent the next four years working to devise such a code. At the institute, he’d pull all-nighters after his classes had finished. Even on vacations home to Coupvray, villagers would describe seeing the boy sitting on a hill with stylus and paper in hand. At the age of 15, he succeeded in creating what would become known as braille writing.\n\nThe basis was cells of six dots arranged in two columns and three rows. Each combination of raised dots represented a letter of the alphabet. It was elegant in its simplicity and logic. The school’s students quickly embraced its use—allowed in an unofficial capacity by Director François-René Pignier. Braille humbly acknowledged his indebtedness to Barbier in his 1829 book Method for Writing Words, Music, and Plainsong by Means of Dots for the Use of the Blind: “If we have pointed out the advantages of our method over his, we must say in his honor that his method gave us the first idea of our own.”\n\nDespite Pignier’s promotion of braille and letters to the government, the system was not immediately accepted. The established order, dictated by the sighted, was resistant to change and favored the uniform use of one writing system. Braille became a teacher at the institute at the age of 19. By 26, he was diagnosed with tuberculosis, leading to long stretches of convalescence at home in Coupvray.\n\nPolitical machinations at the school led to the ousting of Pignier, whose replacement, Pierre-Armand Dufau, flatly rejected the use of braille. He even burned books and punished students caught using it. Gracefully, Braille persisted in his fight for the acceptance of his new writing system. A letter that he wrote in 1840 to Johann Wilhelm Klein, the founder of a school for blind people in Vienna, shows his humble efforts of persuasion when describing yet another invention, decapoint, a means for blind and sighted people to communicate: “I will be happy if my little methods can be useful for your students, and if this specimen is in your eyes the proof of the high consideration with which I have the honor to be, sir, your respectful and very humble servant, Braille.”\n\nA moment of recognition finally came in 1844, at the inauguration of the school’s new premises on the Boulevard des Invalides. By this time, Dufau had changed his mind about braille, thanks to the insistence of assistant director Joseph Guadet. Following a speech about the raised-dot system, students demonstrated its use by transcribing and reading verse. Guadet later wrote: “Braille was modest, too modest ... those around him did not appreciate him ... We were perhaps the first to give him his proper place in the eyes of the public, either in spreading his system more widely in our musical instruction or in making known the full significance of his invention.”\n\nLouis Braille did not live to see the universal adoption of braille. He died on January 6, 1852, surrounded by his brother and friends. Not a single newspaper carried a death notice for the man called “the apostle of light” by Jean Roblin, the first curator of the Louis Braille Museum. Students raised money for Parisian sculptor François Jouffroy to create a marble bust based on Braille’s death mask.\n\nIn 1878 in Paris, a global congress for deaf and blind people proposed an inter- national braille standard. Braille was officially adopted by English speakers in 1932, and postwar UNESCO efforts unified adaptations in India, Africa, and the Middle East. Braille’s profound legacy cannot be overstated. On the centennial of his death, Braille’s accomplishments were finally celebrated in a national homage.\n\nHis body was exhumed from the Coupvray cemetery and transferred to Paris’s Panthéon, the resting place of France’s great citizens.  The parade through the streets of Paris included hundreds of blind people, elbows linked, some wearing dark sunglasses, tapping white canes on the cobblestones. Yet 200 years after the invention of braille writing, the fight continues.\n\nIt is a fight to preserve not only the memory of Louis Braille, the subject of surprisingly few biographies, but also the use of his system in the digital age. Increasingly, visually impaired children are learning via screens and audio programs. But neuroscientists argue that writing is essential for thinking, brain connectivity, and learning. The cognitive benefits of writing are fundamentally important.\n\nStudies have shown that when a blind person reads braille through touch, the visual cortex is illuminated. With a shortage of braille teachers worldwide, braille literacy has plummeted, and its very future is in peril. Saïdi-Hamid, the curator of the Louis Braille Museum for nearly 17 years, equates her fight to defend braille as a “combat to defend intelligence itself.” Noting Braille’s “extraordinary personality,” Saïdi-Hamid said, “he always perceived his disability as a strength and not as a limitation.” As Braille fought during his lifetime, the fight must go on."
    },
    {
        "url": "https://www.nationalgeographic.com/science/article/led-red-light-mask-cold-laser-treatment",
        "text": "Dermatologists explain the science behind wildly popular treatments like red light therapy, which is now available at home—for a steep price. Red light therapy was first tested back in the mid-1960s, when a Hungarian physician named Endre Mester shined a low-power laser on the shaved skin of a mouse. Though he was testing for impacts on tumor growth, he observed an unexpected side effect—the red light seemed to stimulate hair growth and, in later studies, wound healing. Sixty years later, this accidental discovery has found its way into a booming market for clinical procedures and at-home devices using light-emitting diodes (LEDs). Also possible with cold lasers, “low level” or “low power” light therapy requires a certain power level (measured in milliwatts per centimeter) to be effective, without the damaging heat of high-powered lights. From face and full-body masks to portable wands, low-level light therapy products promise all sorts of anti-aging, rejuvenating, and wound-healing benefits.  “There’s a lot of hype in the industry,” says Daniel Barolet, a dermatological laser therapy researcher and clinician, and an adjunct professor at McGill University. But are these devices—which can cost hundreds, even thousands of dollars—worth the price? Experts discuss the benefits and limitations of light therapy, and what consumers should know before making the investment.   In the decades since Mester’s groundbreaking mice trials, research and development of phototherapy has taken off—with promising results. Today, LED light therapy, or “photobiomodulation” (PBM) as it’s more accurately known, uses gentle, low-level light within the visible spectrum—typically blue, red, or near infrared light—to stimulate natural physiological processes. Or, as Barolet says, PBM just means “using light to give our cells a little nudge in the right direction.” For much of human history, we would have gotten these benefits directly from the sun. But now we’re spending more time inside, with often cool-toned indoor lighting. “When we’re using photobiomodulation, we just cut the bad UV rays. We're just using the healing stuff,” Barolet says. “It’s biomimicking, but you harness what’s good for your skin, and you delete what’s not good for your skin.” While the exact mechanisms are not yet fully understood, PBM therapy has been shown to produce a range of positive effects. Blue light, for instance, can be used to treat acne and may mitigate other skin disorders by reducing inflammation. When blue light is absorbed by the skin, it activates the production of toxic free radicals which, over the course of several days, kill off the acne-causing bacteria, P. acnes. Glynis Ablon, a dermatologist and associate clinical professor at UCLA, says she’s seen dramatic effects in her patients undergoing blue light therapy. “What they’ll notice, just from these LEDs alone, is that their acne gets better, that the level of inflammatory lesions reduces, that their overall skin just starts to look better.” Red and near-infrared light, on the other hand, have longer wavelengths that can target the skin cells or travel deeper into the body. When light in this range penetrates the cells, Barolet says, it “kicks off a chain reaction” in the mitochondria. This metabolic process produces several important molecules, including ATP and nitric oxide, that are critical for basic bodily functions like energy production and healthy blood flow.  (Do ice facials actually work? We asked experts.) The application of red and near-infrared light has a domino effect, stimulating collagen production and blood circulation. On a surface level, this can speed healing of wounds, such as burns or ulcers, and even reduce signs of aging, like wrinkles and brown spots. While the skin is the largest and most visible organ in the body, all cells theoretically can enjoy a boost from red or near-infrared light therapy. Generally speaking, the tissues that will improve the most are those in a state of depletion or disorder—such as sunburnt skin—according to Alexander Wunsch, a physician and photobiology expert. Of course, light therapy isn’t a panacea for treating skin or any other conditions.  “There is real science to it and it does work clinically,” says Zakia Rahman, a clinical professor of dermatology at the Stanford School of Medicine, “but it’s not going to have the level of dramatic effect that more aggressive treatments in a medical setting would have.”  (The real science of exosomes—the latest obsession in skincare) Still, LEDs have distinct benefits—they’re non-invasive, pain-free, and essentially harmless. Prolonged exposure to blue light, which is near the UV spectrum, may cause skin damage, aging or irritation, but long-term research is still limited. With red and near-infrared, the only potential concerns are for those with sun allergies or highly sensitive eyes, according to Barolet. “Think of photobiomodulation like your morning cup of coffee … but with light,” Barolet says. “It’s like a wake-up call that gets all those tiny cellular processes up and running, helping the body to repair, rejuvenate, and energize itself.” Clinics around the world offer PBM light to treat cosmetic and medical conditions. Office devices are generally more powerful, producing better and faster results. But for those who want to do their treatment at home, there’s a market full of options.  When it comes to choosing an LED device, experts agree that the most meaningful factor to consider is the output intensity. (Snail mucus is a skin care phenomenon—but does it really work?) “There’s a lot of scams out there … most of the time, energies are very, very low,” Ablon says. For red light, she looks for devices that emit 105 milliwatts per centimeter, but for blue light, the intensity can be lower. “If it’s somewhere in that 40 range, I’m OK, but if it ends up being 10, it’s probably not doing anything.” Barolet also warns consumers to avoid devices that promote “a rainbow” of lights, such as green, yellow, and purple. When it comes to health, he says, the only wavelengths proven to be effective are red, near-infrared, and blue. It’s important to note that not all LED products on the market have been greenlit by the FDA. Wunsch advises consumers to look for 510(k) Clearance, “which gives you at least the information that this device has been evaluated”—though not tested—by the FDA for safety and effectiveness. Barolet notes that, while the research supporting light therapy is robust, scientists are still perfecting their “recipe”—the ideal wavelengths, dosage, intensity, and proximity—for different health goals. (Vitamin C, retinol, biotin? Here’s what your skin actually needs) In some cases, LED therapy works best in concert with other interventions, like pairing red light treatment with anti-aging cream, he says. While not a replacement for products like sunscreen or prescription retinoids, Rahman says that LEDs “can work well in your entire skin regimen.” Ultimately, whether undergoing a clinical procedure or using at-home masks, experts advise people to stay the course of the treatment and not expect immediate results. (Anti-aging peptide injections are the latest wellness trend—but do they work?) While wound healing can happen faster, Wunsch says, the effects on healthy, normally aging skin can be slow and cumulative. With light therapy, “you have to invest in the compliance in the first phase, and you will get your rewards in five or 10 years.”  Copyright © 1996-2015 National Geographic SocietyCopyright © 2015-2025 National Geographic Partners, LLC. All rights reserved",
        "onlytext": "Red light therapy was first tested back in the mid-1960s, when a Hungarian physician named Endre Mester shined a low-power laser on the shaved skin of a mouse. Though he was testing for impacts on tumor growth, he observed an unexpected side effect—the red light seemed to stimulate hair growth and, in later studies, wound healing. Sixty years later, this accidental discovery has found its way into a booming market for clinical procedures and at-home devices using light-emitting diodes (LEDs). Also possible with cold lasers, “low level” or “low power” light therapy requires a certain power level (measured in milliwatts per centimeter) to be effective, without the damaging heat of high-powered lights. From face and full-body masks to portable wands, low-level light therapy products promise all sorts of anti-aging, rejuvenating, and wound-healing benefits.\n\n“There’s a lot of hype in the industry,” says Daniel Barolet, a dermatological laser therapy researcher and clinician, and an adjunct professor at McGill University. But are these devices—which can cost hundreds, even thousands of dollars—worth the price? Experts discuss the benefits and limitations of light therapy, and what consumers should know before making the investment.\n\nIn the decades since Mester’s groundbreaking mice trials, research and development of phototherapy has taken off—with promising results. Today, LED light therapy, or “photobiomodulation” (PBM) as it’s more accurately known, uses gentle, low-level light within the visible spectrum—typically blue, red, or near infrared light—to stimulate natural physiological processes. Or, as Barolet says, PBM just means “using light to give our cells a little nudge in the right direction.” For much of human history, we would have gotten these benefits directly from the sun. But now we’re spending more time inside, with often cool-toned indoor lighting. “When we’re using photobiomodulation, we just cut the bad UV rays. We're just using the healing stuff,” Barolet says. “It’s biomimicking, but you harness what’s good for your skin, and you delete what’s not good for your skin.”\n\nWhile the exact mechanisms are not yet fully understood, PBM therapy has been shown to produce a range of positive effects. Blue light, for instance, can be used to treat acne and may mitigate other skin disorders by reducing inflammation. When blue light is absorbed by the skin, it activates the production of toxic free radicals which, over the course of several days, kill off the acne-causing bacteria, P. acnes. Glynis Ablon, a dermatologist and associate clinical professor at UCLA, says she’s seen dramatic effects in her patients undergoing blue light therapy. “What they’ll notice, just from these LEDs alone, is that their acne gets better, that the level of inflammatory lesions reduces, that their overall skin just starts to look better.”\n\nRed and near-infrared light, on the other hand, have longer wavelengths that can target the skin cells or travel deeper into the body. When light in this range penetrates the cells, Barolet says, it “kicks off a chain reaction” in the mitochondria. This metabolic process produces several important molecules, including ATP and nitric oxide, that are critical for basic bodily functions like energy production and healthy blood flow.\n\nThe application of red and near-infrared light has a domino effect, stimulating collagen production and blood circulation. On a surface level, this can speed healing of wounds, such as burns or ulcers, and even reduce signs of aging, like wrinkles and brown spots. While the skin is the largest and most visible organ in the body, all cells theoretically can enjoy a boost from red or near-infrared light therapy. Generally speaking, the tissues that will improve the most are those in a state of depletion or disorder—such as sunburnt skin—according to Alexander Wunsch, a physician and photobiology expert.\n\nOf course, light therapy isn’t a panacea for treating skin or any other conditions. “There is real science to it and it does work clinically,” says Zakia Rahman, a clinical professor of dermatology at the Stanford School of Medicine, “but it’s not going to have the level of dramatic effect that more aggressive treatments in a medical setting would have.”\n\nStill, LEDs have distinct benefits—they’re non-invasive, pain-free, and essentially harmless. Prolonged exposure to blue light, which is near the UV spectrum, may cause skin damage, aging or irritation, but long-term research is still limited. With red and near-infrared, the only potential concerns are for those with sun allergies or highly sensitive eyes, according to Barolet. “Think of photobiomodulation like your morning cup of coffee … but with light,” Barolet says. “It’s like a wake-up call that gets all those tiny cellular processes up and running, helping the body to repair, rejuvenate, and energize itself.”\n\nClinics around the world offer PBM light to treat cosmetic and medical conditions. Office devices are generally more powerful, producing better and faster results. But for those who want to do their treatment at home, there’s a market full of options.\n\nWhen it comes to choosing an LED device, experts agree that the most meaningful factor to consider is the output intensity. “There’s a lot of scams out there … most of the time, energies are very, very low,” Ablon says. For red light, she looks for devices that emit 105 milliwatts per centimeter, but for blue light, the intensity can be lower. “If it’s somewhere in that 40 range, I’m OK, but if it ends up being 10, it’s probably not doing anything.” Barolet also warns consumers to avoid devices that promote “a rainbow” of lights, such as green, yellow, and purple. When it comes to health, he says, the only wavelengths proven to be effective are red, near-infrared, and blue.\n\nIt’s important to note that not all LED products on the market have been greenlit by the FDA. Wunsch advises consumers to look for 510(k) Clearance, “which gives you at least the information that this device has been evaluated”—though not tested—by the FDA for safety and effectiveness.\n\nBarolet notes that, while the research supporting light therapy is robust, scientists are still perfecting their “recipe”—the ideal wavelengths, dosage, intensity, and proximity—for different health goals. In some cases, LED therapy works best in concert with other interventions, like pairing red light treatment with anti-aging cream, he says. While not a replacement for products like sunscreen or prescription retinoids, Rahman says that LEDs “can work well in your entire skin regimen.”\n\nUltimately, whether undergoing a clinical procedure or using at-home masks, experts advise people to stay the course of the treatment and not expect immediate results. While wound healing can happen faster, Wunsch says, the effects on healthy, normally aging skin can be slow and cumulative. With light therapy, “you have to invest in the compliance in the first phase, and you will get your rewards in five or 10 years.”"
    },
    {
        "url": "https://www.nationalgeographic.com/science/article/dinosaur-eggs-dating-fossils-uranium-lead-paleontology",
        "text": "Radioactive minerals in eggshells can help scientists pinpoint a fossil’s age with stunning accuracy. Eggs laid by dinosaurs have provided paleontologists with a new way to tell prehistoric time. By looking to radioactive minerals taken up by eggshells in the deep past, experts have uncovered a novel method to determine when those eggs were laid—one that could put ancient ecosystems on a more accurate timeline The new research, published today in the journal Communications Earth & Environment, adds to a growing body of evidence that fossil eggshells hold mineral clues that will allow experts to date them directly. Rather than deriving the dates of fossils from the rocks in which they are encased, the study—lead by Dr. Ryan Tucker from Stellenbosch University's Department of Earth Sciences—proposes that radioactive minerals like uranium in fossil eggshells can be used to directly date the fossils and refine prehistoric timelines that have previously been mysteries. (In September, a team of Chinese paleontologists announced they had dated eggs using clues in the mineral calcite contained within the fossils.) “Paleontologists excavate and study fossils so we can reconstruct the story of life on Earth, but a story without a timeline is at best nonsensical, and, at worst, misleading,” says study coauthor and North Carolina Museum of Natural Sciences paleontologist Lindsay Zanno, who is a National Geographic Explorer. Without a sense of when a prehistoric organism lived, she says, “a fossil out of context in time and space is as useful as lips on a chicken.” (How to dig up 55 tons of dinosaur bones in the world's fiercest desert) Up until now, the prehistoric timeline has been set by the rocks in which fossils are buried. When ancient volcanoes belched out molten rock and ash, for example, the outpourings of the eruptions contained radioactive minerals like uranium. Uranium turns to lead at a constant rate over time, what is sometimes referred to as its half-life, and for decades geologists have been able to look at the ratio of uranium to lead in a rock sample to calculate just how long ago that rock must have formed. The problem for paleontologists is that many fossils are found in sediments that lack these radioactive timekeepers. When an expert says a dinosaur is about 75 million years old, that usually means that their fossil was found in a rock layer in close association with a rock layer that contained something like an ash bed that could be dated correctly. Or that a fossil resembles another fossil found in such association with an ash layer. If a dinosaur fossil wasn’t found in a rock layer full of ash or solidified volcanic rocks, paleontologists could only roughly estimate the fossil’s age. The famous, well-preserved fossils of Mongolia’s Gobi Basin—dinosaurs such as the famous Velociraptor—fall into this category, their true age uncertain without a direct way to tell geologic time. “There are important dinosaur sites in North America and Asia that preserve eggshells and bones but these haven’t been accurately dated due to lack of ash layers,” says University of Calgary paleontologist Darla Zelenitsky, who was not involved in the new study. Though she cautions that “dinosaur eggshells aren’t found very often,” she notes that with uranium-lead (U-Pb) dating, “when they are [found], they could help us figure out the age of fossil sites, especially in places that don’t have volcanic ash layers.” (Is this a teen T. Rex ... or a totally different dinosaur?) Previous research on relatively recent eggshells has shown that those shells took in uranium from surrounding sediment when they were buried. The same was true of eggs many millions of years old—and so Zanno and colleagues looked to dinosaur eggs from two different places to test their hunch. The first set of eggs came from prehistoric Utah, laid by parrot-like dinosaurs called oviraptorosaurs and buried between two ash layers that were directly dated to about 99 million years old. The dates on these eggs with U-Pb dating came out to about 97 million years old, very close to the dates drawn from the rocks. The experts note that the porous nature of the eggs and the particulars of how the eggs were preserved likely yielded a slightly younger date than the ash, but the figures are so close that the findings still indicate eggshells can provide a close age estimate when ash beds are not available.  “Our study demonstrates that dating dinosaur eggshell is just as reliable, and in some cases even better, than historical methods” used to date fossils outside of ash layers, Zanno says.  With the close accuracy of the new technique established, Zanno and colleagues tried out the method on eggs laid by an unknown species of dinosaur from Mongolia’s Gobi Desert. The site, known as Teel Ulaan Chaltsai, has been thought to be anywhere from Early Cretaceous to Late Cretaceous in age, anywhere between 145 and 66 million years old. The new results found that the dinosaur eggs from the site were buried about 75 million years ago, confirming the dinosaurs there were living and nesting during the Late Cretaceous. “This dating method is still in the early stages, but it’s showing real promise,” Zelenitsky says. The method could allow experts to get a much clearer view of big-picture evolutionary and ecological questions. “The more dating tools we have, the better we can understand big questions like how dinosaurs evolved and their ecosystems changed over time.” (New megaraptor discovered—with its final meal still in its mouth) The method will be refined through future studies. “We knew we needed to establish how dinosaur eggshells incorporate radioactive elements and what checks should be performed to ensure that an age date from a dinosaur egg is reliable,” Zanno says. But Zanno and colleagues see the potential for better assessing other fossil sites around the planet that are rife with eggs but are of uncertain age. South Africa’s Elliot Formation, for example, contains multiple egg and nest sites from long-necked herbivorous dinosaurs called sauropodomorphs but lacks volcanic rocks to determine accurate ages, just like egg sites in the Auca Mahuevo site in Patagonia that are rife with the eggs of huge, long-necked plant-eaters called titanosaurs. Looking at the possibilities, Zanno says “I couldn’t be more excited about the future.”  Copyright © 1996-2015 National Geographic SocietyCopyright © 2015-2025 National Geographic Partners, LLC. All rights reserved",
        "onlytext": "Radioactive minerals in eggshells can help scientists pinpoint a fossil’s age with stunning accuracy. Eggs laid by dinosaurs have provided paleontologists with a new way to tell prehistoric time. By looking to radioactive minerals taken up by eggshells in the deep past, experts have uncovered a novel method to determine when those eggs were laid—one that could put ancient ecosystems on a more accurate timeline.\n\nThe new research, published today in the journal Communications Earth & Environment, adds to a growing body of evidence that fossil eggshells hold mineral clues that will allow experts to date them directly. Rather than deriving the dates of fossils from the rocks in which they are encased, the study—lead by Dr. Ryan Tucker from Stellenbosch University's Department of Earth Sciences—proposes that radioactive minerals like uranium in fossil eggshells can be used to directly date the fossils and refine prehistoric timelines that have previously been mysteries.\n\n“In paleontologists excavate and study fossils so we can reconstruct the story of life on Earth, but a story without a timeline is at best nonsensical, and, at worst, misleading,” says study coauthor and North Carolina Museum of Natural Sciences paleontologist Lindsay Zanno, who is a National Geographic Explorer. Without a sense of when a prehistoric organism lived, she says, “a fossil out of context in time and space is as useful as lips on a chicken.”\n\nUp until now, the prehistoric timeline has been set by the rocks in which fossils are buried. When ancient volcanoes belched out molten rock and ash, for example, the outpourings of the eruptions contained radioactive minerals like uranium. Uranium turns to lead at a constant rate over time, what is sometimes referred to as its half-life, and for decades geologists have been able to look at the ratio of uranium to lead in a rock sample to calculate just how long ago that rock must have formed.\n\nThe problem for paleontologists is that many fossils are found in sediments that lack these radioactive timekeepers. When an expert says a dinosaur is about 75 million years old, that usually means that their fossil was found in a rock layer in close association with a rock layer that contained something like an ash bed that could be dated correctly. Or that a fossil resembles another fossil found in such association with an ash layer. If a dinosaur fossil wasn’t found in a rock layer full of ash or solidified volcanic rocks, paleontologists could only roughly estimate the fossil’s age.\n\nThe famous, well-preserved fossils of Mongolia’s Gobi Basin—dinosaurs such as the famous Velociraptor—fall into this category, their true age uncertain without a direct way to tell geologic time. “There are important dinosaur sites in North America and Asia that preserve eggshells and bones but these haven’t been accurately dated due to lack of ash layers,” says University of Calgary paleontologist Darla Zelenitsky, who was not involved in the new study.\n\nThough she cautions that “dinosaur eggshells aren’t found very often,” she notes that with uranium-lead (U-Pb) dating, “when they are [found], they could help us figure out the age of fossil sites, especially in places that don’t have volcanic ash layers.”\n\nPrevious research on relatively recent eggshells has shown that those shells took in uranium from surrounding sediment when they were buried. The same was true of eggs many millions of years old—and so Zanno and colleagues looked to dinosaur eggs from two different places to test their hunch.\n\nThe first set of eggs came from prehistoric Utah, laid by parrot-like dinosaurs called oviraptorosaurs and buried between two ash layers that were directly dated to about 99 million years old. The dates on these eggs with U-Pb dating came out to about 97 million years old, very close to the dates drawn from the rocks.\n\nThe experts note that the porous nature of the eggs and the particulars of how the eggs were preserved likely yielded a slightly younger date than the ash, but the figures are so close that the findings still indicate eggshells can provide a close age estimate when ash beds are not available.\n\n“Our study demonstrates that dating dinosaur eggshell is just as reliable, and in some cases even better, than historical methods” used to date fossils outside of ash layers, Zanno says.\n\nWith the close accuracy of the new technique established, Zanno and colleagues tried out the method on eggs laid by an unknown species of dinosaur from Mongolia’s Gobi Desert. The site, known as Teel Ulaan Chaltsai, has been thought to be anywhere from Early Cretaceous to Late Cretaceous in age, anywhere between 145 and 66 million years old.\n\nThe new results found that the dinosaur eggs from the site were buried about 75 million years ago, confirming the dinosaurs there were living and nesting during the Late Cretaceous. “This dating method is still in the early stages, but it’s showing real promise,” Zelenitsky says.\n\nThe method could allow experts to get a much clearer view of big-picture evolutionary and ecological questions. “The more dating tools we have, the better we can understand big questions like how dinosaurs evolved and their ecosystems changed over time.”\n\nThe method will be refined through future studies. “We knew we needed to establish how dinosaur eggshells incorporate radioactive elements and what checks should be performed to ensure that an age date from a dinosaur egg is reliable,” Zanno says.\n\nBut Zanno and colleagues see the potential for better assessing other fossil sites around the planet that are rife with eggs but are of uncertain age. South Africa’s Elliot Formation, for example, contains multiple egg and nest sites from long-necked herbivorous dinosaurs called sauropodomorphs but lacks volcanic rocks to determine accurate ages, just like egg sites in the Auca Mahuevo site in Patagonia that are rife with the eggs of huge, long-necked plant-eaters called titanosaurs.\n\nLooking at the possibilities, Zanno says “I couldn’t be more excited about the future.”"
    },
    {
        "url": "https://www.nationalgeographic.com/science/article/dinosaur-mummy-hoof-edmontosaurus-fossils-skin",
        "text": "A new study sheds light on how these reptiles become “mummies” and paints a picture of what these ancient animals looked like. The bulk of what we know from dinosaurs comes from bones. Skeletal parts can reveal a dinosaur’s size, how fast they grew, and even if they were gravid with eggs when they perished, but all the ribs, jaws, and other pieces can only go so far in revealing what our favorite dinosaurs looked like in life. Fortunately, paleontologists have found some dinosaur bones are preserved with soft tissues like feathers, claw sheaths, and, with luck, entire dinosaur “mummies.” These aren’t traditional mummies purposefully preserved through chemical embalming. Instead, they’re fossils that preserve skin impressions as a thin layer within the rock. As paleontologists have continued to sift through the fossil record, such specimens are finally offering some insights into the external appearance of familiar species. And through untangling the backstories of these fossils, experts are beginning to realize that such cases of exceptional preservation might not be as rare as once believed.  One familiar face getting fleshed out is Edmontosaurus annectens. The duckbilled dinosaur walked the floodplains of ancient Montana and Saskatchewan 68 to 66 million years ago, and today it’s one of the best-known and most common fossils found in the region. Several Edmontosaurus mummies have revealed soft tissue structures that paleontologists never expected to see, including a pair of mummies uncovered in Wyoming described October 23 in Science. Some of the preserved features on the new E. annectens specimens, a juvenile and an adult, confirm what paleontologists predicted. The toes on the back feet of the dinosaurs, for example, had a tough coating on them, like a hoof or a toenail. A row of overlapping spiky scales also ran down their backs that connected to their spines, a feature that paleontologists had not seen before in its entirety. “This is sort of dragon-like,” says Paul Sereno, a paleontologist at the University of Chicago and a coauthor on the new paper. The findings add to previous research that has revealed Edmontosaurus had a fleshy comb on their heads, front toes wrapped together in a mitt of flesh tipped with a hoof-like covering, and rough, shovel-like beaks jutting down from their snouts. The newly-described fossils follow a long history of Edmontosaurus mummies found in western North America, including the first dinosaur mummy discovered in 1908. Scientists began calling these early specimen mummies because of the extensive skin impressions left in the rock that surrounded their skeletons. Since that time, paleontologists have uncovered scaly skin and other soft tissues among such fossils as well. But researchers often assumed that a dinosaur body would have to be rapidly buried in sediment very close to death in order to preserve such fine details. As experts have taken a deeper look, however, it seems that dinosaur mummies may not have required a quick burial, after all. In fact, laying exposed on the surface for weeks to months after death might have helped some of these unique fossils form. In 2022, Clint Boyd, Stephanie Drumheller-Horton, and their colleagues proposed that the exceptional Edmontosaurus mummy “Dakota” had lain exposed on the surface long enough for scavengers to take a few bites and that this long exposure helped preserve it. Scavenging animals and the decay process left holes in the dinosaur’s body, releasing gases, fluids, and microbes that allowed the tougher, scaly skin and front “hoof” to dry out and persist alongside the bone until sediment eventually covered the carcass. The new Wyoming mummies suggest yet another pathway to exceptional preservation. The spiky scales and back hoof-like features of the fossils described by Sereno and colleagues are made up of a thin clay layer sandwiched between harder rinds of sandstone. The thin layer of clay adhered to the dinosaur’s body as it decayed, taking the shape of the soft tissue and creating a natural cast of the hadrosaurs’ bodies.  “It's a 100th of an inch thick clay layer,” says Sereno, who is also a National Geographic Explorer. It’s so thin that it would be easy to miss in the excavation and preparation process. “You're working in a lot of dust,” he says. “You don't want to take extra, or you’d just completely miss it. You’d blow it away.”  (Here's how a group of miners found a nodosaur fossil in Canada.) Edmontosaurus is far from the only dinosaur to be preserved with remnants of skin and skin impressions. Paleontologists have found skin impressions associated with other hadrosaurs like Saurolophus and Gryposaurus, as well as tyrannosaurs, horned dinosaurs, sauropod dinosaurs, and others. Not all of these specimens contained enough skin fossils to qualify as mummies, and the most extensive skin preservation is often found among hadrosaurs. “You’re very likely to find at least a patch of a skin impression any time you find articulated hadrosaur bones, no need of complete skeletons,” says Royal Tyrrell Museum of Paleontology researcher François Therrien. Even though dinosaur mummies have been known for a long time, it’s only been relatively recently that paleontologists have understood that such detailed fossils are more common than previously assumed. “Unfortunately, we’ve seen several examples of fossils that seem to have been mummies but were not recognized as such fast enough, so much of the skin was actually prepared away” by experts removing rock from around the bones, says Drumheller-Horton, a paleontologist the University of Tennessee, Knoxville. What paleontologists look for and expect in the fossil record has a role to play in how many dinosaur mummies turn up. For the moment, at least, hadrosaurs have some of the most extensive and best-known examples of skin preservation. “My thoughts on why so many hadrosaurs have skin preserved is that it’s a bit of a numbers game,” says Boyd, a paleontologist at North Dakota State University. Hadrosaurs like Edmontosaurus were among the most numerous dinosaurs in their environments. More hadrosaur carcasses meant more opportunities for them to undergo the conditions necessary for mummies to form. “Large animals are also more likely to be preserved at least partially articulated,” he says, bones and skin held together instead of being pulled apart and scattered. Dinosaur mummies and other soft tissue fossils help paleontologists and paleoartists envision what the living animals once looked like as they moved around the Mesozoic world. But that’s hardly all. As experts have cataloged various examples of dinosaur mummies and soft tissue fossils, they’ve begun to pick up on some unexpected facets of dinosaur lives. Along with existing footprints, the hoof-like structures seen in both the Wyoming and Dakota E. annectens mummies help scientists hypothesize how the dinosaurs moved based on the slight differences between its front and back feet. “This is an animal that decides to run on two legs and walk on four,” says Sereno. Extensive skin impressions from two species of the duckbilled dinosaur Saurolophus have settled debates over whether the two skeletally similar dinosaurs were different species. When University of New England paleontologist Phil Bell compared the skin impressions of the Saurolophus angustirostris found in Mongolia with the other Saurolophus osborni found in Canada in a 2012 study, he found that each had distinctive scale patterns on their bodies. S. angustirostris had vertical bands of scales along the tail and large, bumpy scales along the midline of the back, while S. osborni lacked these ornate specializations. The scale patterns may have helped dinosaurs identify members of their own species. Scales can reveal dinosaur coloration, as well. First established by comparing the microscopic details of dinosaur feathers to those of modern birds, paleontologists have been able to reconstruct the hues ancient dinosaurs were. The same principles hold for exceptionally-preserved scales. Take the  the small, horned dinosaur Psittacosaurus. A 2016 study of the dinosaur’s skin found that it had darker scales above and lighter scales beneath. This form of camouflage is known as countershading. If spotted from above, the dark colors on the dinosaur’s back would have helped it blend in with the forest habitat. A study of the spiky, armored dinosaur Borealopelta published the following year found a similar pattern. The dinosaur was reddish above and lighter below, which would have helped the dinosaur hide from tyrannosaurs in the ancient forest.  (Read more about how soft tissue finds have given dinosaurs a revamp.) Even as paleontologists uncover more dinosaur soft tissues in the field and in museum collections, experts are only just beginning to understand how such fossils formed. “There’s just so much amazing research happening now on the different pathways that an organism can take to become a fossil,” Drumheller-Horton says, from researchers looking to how organisms decompose to experts studying the chemical changes dinosaur bodies must have undergone as they became fossilized. At this point, it’s unclear which mummy-making process is most common. Sereno calls the rock layers in Wyoming that yielded the first dinosaur mummies and the two new E. annectens fossils the “mummy zone.” Two additional tyrannosaur and Triceratops mummies have been found in the same area, and he suspects that more examples of clay-mask mummies could be buried there. Only further investigations will confirm whether clay can preserve skin renderings at other sites, too. For Boyd, the chemical process that allows skin impressions and soft tissues to fossilize alongside bones is a tantalizing question. “We know why bones fossilizes, since it already has a natural mineral framework that promotes fossilization,” he says, “but these dinosaur mummies typically lack mineralization in their skin and so would require a different pathway to fossilize.” Working out how skin fossilizes would not only solve that mystery, but help paleontologists better look out for fossils that have the potential to preserve soft tissues. Through understanding the making of dinosaur mummies, paleontologists will unveil even more. Copyright © 1996-2015 National Geographic SocietyCopyright © 2015-2025 National Geographic Partners, LLC. All rights reserved",
        "onlytext": "A new study sheds light on how these reptiles become “mummies” and paints a picture of what these ancient animals looked like. The bulk of what we know from dinosaurs comes from bones. Skeletal parts can reveal a dinosaur’s size, how fast they grew, and even if they were gravid with eggs when they perished, but all the ribs, jaws, and other pieces can only go so far in revealing what our favorite dinosaurs looked like in life. Fortunately, paleontologists have found some dinosaur bones are preserved with soft tissues like feathers, claw sheaths, and, with luck, entire dinosaur “mummies.” These aren’t traditional mummies purposefully preserved through chemical embalming. Instead, they’re fossils that preserve skin impressions as a thin layer within the rock. As paleontologists have continued to sift through the fossil record, such specimens are finally offering some insights into the external appearance of familiar species. And through untangling the backstories of these fossils, experts are beginning to realize that such cases of exceptional preservation might not be as rare as once believed.\n\nOne familiar face getting fleshed out is Edmontosaurus annectens. The duckbilled dinosaur walked the floodplains of ancient Montana and Saskatchewan 68 to 66 million years ago, and today it’s one of the best-known and most common fossils found in the region. Several Edmontosaurus mummies have revealed soft tissue structures that paleontologists never expected to see, including a pair of mummies uncovered in Wyoming described October 23 in Science. Some of the preserved features on the new E. annectens specimens, a juvenile and an adult, confirm what paleontologists predicted. The toes on the back feet of the dinosaurs, for example, had a tough coating on them, like a hoof or a toenail. A row of overlapping spiky scales also ran down their backs that connected to their spines, a feature that paleontologists had not seen before in its entirety. “This is sort of dragon-like,” says Paul Sereno, a paleontologist at the University of Chicago and a coauthor on the new paper.\n\nThe findings add to previous research that has revealed Edmontosaurus had a fleshy comb on their heads, front toes wrapped together in a mitt of flesh tipped with a hoof-like covering, and rough, shovel-like beaks jutting down from their snouts. The newly-described fossils follow a long history of Edmontosaurus mummies found in western North America, including the first dinosaur mummy discovered in 1908. Scientists began calling these early specimen mummies because of the extensive skin impressions left in the rock that surrounded their skeletons. Since that time, paleontologists have uncovered scaly skin and other soft tissues among such fossils as well.\n\nBut researchers often assumed that a dinosaur body would have to be rapidly buried in sediment very close to death in order to preserve such fine details. As experts have taken a deeper look, however, it seems that dinosaur mummies may not have required a quick burial, after all. In fact, laying exposed on the surface for weeks to months after death might have helped some of these unique fossils form. In 2022, Clint Boyd, Stephanie Drumheller-Horton, and their colleagues proposed that the exceptional Edmontosaurus mummy “Dakota” had lain exposed on the surface long enough for scavengers to take a few bites and that this long exposure helped preserve it. Scavenging animals and the decay process left holes in the dinosaur’s body, releasing gases, fluids, and microbes that allowed the tougher, scaly skin and front “hoof” to dry out and persist alongside the bone until sediment eventually covered the carcass.\n\nThe new Wyoming mummies suggest yet another pathway to exceptional preservation. The spiky scales and back hoof-like features of the fossils described by Sereno and colleagues are made up of a thin clay layer sandwiched between harder rinds of sandstone. The thin layer of clay adhered to the dinosaur’s body as it decayed, taking the shape of the soft tissue and creating a natural cast of the hadrosaurs’ bodies.\n\n“It’s a 100th of an inch thick clay layer,” says Sereno, who is also a National Geographic Explorer. It’s so thin that it would be easy to miss in the excavation and preparation process. “You’re working in a lot of dust,” he says. “You don’t want to take extra, or you’d just completely miss it. You’d blow it away.”\n\nEdmontosaurus is far from the only dinosaur to be preserved with remnants of skin and skin impressions. Paleontologists have found skin impressions associated with other hadrosaurs like Saurolophus and Gryposaurus, as well as tyrannosaurs, horned dinosaurs, sauropod dinosaurs, and others. Not all of these specimens contained enough skin fossils to qualify as mummies, and the most extensive skin preservation is often found among hadrosaurs. “You’re very likely to find at least a patch of a skin impression any time you find articulated hadrosaur bones, no need of complete skeletons,” says Royal Tyrrell Museum of Paleontology researcher François Therrien.\n\nEven though dinosaur mummies have been known for a long time, it’s only been relatively recently that paleontologists have understood that such detailed fossils are more common than previously assumed. “Unfortunately, we’ve seen several examples of fossils that seem to have been mummies but were not recognized as such fast enough, so much of the skin was actually prepared away” by experts removing rock from around the bones, says Drumheller-Horton, a paleontologist the University of Tennessee, Knoxville.\n\nWhat paleontologists look for and expect in the fossil record has a role to play in how many dinosaur mummies turn up. For the moment, at least, hadrosaurs have some of the most extensive and best-known examples of skin preservation. “My thoughts on why so many hadrosaurs have skin preserved is that it’s a bit of a numbers game,” says Boyd, a paleontologist at North Dakota State University. Hadrosaurs like Edmontosaurus were among the most numerous dinosaurs in their environments. More hadrosaur carcasses meant more opportunities for them to undergo the conditions necessary for mummies to form. “Large animals are also more likely to be preserved at least partially articulated,” he says, bones and skin held together instead of being pulled apart and scattered.\n\nDinosaur mummies and other soft tissue fossils help paleontologists and paleoartists envision what the living animals once looked like as they moved around the Mesozoic world. But that’s hardly all. As experts have cataloged various examples of dinosaur mummies and soft tissue fossils, they’ve begun to pick up on some unexpected facets of dinosaur lives. Along with existing footprints, the hoof-like structures seen in both the Wyoming and Dakota E. annectens mummies help scientists hypothesize how the dinosaurs moved based on the slight differences between its front and back feet. “This is an animal that decides to run on two legs and walk on four,” says Sereno.\n\nExtensive skin impressions from two species of the duckbilled dinosaur Saurolophus have settled debates over whether the two skeletally similar dinosaurs were different species. When University of New England paleontologist Phil Bell compared the skin impressions of the Saurolophus angustirostris found in Mongolia with the other Saurolophus osborni found in Canada in a 2012 study, he found that each had distinctive scale patterns on their bodies. S. angustirostris had vertical bands of scales along the tail and large, bumpy scales along the midline of the back, while S. osborni lacked these ornate specializations.\n\nThe scale patterns may have helped dinosaurs identify members of their own species. Scales can reveal dinosaur coloration, as well. First established by comparing the microscopic details of dinosaur feathers to those of modern birds, paleontologists have been able to reconstruct the hues ancient dinosaurs were. The same principles hold for exceptionally-preserved scales. Take the small, horned dinosaur Psittacosaurus. A 2016 study of the dinosaur’s skin found that it had darker scales above and lighter scales beneath. This form of camouflage is known as countershading. If spotted from above, the dark colors on the dinosaur’s back would have helped it blend in with the forest habitat. A study of the spiky, armored dinosaur Borealopelta published the following year found a similar pattern. The dinosaur was reddish above and lighter below, which would have helped the dinosaur hide from tyrannosaurs in the ancient forest.\n\nEven as paleontologists uncover more dinosaur soft tissues in the field and in museum collections, experts are only just beginning to understand how such fossils formed. “There’s just so much amazing research happening now on the different pathways that an organism can take to become a fossil,” Drumheller-Horton says, from researchers looking to how organisms decompose to experts studying the chemical changes dinosaur bodies must have undergone as they became fossilized. At this point, it’s unclear which mummy-making process is most common.\n\nSereno calls the rock layers in Wyoming that yielded the first dinosaur mummies and the two new E. annectens fossils the “mummy zone.” Two additional tyrannosaur and Triceratops mummies have been found in the same area, and he suspects that more examples of clay-mask mummies could be buried there. Only further investigations will confirm whether clay can preserve skin renderings at other sites, too.\n\nFor Boyd, the chemical process that allows skin impressions and soft tissues to fossilize alongside bones is a tantalizing question. “We know why bones fossilizes, since it already has a natural mineral framework that promotes fossilization,” he says, “but these dinosaur mummies typically lack mineralization in their skin and so would require a different pathway to fossilize.” Working out how skin fossilizes would not only solve that mystery, but help paleontologists better look out for fossils that have the potential to preserve soft tissues. Through understanding the making of dinosaur mummies, paleontologists will unveil even more."
    },
    {
        "url": "https://www.nationalgeographic.com/science/article/did-this-ancient-croc-hunt-dinosaurs-on-land",
        "text": "The new fossil was unearthed in a remote part of Argentinian Patagonia—and had teeth comparable to a T. rex, says National Geographic Explorer Diego Pol. Near the dusk of the dinosaurs, some 70 million years ago, an ancient crocodile prowled the floodplains of what is now the southern tip of Patagonia, its sinister smile lined with more than fifty sharp, serrated teeth. Named Kostensuchus atrox, this extinct species of croc was a hypercarnivore, meaning it feasted almost exclusively on meat. It was also an apex predator that had “teeth that are comparable to a T. rex,” conical and knife-like, says Diego Pol, a paleontologist and National Geographic Explorer who helped discover the new species. With its massive jaw muscles built for tearing apart flesh, Pol says, Kostensuchus “can break you in two pieces with a single bite.” He and his colleagues at the Museo Argentino de Ciencias Naturales Bernardino Rivadavia in Buenos Aires, along with researchers from Brazil and Japan, described the species’ exquisitely well-preserved skull and partial skeleton on August 27 in the journal PLOS One. Though it’s smaller than the largest crocodiles and alligators found today, the team thinks Kostensuchus may have had longer, more upright limbs than its modern counterparts, which they say would have helped it hunt prey—including dinosaurs—on land. But some researchers not involved in the study aren't convinced. Kostensuchus’ discovery in southern Patagonia, close to Antarctica, indicates that ancient relatives of crocodiles in the Cretaceous thrived on land and at high latitudes in warm, humid environments that are now often buried beneath snow and ice.  “It gives us an idea of how dramatically the climate has changed since then,” says Pol.  (Read more about some of Patagonia’s other recent fossil finds.) Today’s crocodiles and their many living and extinct relatives are part of a broad group called crocodyliforms. Within that group Kostensuchus atrox belonged to an extinct family called peirosaurids, distant cousins of modern crocodiles, alligators, gharials, and caimans, but not their direct ancestor. (Crocodiles Body Surf to Hop Between Islands) These extinct croc cousins, many of whom lived on land, perished in the same mass extinction event 66 million years ago that killed all non-avian dinosaurs. Kostensuchus atrox is the latest-known peirosaurid in the fossil record, as well as the southernmost ever-found. It is also one of the best-preserved and most complete peirosaurids discovered thus far. (Ancient Arboreal Mammal Discovered at Root of Carnivore Family Tree) “This is a really beautiful fossil from a very unusual, poorly-understood part of the croc family tree,” says Stephanie Drumheller-Horton, a vertebrate paleontologist from the University of Tennessee, Knoxville who was not involved in the work. In early March 2020, museum paleontologists Fernando Novas and Marcelo Isasi set out for Argentina’s Chorrillo Formation to search for the bones of dinosaurs and other ancient creatures. In this same area, they and their colleagues had previously found the megaraptor Maip macrothorax, the titanosaur Nullotitan glaciaris, and the ornithopod Isasicursor santacrucensis (which bears Isasi’s name). “The Chorrillo bedrocks constitute a formidable ‘window’ into an ancient ecosystem which may offer novel clues about the last dinosaurs in Patagonia,” says Novas. Perched on a hill, the rocks at the site date to about 70-million years ago. To get there, they drove trucks three hours across rivers, up slopes, and on the edge of cliffs with scenic views of Lake Argentino and the Perito Moreno Glacier emerging from the Andes.  After setting up camp on the first day of the field expedition, the team of more than 30 researchers and technicians walked several miles through the mountains carrying heavy scanning equipment and machines but didn’t spot much. “Then, with the last rays of sun, our luck changed,” says Isasi.  While the rest of the team went back to camp, he stayed behind with his colleague Gabriel Lio, a paleoartist, to wait for a team member who hadn’t checked in. As time went on, they began to worry and decided to search for him. In the distance, they heard a shout from their missing colleague. Feeling a sense of relief, Isasi started to slow his pace as he walked through a spot with large rocks and calcium concretions. There, embedded within a beige rock something caught Isasi’s eye: black bones. “I showed it to my colleague, who, astonished, said, ‘Marcelo, those are teeth, and they're very big!’” says Isasi. “I immediately looked up and saw a break in the rock with an unmistakable silhouette, and I said, ‘That's a skull!’” Lio, an expert on fossilized crocodiles, immediately identified the skull as belonging to a croc. In addition to the skull, they also excavated from within the rocks the reptile’s partial skeleton, which looked larger and much more complete than any they had seen before.   “It was a marvelous, incredible, and unforgettable moment!” says Isasi. While the team was camping, the COVID-19 pandemic sent the world into lockdowns. With only a weak cell signal reaching them on the mountain ridge, they had little idea what was happening in the rest of the world. Then on March 24 they had to suspend their field work. They attempted to return to a nearby city called El Calafate, but it was under lockdown. They had to then spend ten days confined inside cabins. Finally, Novas secured permits for the team to travel from the site, so the researchers loaded the fossils into trucks. They transported their bounty some 1,500 miles from southern Patagonia to Buenos Aires on an empty road. “I remember the line of trucks bringing explorers, camping gear and valuable new fossils traveling alone as in a science fiction movie,” says Novas.  In Buenos Aires, social distancing made the museum’s lab forbidden territory, so Novas asked Isasi to bring the heavy fossil block with the skull home. It was so large that Isasi had to recruit the help of his wife, Emilia, and two children, Tomás and Lara, to drag it out of his vehicle and into his patio. He dedicated the next six months to chipping away the rock with high-precision pneumatic hammers. In time the large, black and shining teeth emerged menacingly from under the snout, Novas says.  The skull was short, measuring about 20 inches long, and its snout was broad. Snaggleteeth jutting from its mouth measured up to two inches long and, relative to its skull, were larger than those of modern crocodiles and most other extinct crocs. Its skull and lower jaw suggest it had high muscle insertions that would have produced a powerful bite. These clues, the researchers say, support Kostensuchus’ status as a hypercarnivore and apex predator. It likely hunted small and midsized herbivores, possibly ornithischians, like small ankylosaurs and hadrosaurs. Following many months of meticulous work by technicians, the Kostensuchus atrox bones finally emerged from the rocks. “It's a very slow process of discovery,” filled with drama and tension, says Pol. “I love that part, because more and more surprises appear every day in the lab.” Once finished, the team had also uncovered its neck, back, hip bones, ribs, and parts of a forelimb. Missing were the tail and most of the hindlimbs. Compared to modern crocs, Kostensuchus had a longer forelimb, hinting that it may have held itself upright and walked on land rather than spent most of its time in the water. Eric Willberg,  a vertebrate paleontologist from Stony Brook University in New York who wasn’t part of the team, says the terrestrial evidence is mixed. The croc’s pelvis, he says, suggests the hindlimb may have sprawled outward, like modern crocodiles, rather than fully upright like in mammals and other extinct terrestrial crocodyliforms. The forelimb traits could alternatively reflect an adaptation for seizing and tearing prey rather than for moving around, Pol and his colleagues acknowledge. But without the hindlimb bones it’s hard to reconstruct how it moved and subdued its prey, notes Adam Cossette, a vertebrate paleontologist at the New York Institute of Technology. The authors estimate that Kostensuchus measured 11.5 feet long and weighed about 550 pounds by extrapolating from modern caimans and alligators. Though the specimen is “truly impressive,” Pedro Godoy, a vertebrate paleontologist from the University of São Paulo in Brazil, notes that these living species may have different body proportions from what extinct terrestrial crocodyliforms had. (This ancient croc was bigger than most dinosaurs.) The backbones of Kostensuchus showed evidence of healed fractures, says Pol, which may support their idea that the crocs fought each other. “Combat or aggression due to food or territorial [behaviors] is one of the possible scenarios for getting injured,” says Pol. To imagine how such clashes might have unfolded between the Cretaceous crocs, the team created a video reconstruction. In it a Kostensuchus gnaws at the carcass of a fresh dinosaur kill when a rival approaches, possibly drawn by the smell. The two exchange snarls before rearing onto their hindlimbs and slamming their massive bodies against each other, like how Komodo dragons duke it out. Though the fossil bed where Kostensuchus was found dates to 70 million years ago, Pol thinks Kostensuchus lived even closer in time to the extinction event. If so, Kostensuchus provides scientists with further insight into the extraordinary diversity of Cretaceous crocodyliforms right before they were wiped out.  People typically think that ancient crocs all resembled the low-slung, semi-aquatic ambush predators lurking in murky waters today. But during the Cretaceous and earlier there were crocs that were herbivores, giant carnivores and even had armor like an armadillo. “They were very diverse until the very end,” says Pol. He suspects the reason we no longer see this same diversity among crocs today is because ancient crocs who survived the extinction event were most likely those adapted to living in freshwater environments like rivers and lakes, which were less devastated in the aftermath of the asteroid impact. “There’s this persistent idea that crocs are ‘living fossils,’ but it doesn’t take a lot of digging to find a lot of fun, weird species,” says Drumheller-Horton. “It’s also a great reminder that even though we call it the ‘Age of Dinosaurs,’ dinosaurs weren’t the only game in town.”  Copyright © 1996-2015 National Geographic SocietyCopyright © 2015-2025 National Geographic Partners, LLC. All rights reserved",
        "onlytext": "Near the dusk of the dinosaurs, some 70 million years ago, an ancient crocodile prowled the floodplains of what is now the southern tip of Patagonia, its sinister smile lined with more than fifty sharp, serrated teeth. Named Kostensuchus atrox, this extinct species of croc was a hypercarnivore, meaning it feasted almost exclusively on meat. It was also an apex predator that had “teeth that are comparable to a T. rex,” conical and knife-like, says Diego Pol, a paleontologist and National Geographic Explorer who helped discover the new species. With its massive jaw muscles built for tearing apart flesh, Pol says, Kostensuchus “can break you in two pieces with a single bite.” \n\nHe and his colleagues at the Museo Argentino de Ciencias Naturales Bernardino Rivadavia in Buenos Aires, along with researchers from Brazil and Japan, described the species’ exquisitely well-preserved skull and partial skeleton on August 27 in the journal PLOS One. Though it’s smaller than the largest crocodiles and alligators found today, the team thinks Kostensuchus may have had longer, more upright limbs than its modern counterparts, which they say would have helped it hunt prey—including dinosaurs—on land. But some researchers not involved in the study aren't convinced. \n\nKostensuchus’ discovery in southern Patagonia, close to Antarctica, indicates that ancient relatives of crocodiles in the Cretaceous thrived on land and at high latitudes in warm, humid environments that are now often buried beneath snow and ice.  “It gives us an idea of how dramatically the climate has changed since then,” says Pol. \n\nToday’s crocodiles and their many living and extinct relatives are part of a broad group called crocodyliforms. Within that group Kostensuchus atrox belonged to an extinct family called peirosaurids, distant cousins of modern crocodiles, alligators, gharials, and caimans, but not their direct ancestor. \n\nThese extinct croc cousins, many of whom lived on land, perished in the same mass extinction event 66 million years ago that killed all non-avian dinosaurs. Kostensuchus atrox is the latest-known peirosaurid in the fossil record, as well as the southernmost ever-found. It is also one of the best-preserved and most complete peirosaurids discovered thus far.\n\n“This is a really beautiful fossil from a very unusual, poorly-understood part of the croc family tree,” says Stephanie Drumheller-Horton, a vertebrate paleontologist from the University of Tennessee, Knoxville who was not involved in the work.\n\nThe team had previously found the megaraptor Maip macrothorax, the titanosaur Nullotitan glaciaris, and the ornithopod Isasicursor santacrucensis (which bears Isasi’s name). “The Chorrillo bedrocks constitute a formidable ‘window’ into an ancient ecosystem which may offer novel clues about the last dinosaurs in Patagonia,” says Novas.\n\nIn early March 2020, museum paleontologists Fernando Novas and Marcelo Isasi set out for Argentina’s Chorrillo Formation to search for the bones of dinosaurs and other ancient creatures. \n\nAfter setting up camp on the first day of the field expedition, the team of more than 30 researchers and technicians walked several miles through the mountains carrying heavy scanning equipment and machines but didn’t spot much. “Then, with the last rays of sun, our luck changed,” says Isasi.  While the rest of the team went back to camp, he stayed behind with his colleague Gabriel Lio, a paleoartist, to wait for a team member who hadn’t checked in. As time went on, they began to worry and decided to search for him. In the distance, they heard a shout from their missing colleague. Feeling a sense of relief, Isasi started to slow his pace as he walked through a spot with large rocks and calcium concretions. There, embedded within a beige rock something caught Isasi’s eye: black bones. “I showed it to my colleague, who, astonished, said, ‘Marcelo, those are teeth, and they're very big!’” says Isasi. “I immediately looked up and saw a break in the rock with an unmistakable silhouette, and I said, ‘That's a skull!’” \n\nIn addition to the skull, they also excavated from within the rocks the reptile’s partial skeleton, which looked larger and much more complete than any they had seen before.   “It was a marvelous, incredible, and unforgettable moment!” says Isasi.\n\nThe skull was short, measuring about 20 inches long, and its snout was broad. Snaggleteeth jutting from its mouth measured up to two inches long and, relative to its skull, were larger than those of modern crocodiles and most other extinct crocs. Its skull and lower jaw suggest it had high muscle insertions that would have produced a powerful bite. These clues, the researchers say, support Kostensuchus’ status as a hypercarnivore and apex predator. It likely hunted small and midsized herbivores, possibly ornithischians, like small ankylosaurs and hadrosaurs.\n\nFollowing many months of meticulous work by technicians, the Kostensuchus atrox bones finally emerged from the rocks. “It's a very slow process of discovery,” filled with drama and tension, says Pol. “I love that part, because more and more surprises appear every day in the lab.” Once finished, the team had also uncovered its neck, back, hip bones, ribs, and parts of a forelimb. Missing were the tail and most of the hindlimbs. \n\nCompared to modern crocs, Kostensuchus had a longer forelimb, hinting that it may have held itself upright and walked on land rather than spent most of its time in the water. \n\nThe authors estimate that Kostensuchus measured 11.5 feet long and weighed about 550 pounds by extrapolating from modern caimans and alligators. \n\nThe backbones of Kostensuchus showed evidence of healed fractures, says Pol, which may support their idea that the crocs fought each other. “Combat or aggression due to food or territorial [behaviors] is one of the possible scenarios for getting injured,” says Pol. \n\nTo imagine how such clashes might have unfolded between the Cretaceous crocs, the team created a video reconstruction. In it a Kostensuchus gnaws at the carcass of a fresh dinosaur kill when a rival approaches, possibly drawn by the smell. The two exchange snarls before rearing onto their hindlimbs and slamming their massive bodies against each other, like how Komodo dragons duke it out.\n\nThough the fossil bed where Kostensuchus was found dates to 70 million years ago, Pol thinks Kostensuchus lived even closer in time to the extinction event. If so, Kostensuchus provides scientists with further insight into the extraordinary diversity of Cretaceous crocodyliforms right before they were wiped out. \n\nPeople typically think that ancient crocs all resembled the low-slung, semi-aquatic ambush predators lurking in murky waters today. But during the Cretaceous and earlier there were crocs that were herbivores, giant carnivores and even had armor like an armadillo. “They were very diverse until the very end,” says Pol. \n\nHe suspects the reason we no longer see this same diversity among crocs today is because ancient crocs who survived the extinction event were most likely those adapted to living in freshwater environments like rivers and lakes, which were less devastated in the aftermath of the asteroid impact. “There’s this persistent idea that crocs are ‘living fossils,’ but it doesn’t take a lot of digging to find a lot of fun, weird species,” says Drumheller-Horton. “It’s also a great reminder that even though we call it the ‘Age of Dinosaurs,’ dinosaurs weren’t the only game in town.”"
    },
    {
        "url": "https://www.nationalgeographic.com/animals/article/wolf-uses-crab-trap-like-tools",
        "text": "While researchers are divided on the conclusion, the behavior shows wolves may be smarter than we thought. In 2023, crab traps along the coast of British Columbia started showing mysterious damage. The Heiltsuk (Haíɫzaqv) Nation, an Indigenous group, had set them up to capture destructive, invasive European green crabs. Sometimes the traps were destroyed. Other times their nets had been torn. Whoever raided the traps always went after the plastic bait cups, which held bits of herring or sea lion carcass.  It looked like the work of wolves or bears, but many of the compromised traps stayed submerged during the low tide, so maybe sea otters?  To find out, scientists, in partnership with the Heiltsuk Guardians, a group that monitors the territory and conducts their own research, set up a camera pointed at an underwater crab trap in May 2024. The camera captured a wolf nabbing bait from a crab trap. In a new study describing this behavior in the journal Ecology and Evolution, the researchers suggest that the incident might be the first reported tool use by a wild wolf.  The footage shows a female wolf stalking out of the water and holding a buoy in her mouth. Stepping backwards, she walks on the rocky beach and sets down the buoy. Then she pads back into the water and grasps the rope extending from the buoy with her teeth to draw an unseen object closer to shore. Once the trap—a cone formed of a metal frame covered in netting—is exposed, she grabs it in her mouth and pulls into the shallows. Nosing through and chewing the netting, she frees the bait cup, scarfs down her snack and struts away.   From “the very first watch of the video, it was like—from my interpretation—this is tool use,” says Kyle Artelle, a study co-author and an ecologist at SUNY College of Environmental Science and Forestry in Syracuse, New York who was part of the team that set the traps. “Every motion is perfectly efficient,” and the animal seemed to know the connection between the trap’s parts. In another camera trap video, a different wolf is seen tugging a line attached to a buoy, and dozens if not hundreds of crab traps have been similarly damaged in the area.  People have seen canids, the group that includes dogs and their kin, using tools in captivity. In 2012, ethologist Bradley Smith of Central Queensland University in Adelaide, Australia, and his colleagues shared observations of a captive dingo dragging a table some 6 feet and then climbing onto it and grabbing an object that had been out of reach. The same dingo also moved a dog crate and stood on it, allowing him to see out of his enclosure.  “I couldn’t believe it,” says Smith, who wasn’t involved with the new work, referring to those early dingo observations. At the time, the list of animals known to be capable of higher-order tasks—going beyond using their instincts and simple reactions— was short, including primates, dolphins, elephants and crows. The dingo findings opened up new knowledge on canid capabilities.  (5 clever animals that treat and prevent their own illnesses) This new study shows how adaptable and clever wild wolves are, he says. While the dingoes lived in a sanctuary, the wolf behavior was spotted in the wild, “so that makes the wolf discovery more special and valuable,” he says. But there is disagreement in the scientific community about whether the wolf’s crab trap hack counts as tool use.  Some argue that the creature has to fashion the tool itself for it to qualify as “tool use.” But humans use tools, such as computers, that they don’t personally build, Artelle says.  One definition of tool use is using an object to achieve a goal. By that logic this encounter with the trap counts, though it’s “not an advanced example of tool use,” Smith says.  But that definition doesn't capture all dimensions of tool use, says Robert Shumaker, an evolutionary biologist at the Indianapolis Zoo, who wasn’t part of the new study. He says that the most widely accepted definition includes several criteria for tool use.The wolf observation doesn’t fulfil every requirement because the animal did not connect the rope to the trap or arrange it some way connected to getting food. “Just pulling on something that someone else arranged is not tool use,” Shumaker says.   Regardless of bona fide “tool use,” the observation of an unusual foraging technique is important for understanding wolf mental flexibility and complexity, Shumaker says. And qualifying as tool-use doesn’t make a behavior more sophisticated or a bigger accomplishment, he says. Smith agrees that the value of this observation doesn’t come from how it fits with researchers’ definitions of tool use. “This shouldn’t detract from this being impressive and being a clear example of higher-order problem solving and thinking,” Smith says. The event shows goal-directed planning, understanding of a hidden reward, multi-step problem solving and persistence—though not all wolves may be as capable as this one.  (Dolphins learn how to use tools from peers, just like great apes) This particular environment may help explain the curious behavior, Artelle says. Wolves in this part of British Columbia don’t experience much persecution from people. That might allow them free time on the beach to experiment with new behaviors, Artelle says. He and others are continuing to study wolves’ distribution, their behavior, and their roles in ecosystems through the ongoing Heiltsuk Wolf and Biodiversity Project. The Heiltsuk Nation has lived among wolves for thousands of years, says William Housty, a hereditary chief in the Heiltsuk Nation and director of the Heiltsuk Integrated Resource Management Department, a group tasked with stewardship of the territory. He wasn’t part of this study, but collaborates with Artelle in other capacities.   “From a traditional perspective, we’ve always known that wolves are very intelligent beings,” Housty says. The discovery opens up new questions about what wolves are capable of. Copyright © 1996-2015 National Geographic SocietyCopyright © 2015-2025 National Geographic Partners, LLC. All rights reserved",
        "onlytext": "While researchers are divided on the conclusion, the behavior shows wolves may be smarter than we thought. In 2023, crab traps along the coast of British Columbia started showing mysterious damage. The Heiltsuk (Haíɫzaqv) Nation, an Indigenous group, had set them up to capture destructive, invasive European green crabs. Sometimes the traps were destroyed. Other times their nets had been torn. Whoever raided the traps always went after the plastic bait cups, which held bits of herring or sea lion carcass. \n\nIt looked like the work of wolves or bears, but many of the compromised traps stayed submerged during the low tide, so maybe sea otters? \n\nTo find out, scientists, in partnership with the Heiltsuk Guardians, a group that monitors the territory and conducts their own research, set up a camera pointed at an underwater crab trap in May 2024. The camera captured a wolf nabbing bait from a crab trap. In a new study describing this behavior in the journal Ecology and Evolution, the researchers suggest that the incident might be the first reported tool use by a wild wolf.\n\nThe footage shows a female wolf stalking out of the water and holding a buoy in her mouth. Stepping backwards, she walks on the rocky beach and sets down the buoy. Then she pads back into the water and grasps the rope extending from the buoy with her teeth to draw an unseen object closer to shore. Once the trap—a cone formed of a metal frame covered in netting—is exposed, she grabs it in her mouth and pulls into the shallows. Nosing through and chewing the netting, she frees the bait cup, scarfs down her snack and struts away.\n\nFrom “the very first watch of the video, it was like—from my interpretation—this is tool use,” says Kyle Artelle, a study co-author and an ecologist at SUNY College of Environmental Science and Forestry in Syracuse, New York who was part of the team that set the traps. “Every motion is perfectly efficient,” and the animal seemed to know the connection between the trap’s parts. In another camera trap video, a different wolf is seen tugging a line attached to a buoy, and dozens if not hundreds of crab traps have been similarly damaged in the area.\n\nPeople have seen canids, the group that includes dogs and their kin, using tools in captivity. In 2012, ethologist Bradley Smith of Central Queensland University in Adelaide, Australia, and his colleagues shared observations of a captive dingo dragging a table some 6 feet and then climbing onto it and grabbing an object that had been out of reach. The same dingo also moved a dog crate and stood on it, allowing him to see out of his enclosure.\n\n“I couldn’t believe it,” says Smith, who wasn’t involved with the new work, referring to those early dingo observations. At the time, the list of animals known to be capable of higher-order tasks—going beyond using their instincts and simple reactions— was short, including primates, dolphins, elephants and crows. The dingo findings opened up new knowledge on canid capabilities.\n\nThis new study shows how adaptable and clever wild wolves are, he says. While the dingoes lived in a sanctuary, the wolf behavior was spotted in the wild, “so that makes the wolf discovery more special and valuable,” he says. But there is disagreement in the scientific community about whether the wolf’s crab trap hack counts as tool use.\n\nSome argue that the creature has to fashion the tool itself for it to qualify as “tool use.” But humans use tools, such as computers, that they don’t personally build, Artelle says.\n\nOne definition of tool use is using an object to achieve a goal. By that logic this encounter with the trap counts, though it’s “not an advanced example of tool use,” Smith says.\n\nBut that definition doesn't capture all dimensions of tool use, says Robert Shumaker, an evolutionary biologist at the Indianapolis Zoo, who wasn’t part of the new study. He says that the most widely accepted definition includes several criteria for tool use.The wolf observation doesn’t fulfil every requirement because the animal did not connect the rope to the trap or arrange it some way connected to getting food. “Just pulling on something that someone else arranged is not tool use,” Shumaker says.\n\nRegardless of bona fide “tool use,” the observation of an unusual foraging technique is important for understanding wolf mental flexibility and complexity, Shumaker says. And qualifying as tool-use doesn’t make a behavior more sophisticated or a bigger accomplishment, he says. Smith agrees that the value of this observation doesn’t come from how it fits with researchers’ definitions of tool use. “This shouldn’t detract from this being impressive and being a clear example of higher-order problem solving and thinking,” Smith says. The event shows goal-directed planning, understanding of a hidden reward, multi-step problem solving and persistence—though not all wolves may be as capable as this one.\n\nThis particular environment may help explain the curious behavior, Artelle says. Wolves in this part of British Columbia don’t experience much persecution from people. That might allow them free time on the beach to experiment with new behaviors, Artelle says. He and others are continuing to study wolves’ distribution, their behavior, and their roles in ecosystems through the ongoing Heiltsuk Wolf and Biodiversity Project.\n\nThe Heiltsuk Nation has lived among wolves for thousands of years, says William Housty, a hereditary chief in the Heiltsuk Nation and director of the Heiltsuk Integrated Resource Management Department, a group tasked with stewardship of the territory. He wasn’t part of this study, but collaborates with Artelle in other capacities.\n\n“From a traditional perspective, we’ve always known that wolves are very intelligent beings,” Housty says. The discovery opens up new questions about what wolves are capable of."
    },
    {
        "url": "https://www.nationalgeographic.com/animals/article/toads-turn-neon-yellow-mating",
        "text": "The color change, driven by hormones, seems aimed at preventing males from accidentally mating with each other. As the first monsoon rains in India and southeast Asia begin to swell each year, one type of toad experiences an almost literal glow up. A temporary, hormone-induced wardrobe change transforms male Asian common toads (Duttaphrynus melanostictus) from chocolate pudding brown to lemon yellow in a matter of minutes.   While the females’ skin remains brown, males undergo a color palette makeover to get ready for a lightning round of speed dating. Scientists have long known that the color change coincided with a frenzied, annual two-day breeding event, but they only recently confirmed the specific role it plays.  To investigate, researchers from Schönbrunn Zoo in Vienna 3D printed toads — some brown, some yellow — and placed them amid real toads gathering to mate. They found that the male toads largely ignored the yellow models, but frequently attempted to mate with the brown ones, which matched the color they would expect of females. The researchers tried varying other factors, such as the model toads’ weight, size, and color saturation, but nothing else seemed to affect which models most attracted the male toads.    Scientists say this strongly suggests the toads naturally color-code themselves to prevent any cases of mistaken identity. While males of other species often display brilliant colors to attract females, these toads seem to turn traffic light-yellow to send a signal that repels other males.  “In explosive breeding species, mispairings are common,” said Susanne Stückler, a research associate at Schönbrunn Zoo who led the study. Since the breeding period is so brief, toads must find mates very quickly. Competition is fierce, especially because female toads are scarce.  In their fervor, Stückler says males may attempt to mate with other males, the wrong species of toad, fish, or even inanimate objects. “This suggests that identifying the correct mating partner can be difficult in these dense and stressful conditions,” Stückler said. “Coloration appears to be one evolutionary solution to this problem.” This type of research could change how scientists think about the evolution of color across the animal tree of life, says Rayna Bell, the curator of herpetology at the California Academy of Sciences who was not involved in the study. “This isn’t the first study to show this kind of signaling in frogs, but what I think is really cool about it is that it could alter how we interpret color signals even in those groups we think we already understand, including more charismatic examples like birds or butterflies,” Bell said. “Paying attention to less-studied animals could offer new ideas that make us reevaluate what we thought we knew about signaling processes across the board.”       Unlike other animals such as octopuses and chameleons, which can change color within seconds, it takes about 10 minutes for the male toads to turn yellow. That’s because it’s driven by hormones, rather than skin cells that are directly controlled by nerves. The yellow hue lasts up to 2 days before fading back to brown.  How do they transform themselves? Beneath the toads’ skin, there are layers of specialized cells called chromatophores. Some contain dark pigments, others carry yellow and red ones, and a third type reflects light like tiny mirrors. Stress hormones like adrenaline seem to trigger the toads’ bodies to rearrange pigments and tilt those reflective plates.   The color change may sound like a rare instance of cooperation in the wild. But the toads are still very much in competition. “They fight, kick, and try to displace other males” who are already attempting to mate, Stückler said. “Sometimes, several males try to [simultaneously mate with] the same female, forming ‘mating balls,’ which can even lead to female drowning.”  Climate change may further strain an already hectic and precisely timed event. Though monsoon season lasts for a few months, Asian common toads, along with many other amphibians, breed during a specific one- to two-day period at the beginning. That’s because the babies need to hatch and develop as much as possible prior to winter for the best chance of survival. When the rains arrive and begin to puddle up, the toads quickly mate and tadpoles have time to grow into baby frogs before their habitat dries up again. But shifting weather patterns are scrambling both the timing and intensity of monsoon season, which could disrupt many species’ already narrow window of opportunity. If toads lay eggs during a short rain spell before a long duration of sunny days, “all the eggs would desiccate and reduce the population in the coming years,” says K. V. Gururaja, an amphibian expert at Srishti Manipal Institute of Art, Design and Technology in India, who co-authored the study. The survival of explosive breeding species like the Asian common toad may hinge on their ability to go with the monsoon’s shifting flow. Copyright © 1996-2015 National Geographic SocietyCopyright © 2015-2025 National Geographic Partners, LLC. All rights reserved",
        "onlytext": "The color change, driven by hormones, seems aimed at preventing males from accidentally mating with each other. As the first monsoon rains in India and southeast Asia begin to swell each year, one type of toad experiences an almost literal glow up. A temporary, hormone-induced wardrobe change transforms male Asian common toads (Duttaphrynus melanostictus) from chocolate pudding brown to lemon yellow in a matter of minutes.   \n\nWhile the females’ skin remains brown, males undergo a color palette makeover to get ready for a lightning round of speed dating. Scientists have long known that the color change coincided with a frenzied, annual two-day breeding event, but they only recently confirmed the specific role it plays.\n\nTo investigate, researchers from Schönbrunn Zoo in Vienna 3D printed toads — some brown, some yellow — and placed them amid real toads gathering to mate. They found that the male toads largely ignored the yellow models, but frequently attempted to mate with the brown ones, which matched the color they would expect of females. The researchers tried varying other factors, such as the model toads’ weight, size, and color saturation, but nothing else seemed to affect which models most attracted the male toads.\n\nScientists say this strongly suggests the toads naturally color-code themselves to prevent any cases of mistaken identity. While males of other species often display brilliant colors to attract females, these toads seem to turn traffic light-yellow to send a signal that repels other males.\n\n“In explosive breeding species, mispairings are common,” said Susanne Stückler, a research associate at Schönbrunn Zoo who led the study. Since the breeding period is so brief, toads must find mates very quickly. Competition is fierce, especially because female toads are scarce.\n\nIn their fervor, Stückler says males may attempt to mate with other males, the wrong species of toad, fish, or even inanimate objects. “This suggests that identifying the correct mating partner can be difficult in these dense and stressful conditions,” Stückler said. “Coloration appears to be one evolutionary solution to this problem.”\n\nThis type of research could change how scientists think about the evolution of color across the animal tree of life, says Rayna Bell, the curator of herpetology at the California Academy of Sciences who was not involved in the study. “This isn’t the first study to show this kind of signaling in frogs, but what I think is really cool about it is that it could alter how we interpret color signals even in those groups we think we already understand, including more charismatic examples like birds or butterflies,” Bell said. “Paying attention to less-studied animals could offer new ideas that make us reevaluate what we thought we knew about signaling processes across the board.”\n\nUnlike other animals such as octopuses and chameleons, which can change color within seconds, it takes about 10 minutes for the male toads to turn yellow. That’s because it’s driven by hormones, rather than skin cells that are directly controlled by nerves. The yellow hue lasts up to 2 days before fading back to brown.\n\nHow do they transform themselves? Beneath the toads’ skin, there are layers of specialized cells called chromatophores. Some contain dark pigments, others carry yellow and red ones, and a third type reflects light like tiny mirrors. Stress hormones like adrenaline seem to trigger the toads’ bodies to rearrange pigments and tilt those reflective plates.\n\nThe color change may sound like a rare instance of cooperation in the wild. But the toads are still very much in competition. “They fight, kick, and try to displace other males” who are already attempting to mate, Stückler said. “Sometimes, several males try to [simultaneously mate with] the same female, forming ‘mating balls,’ which can even lead to female drowning.”\n\nClimate change may further strain an already hectic and precisely timed event. Though monsoon season lasts for a few months, Asian common toads, along with many other amphibians, breed during a specific one- to two-day period at the beginning. That’s because the babies need to hatch and develop as much as possible prior to winter for the best chance of survival. When the rains arrive and begin to puddle up, the toads quickly mate and tadpoles have time to grow into baby frogs before their habitat dries up again. But shifting weather patterns are scrambling both the timing and intensity of monsoon season, which could disrupt many species’ already narrow window of opportunity. If toads lay eggs during a short rain spell before a long duration of sunny days, “all the eggs would desiccate and reduce the population in the coming years,” says K. V. Gururaja, an amphibian expert at Srishti Manipal Institute of Art, Design and Technology in India, who co-authored the study. The survival of explosive breeding species like the Asian common toad may hinge on their ability to go with the monsoon’s shifting flow."
    },
    {
        "url": "https://www.nationalgeographic.com/animals/article/dog-toy-addiction",
        "text": "Scientists find that some dogs exhibit behaviors similar to human addiction around their favorite toys.  Have you ever seen a dog playing slot machines in a casino? Probably not, but you might have seen one that really likes to play with toys. A new study shows that there might not be much of a difference.  Some dogs behave toward their toys in ways that resemble those of behavioral addictions in humans, such as gambling and internet gaming, scientists write in the journal Scientific Reports. Stefanie Riemer, a behavioral biologist at the University of Veterinary Medicine in Vienna and author of the new study had long heard of pet owners calling their dogs “ball junkies” because of their love for toy balls. But when she realized no one had ever explored whether the criteria for human addiction could be applied to dogs and their toys, she decided to find out — with science. For now, researchers don’t want to claim that what they are seeing is an actual addiction — just “indicators that seems a little bit like an addiction,” Riemer says. In the human realm, addiction has two sides. The first is a craving and compulsion toward a particular stimulus—like drugs or the rush of gambling—and the change of mood when they get that reward. The second is feelings like withdrawal symptoms when that stimulus is taken away.  “An addiction means persisting in something despite it having negative consequences in long term,” says Riemer. So, can dogs experience something similar with their toys?  To answer this question, Riemer and her team designed 14 different tests for 105 dogs (56 males, 49 females), with an age ranging from 12 months to 10 years. Different breed groups were tested, including shepherds, terriers and retrievers. In one of the tests, for example, the dogs could choose between a favored toy that was inaccessible—such as in a box or on a shelf—and another type of reward or interaction, such as food or play with their owner. Dogs that seemed addicted to the toy remained fixated on it, trying to break the box or staying focused on the shelf, instead of going for the available reward.  In another test, the researchers analyzed whether and how a dog would calm down after toys, food, and everything else was removed from the experimental room. Dogs that showed more addiction-like behavior kept walking around for the entire duration of the test, says Riemer. “They would focus on the door where the toys had disappeared from, or on the shelves where the toys had been previously stored.”  Researchers observed that among all the breeds they studied, shepherd breeds such as German Shepherd and Belgian Shepherd had the highest scores for addiction-like behaviors. Shepherds are bred for high-focus activities such as livestock protection, police work, and search and rescue, which require persistence and strong motivation. But while these traits are desirable for working dogs, in extreme cases they could lead to addiction-like behaviors, and there might be negative consequences. For example, “in some dogs the welfare is definitely impaired if they have high frustration levels when they don't get access to the reward,” says Riemer. “That's not healthy.” The association with particular breeds also leads researchers to hypothesize that there could be a strong genetic component in the addiction-like behaviors they observed. “It does seem that it's very much a characteristic within the dogs already,” Riemer says. Animal welfare and behavior experts who were not involved in this research say they appreciate how the paper explores a new animal psychology frontier. But they point out that many questions remain unanswered. “This is a really good first step,” says Julia Espinosa, a post-doctoral researcher at York University in Toronto, Canada. Espinosa says she’s not sure whether we can equate human addictions with what we see in these dogs. That’s because in addictions like gambling, people are aware of the risk they are taking. “In fact, the risk might be what makes it so addictive,” says Espinosa. On the contrary, while there might be negative consequences for the dogs as well, our furry friends don’t know that. An open question, Espinosa adds, is how strong would be a dog's tendency to engage in addiction-like behaviors if there were adverse consequence —something the researchers didn’t test to respect the dog’s welfare. Despite that, “this is something that addresses a really important aspect of dog welfare, and highlights that this it's not just people characterizing or anthropomorphizing something about the dog,” she says. Addiction-like behaviors could have implications for future approaches to dog training, Espinosa adds. A relatively small subset of dogs show addiction-like behaviors that reach a level that could be concerning, Riemer notes, but in those dogs their attachment to particular objects should be addressed. Future research could dive into questions like: what’s the best way to make a dog behave less compulsively around a toy?  Holly Molinaro, an animal welfare scientist at Animal Wellbeing Solutions, notes that the authors only recruited and tested play-motivated dogs and dogs that excessively play, which makes it hard to know how common these behaviors are in the broader world of dogs. She said it’s an interesting starting point, but further research is needed.  Overall, it’s too soon to give dog owners advice based on this research.  “The authors were very clear, this is exploratory,” Molinaro says. “We should not diagnose from this study. So, I hope that people know that. Don't start self-diagnosing your dog.” Copyright © 1996-2015 National Geographic SocietyCopyright © 2015-2025 National Geographic Partners, LLC. All rights reserved",
        "onlytext": "Scientists find that some dogs exhibit behaviors similar to human addiction around their favorite toys. Have you ever seen a dog playing slot machines in a casino? Probably not, but you might have seen one that really likes to play with toys. A new study shows that there might not be much of a difference. \n\nSome dogs behave toward their toys in ways that resemble those of behavioral addictions in humans, such as gambling and internet gaming, scientists write in the journal Scientific Reports. Stefanie Riemer, a behavioral biologist at the University of Veterinary Medicine in Vienna and author of the new study had long heard of pet owners calling their dogs “ball junkies” because of their love for toy balls. But when she realized no one had ever explored whether the criteria for human addiction could be applied to dogs and their toys, she decided to find out — with science.\n\nFor now, researchers don’t want to claim that what they are seeing is an actual addiction — just “indicators that seems a little bit like an addiction,” Riemer says. In the human realm, addiction has two sides. The first is a craving and compulsion toward a particular stimulus—like drugs or the rush of gambling—and the change of mood when they get that reward. The second is feelings like withdrawal symptoms when that stimulus is taken away.\n\n“An addiction means persisting in something despite it having negative consequences in long term,” says Riemer. So, can dogs experience something similar with their toys?\n\nTo answer this question, Riemer and her team designed 14 different tests for 105 dogs (56 males, 49 females), with an age ranging from 12 months to 10 years. Different breed groups were tested, including shepherds, terriers and retrievers. In one of the tests, for example, the dogs could choose between a favored toy that was inaccessible—such as in a box or on a shelf—and another type of reward or interaction, such as food or play with their owner. Dogs that seemed addicted to the toy remained fixated on it, trying to break the box or staying focused on the shelf, instead of going for the available reward.\n\nIn another test, the researchers analyzed whether and how a dog would calm down after toys, food, and everything else was removed from the experimental room. Dogs that showed more addiction-like behavior kept walking around for the entire duration of the test, says Riemer. “They would focus on the door where the toys had disappeared from, or on the shelves where the toys had been previously stored.”\n\nResearchers observed that among all the breeds they studied, shepherd breeds such as German Shepherd and Belgian Shepherd had the highest scores for addiction-like behaviors. Shepherds are bred for high-focus activities such as livestock protection, police work, and search and rescue, which require persistence and strong motivation. But while these traits are desirable for working dogs, in extreme cases they could lead to addiction-like behaviors, and there might be negative consequences. For example, “in some dogs the welfare is definitely impaired if they have high frustration levels when they don't get access to the reward,” says Riemer. “That's not healthy.”\n\nThe association with particular breeds also leads researchers to hypothesize that there could be a strong genetic component in the addiction-like behaviors they observed. “It does seem that it's very much a characteristic within the dogs already,” Riemer says.\n\nAnimal welfare and behavior experts who were not involved in this research say they appreciate how the paper explores a new animal psychology frontier. But they point out that many questions remain unanswered. “This is a really good first step,” says Julia Espinosa, a post-doctoral researcher at York University in Toronto, Canada. Espinosa says she’s not sure whether we can equate human addictions with what we see in these dogs. That’s because in addictions like gambling, people are aware of the risk they are taking. “In fact, the risk might be what makes it so addictive,” says Espinosa. On the contrary, while there might be negative consequences for the dogs as well, our furry friends don’t know that.\n\nAn open question, Espinosa adds, is how strong would be a dog's tendency to engage in addiction-like behaviors if there were adverse consequence —something the researchers didn’t test to respect the dog’s welfare. Despite that, “this is something that addresses a really important aspect of dog welfare, and highlights that this it's not just people characterizing or anthropomorphizing something about the dog,” she says.\n\nAddiction-like behaviors could have implications for future approaches to dog training, Espinosa adds. A relatively small subset of dogs show addiction-like behaviors that reach a level that could be concerning, Riemer notes, but in those dogs their attachment to particular objects should be addressed. Future research could dive into questions like: what’s the best way to make a dog behave less compulsively around a toy?\n\nHolly Molinaro, an animal welfare scientist at Animal Wellbeing Solutions, notes that the authors only recruited and tested play-motivated dogs and dogs that excessively play, which makes it hard to know how common these behaviors are in the broader world of dogs. She said it’s an interesting starting point, but further research is needed.\n\nOverall, it’s too soon to give dog owners advice based on this research. “The authors were very clear, this is exploratory,” Molinaro says. “We should not diagnose from this study. So, I hope that people know that. Don't start self-diagnosing your dog.”"
    },
    {
        "url": "https://www.nationalgeographic.com/science/article/september-total-lunar-eclipse-blood-moon",
        "text": "On September 7, the moon will turn a coppery red during an 82-minute eclipse, one of the most widely seen celestial shows of the decade. On the night of September 7–8, the moon will slip into Earth’s shadow, darkening and shifting from silver to red during a total lunar eclipse. While this one won’t be visible from the Americas, much of the Eastern Hemisphere will have a perfect view to see the impressively long totality lasting 82 minutes. This will be the second lunar eclipse of 2025, and thanks to its visibility across some of the world’s most populous geographical regions, nearly six billion people will have front row seats to this sky show. A lunar eclipse occurs when the Earth passes directly between the sun and the moon, casting its shadow across the lunar surface. As the moon moves into the deeper, central part of Earth’s shadow, called the umbra, its familiar silvery glow shifts towards a distinctive coppery or reddish hue. Because the moon’s orbit is tilted, lunar eclipses don’t occur every month when there is a full moon, occurring only two or three times annually, and visible only on one half of the Earth. (Here are the phases of the moon explained.) During totality, sunlight streaming through Earth’s atmosphere is bent and scattered, filtering out the blue light and letting the redder wavelengths shine onto the moon. That’s why it earns the nickname “blood moon.” Depending on our planet’s atmospheric conditions, the shade can range from bright orange to a deep, rusty red. Predictions for this eclipse suggest a fairly bright orange-red disk since Earth’s atmosphere is relatively clear of volcanic debris and heavy dust right now. This September’s eclipse is a total one, meaning Earth’s shadow will engulf the entire moon. All the eclipse phases unfold over about five and a half hours. Here are the key moments in Universal Time (UTC): Penumbral Eclipse Begins:15:28 UTC Partial Eclipse Begins: 16:27 UTC Total Eclipse Begins: 17:30 UTC Greatest Eclipse: 18:11 UTC Total Eclipse Ends: 18:52 UTC Partial Eclipse Ends: 19:56 UTC Penumbral Eclipse Ends: 20:55 UTC The absolute ‘must-see’ moments will be during the total phase when the moon is at its deepest coloration, lasting 82 minutes, which not only gives observers lots of time to catch views, but also makes this one of the longest moments of totality of the decade. Unlike the March eclipse, this September event will be invisible across North and South America since the eclipse unfolds during their daytime hours. Instead, Asia and Western Australia will enjoy the best views of the entire eclipse from beginning to end during their overnight hours. Meanwhile, large portions of Europe and Africa will see the full moon rise soon after local sunset, with the eclipse already well underway. Thanks to its visibility across densely populated regions, the majority of the world’s population will get to witness this blood moon firsthand. Unlike solar eclipses, no special glasses are needed to watch a lunar eclipse. Just find a dark sky with a clear view. A smartphone or digital camera on a tripod can capture dramatic shots, especially with shorter exposure times that highlight the moon’s details against a landscape foreground. Binoculars or small telescopes can add even more perspectives by giving you zoomed-in views, showing craters and mountains bathed in eerie reddish light. During the 82 minutes of totality, the moon will darken enough to let the surrounding night sky come alive with stars. Keen-eyed skywatchers may notice a bright yellow-hued point of light near the moon, which is planet Saturn. A small backyard telescope will reveal the gas giant’s famous rings, along with another very faint blue-green dot nearby that is the most distant major planet in the solar system, Neptune. (10 night sky events to see in September.) If you are out of luck by being clouded out or stuck on the wrong side of the Earth, check out all the celestial action online with a livestream of the lunar eclipse from various geographical locations provided by The Virtual Telescope Project. And for those who missed out on this year’s, the next lunar eclipse will occur on March 2–3, 2026. This time it will be visible across Eastern Europe, Asia, Australia, the Americas, and even the polar regions. Copyright © 1996-2015 National Geographic SocietyCopyright © 2015-2025 National Geographic Partners, LLC. All rights reserved",
        "onlytext": "On September 7, the moon will turn a coppery red during an 82-minute eclipse, one of the most widely seen celestial shows of the decade. On the night of September 7–8, the moon will slip into Earth’s shadow, darkening and shifting from silver to red during a total lunar eclipse. While this one won’t be visible from the Americas, much of the Eastern Hemisphere will have a perfect view to see the impressively long totality lasting 82 minutes. This will be the second lunar eclipse of 2025, and thanks to its visibility across some of the world’s most populous geographical regions, nearly six billion people will have front row seats to this sky show.\n\nA lunar eclipse occurs when the Earth passes directly between the sun and the moon, casting its shadow across the lunar surface. As the moon moves into the deeper, central part of Earth’s shadow, called the umbra, its familiar silvery glow shifts towards a distinctive coppery or reddish hue. Because the moon’s orbit is tilted, lunar eclipses don’t occur every month when there is a full moon, occurring only two or three times annually, and visible only on one half of the Earth.\n\nDuring totality, sunlight streaming through Earth’s atmosphere is bent and scattered, filtering out the blue light and letting the redder wavelengths shine onto the moon. That’s why it earns the nickname “blood moon.” Depending on our planet’s atmospheric conditions, the shade can range from bright orange to a deep, rusty red. Predictions for this eclipse suggest a fairly bright orange-red disk since Earth’s atmosphere is relatively clear of volcanic debris and heavy dust right now.\n\nThis September’s eclipse is a total one, meaning Earth’s shadow will engulf the entire moon. All the eclipse phases unfold over about five and a half hours. Here are the key moments in Universal Time (UTC): \nPenumbral Eclipse Begins:15:28 UTC \nPartial Eclipse Begins: 16:27 UTC \nTotal Eclipse Begins: 17:30 UTC \nGreatest Eclipse: 18:11 UTC \nTotal Eclipse Ends: 18:52 UTC \nPartial Eclipse Ends: 19:56 UTC \nPenumbral Eclipse Ends: 20:55 UTC\n\nThe absolute ‘must-see’ moments will be during the total phase when the moon is at its deepest coloration, lasting 82 minutes, which not only gives observers lots of time to catch views, but also makes this one of the longest moments of totality of the decade. Unlike the March eclipse, this September event will be invisible across North and South America since the eclipse unfolds during their daytime hours. Instead, Asia and Western Australia will enjoy the best views of the entire eclipse from beginning to end during their overnight hours. Meanwhile, large portions of Europe and Africa will see the full moon rise soon after local sunset, with the eclipse already well underway.\n\nThanks to its visibility across densely populated regions, the majority of the world’s population will get to witness this blood moon firsthand. Unlike solar eclipses, no special glasses are needed to watch a lunar eclipse. Just find a dark sky with a clear view. A smartphone or digital camera on a tripod can capture dramatic shots, especially with shorter exposure times that highlight the moon’s details against a landscape foreground. Binoculars or small telescopes can add even more perspectives by giving you zoomed-in views, showing craters and mountains bathed in eerie reddish light.\n\nDuring the 82 minutes of totality, the moon will darken enough to let the surrounding night sky come alive with stars. Keen-eyed skywatchers may notice a bright yellow-hued point of light near the moon, which is planet Saturn. A small backyard telescope will reveal the gas giant’s famous rings, along with another very faint blue-green dot nearby that is the most distant major planet in the solar system, Neptune.\n\nIf you are out of luck by being clouded out or stuck on the wrong side of the Earth, check out all the celestial action online with a livestream of the lunar eclipse from various geographical locations provided by The Virtual Telescope Project. And for those who missed out on this year’s, the next lunar eclipse will occur on March 2–3, 2026. This time it will be visible across Eastern Europe, Asia, Australia, the Americas, and even the polar regions."
    },
    {
        "url": "https://www.nationalgeographic.com/science/article/animals-react-total-solar-eclipse-august-space-science",
        "text": "Giraffes, spiders, and even whales have been seen changing their behavior during a total solar eclipse. Solar eclipses occur twice a year, when the moon passes between the sun and Earth, which casts a shadow across some areas. There are several types of solar eclipses. An annular solar eclipse occurs when the moon is at its farthest point from Earth, so it appears smaller than the sun. When the moon blocks part of the sun, it creates a partial solar eclipse and the sun appears as a crescent. Sometimes, the moon’s path causes a total solar eclipse. When that happens, the sky darkens to twilight levels and air temperatures drop within the “path of totality.” In that case, the moon completely blocks the sun, which appears as a ring around the moon. It can also produce a dazzling “diamond ring” effect before and after totality. Whether they’re total or annular, solar eclipses are awe-inspiring phenomena that can provoke some extreme psychological responses in many people. According to biologists and longtime eclipse chasers, humans aren’t the only ones reacting to the dramatic changes in the sky. People who have witnessed these effects have also noted that a variety of animals seem to change their behaviors in response to total solar eclipses. Reports of unusual animal reactions to solar eclipses date back centuries. One of the earliest stories comes from Ristoro d’Arezzo, an Italian believed to be a monk, who described what happened during a total eclipse on June 3, 1239. As the sun disappeared and the sky turned dark, “all the animals and birds were terrified; and the wild beasts could easily be caught,” he wrote. (Amazing sights you can only spot while viewing a solar eclipse) During an eclipse seen in Portugal on August 21, 1560, astronomer Christoph Clavius wrote that during totality, “stars appeared in the sky and (marvellous to behold) the birds fell down from the sky to the ground in terror of such horrid darkness.” While it’s hard to confirm such colorful anecdotes from history, modern astronomers and eclipse chasers have also reported wild and domestic animals noticeably reacting to eclipses. Dairy cows return to the barn, crickets begin chirping, birds either go to roost or become more active, and whales breach in the seas. Veteran eclipse-chaser Peter den Hartog traveled to Hungary in 1999 to experience totality, and he remembers seeing various species of birds and bats suddenly appear during totality. “[Was it because of] the light intensity, or the flies and mosquitoes that came out … I’m not sure, but I‘ve definitely experienced more activity during eclipses,” Hartog says. (Lunar eclipse myths from around the world) Eclipse-chaser and author Dave Balch was in Kona, Hawaii, for the 1991 total eclipse and noticed excited activity among the birds along the pier during the partial phases before and after totality. “We could hardly hear each other talk! Then came totality—not a sound. It was deathly quiet. The difference between the noise levels before and during totality was stunning.” Eclipse-chaser Tora Greve was on an expedition to Zambia in 2001 when she noticed that, just as the sun disappeared, frogs began making sounds and raptors stopped circling, possibly due to the change in thermals as the air cooled. Around the water hole where she was standing, she says, giraffes “started running about during the whole totality. When the sun came back, they stopped and began grazing the trees again.” (See 100 years of eclipse-chasing captured in quirky pictures.) Collecting scientifically meaningful data on animal reactions to total solar eclipses is tough business. The paths of eclipses are scattered around the world, and many are visible only from remote regions. That makes it hard to obtain anything more than a few data points per event. “If you really want to study behavior in a comprehensive way, you have to spend a lot of time in the field observing and have rigorous protocols in place,” says ecologist Rebecca Johnson at the California Academy of Sciences. “If you are an animal behavior ecologist setting up to just study effects of solar eclipses, it can be near impossible.” To improve the scientific record, Johnson helped create the Life Responds project, which runs on a dedicated smartphone app called iNaturalist. Her team of biologists and astronomers used the app to collect data from the millions of people who witnessed the total eclipse that occurred on August 21, 2017 and on April 8, 2024. “We created this project that very simply asks people wherever they are—whether they are under totality or partial eclipse—to spend some time outside looking at animals and observing their behavior before, during, and after the eclipse,” Johnson told National Geographic in 2017. In 2024, NASA launched its own citizen science project, Eclipse Soundscapes, which is focused on learning how crickets respond to the eclipse. Anyone can participate by recording data, analyzing audio, or submitting their own observations. (How do animals respond to eclipses? Help NASA find out.) People who want to help by reporting observations through these citizen science opportunities should scout out in advance where they will be watching the eclipse and think about which animals will be around them. For instance, if you happen to be in a suburban backyard or city park, you may be best able to report on urban invertebrates such as ants and spiders. It’s important to note that you should always wear eclipse glasses when viewing the sky during a solar eclipse. (Here’s how to view a solar eclipse safely.) Anecdotal evidence suggests that orb-weaving spiders destroy their webs during an eclipse, so Johnson recommends finding a web to watch. “That might be something particularly easy for people to observe,” she says, “because [spiders] don’t move very far like flying birds, and so there is a higher likelihood of recording slower invertebrate behavior.” The hope is to create a meaningful clearinghouse for animal behavior during eclipses that scientists can use to advance their research. (Do bats take flight during a solar eclipse? Here’s what we learned.) “The collection of observations and looking for patterns is where science begins, and we hope to bring scientists to the data to spur their research moving forward,” Johnson said in 2017. “Hopefully we’ll end up documenting something [during solar eclipses] that no one has ever seen before.”  Copyright © 1996-2015 National Geographic SocietyCopyright © 2015-2025 National Geographic Partners, LLC. All rights reserved",
        "onlytext": "Giraffes, spiders, and even whales have been seen changing their behavior during a total solar eclipse. Solar eclipses occur twice a year, when the moon passes between the sun and Earth, which casts a shadow across some areas. There are several types of solar eclipses. An annular solar eclipse occurs when the moon is at its farthest point from Earth, so it appears smaller than the sun. When the moon blocks part of the sun, it creates a partial solar eclipse and the sun appears as a crescent. Sometimes, the moon’s path causes a total solar eclipse. When that happens, the sky darkens to twilight levels and air temperatures drop within the “path of totality.” In that case, the moon completely blocks the sun, which appears as a ring around the moon. It can also produce a dazzling “diamond ring” effect before and after totality.\n\nWhether they’re total or annular, solar eclipses are awe-inspiring phenomena that can provoke some extreme psychological responses in many people. According to biologists and longtime eclipse chasers, humans aren’t the only ones reacting to the dramatic changes in the sky. People who have witnessed these effects have also noted that a variety of animals seem to change their behaviors in response to total solar eclipses.\n\nReports of unusual animal reactions to solar eclipses date back centuries. One of the earliest stories comes from Ristoro d’Arezzo, an Italian believed to be a monk, who described what happened during a total eclipse on June 3, 1239. As the sun disappeared and the sky turned dark, “all the animals and birds were terrified; and the wild beasts could easily be caught,” he wrote.\n\nDuring an eclipse seen in Portugal on August 21, 1560, astronomer Christoph Clavius wrote that during totality, “stars appeared in the sky and (marvellous to behold) the birds fell down from the sky to the ground in terror of such horrid darkness.”\n\nWhile it’s hard to confirm such colorful anecdotes from history, modern astronomers and eclipse chasers have also reported wild and domestic animals noticeably reacting to eclipses. Dairy cows return to the barn, crickets begin chirping, birds either go to roost or become more active, and whales breach in the seas.\n\nVeteran eclipse-chaser Peter den Hartog traveled to Hungary in 1999 to experience totality, and he remembers seeing various species of birds and bats suddenly appear during totality. “[Was it because of] the light intensity, or the flies and mosquitoes that came out … I’m not sure, but I‘ve definitely experienced more activity during eclipses,” Hartog says.\n\nEclipse-chaser and author Dave Balch was in Kona, Hawaii, for the 1991 total eclipse and noticed excited activity among the birds along the pier during the partial phases before and after totality. “We could hardly hear each other talk! Then came totality—not a sound. It was deathly quiet. The difference between the noise levels before and during totality was stunning.”\n\nEclipse-chaser Tora Greve was on an expedition to Zambia in 2001 when she noticed that, just as the sun disappeared, frogs began making sounds and raptors stopped circling, possibly due to the change in thermals as the air cooled. Around the water hole where she was standing, she says, giraffes “started running about during the whole totality. When the sun came back, they stopped and began grazing the trees again.”\n\nCollecting scientifically meaningful data on animal reactions to total solar eclipses is tough business. The paths of eclipses are scattered around the world, and many are visible only from remote regions. That makes it hard to obtain anything more than a few data points per event. “If you really want to study behavior in a comprehensive way, you have to spend a lot of time in the field observing and have rigorous protocols in place,” says ecologist Rebecca Johnson at the California Academy of Sciences. “If you are an animal behavior ecologist setting up to just study effects of solar eclipses, it can be near impossible.”\n\nTo improve the scientific record, Johnson helped create the Life Responds project, which runs on a dedicated smartphone app called iNaturalist. Her team of biologists and astronomers used the app to collect data from the millions of people who witnessed the total eclipse that occurred on August 21, 2017 and on April 8, 2024.\n\n“We created this project that very simply asks people wherever they are—whether they are under totality or partial eclipse—to spend some time outside looking at animals and observing their behavior before, during, and after the eclipse,” Johnson told National Geographic in 2017.\n\nIn 2024, NASA launched its own citizen science project, Eclipse Soundscapes, which is focused on learning how crickets respond to the eclipse. Anyone can participate by recording data, analyzing audio, or submitting their own observations.\n\nPeople who want to help by reporting observations through these citizen science opportunities should scout out in advance where they will be watching the eclipse and think about which animals will be around them. For instance, if you happen to be in a suburban backyard or city park, you may be best able to report on urban invertebrates such as ants and spiders.\n\nIt’s important to note that you should always wear eclipse glasses when viewing the sky during a solar eclipse.\n\nAnecdotal evidence suggests that orb-weaving spiders destroy their webs during an eclipse, so Johnson recommends finding a web to watch. “That might be something particularly easy for people to observe,” she says, “because [spiders] don’t move very far like flying birds, and so there is a higher likelihood of recording slower invertebrate behavior.”\n\nThe hope is to create a meaningful clearinghouse for animal behavior during eclipses that scientists can use to advance their research. “The collection of observations and looking for patterns is where science begins, and we hope to bring scientists to the data to spur their research moving forward,” Johnson said in 2017. “Hopefully we’ll end up documenting something [during solar eclipses] that no one has ever seen before.”"
    },
    {
        "url": "https://www.nationalgeographic.com/health/article/cosmic-radiation-health-effects",
        "text": "It has long been a concern for astronauts. But how much does cosmic radiation affect us on Earth? When we gaze up at the night sky, we often marvel at the twinkling stars, distant planets, and expansive galaxies. Yet, beyond the visible spectrum lies a more mysterious aspect of the cosmos—cosmic radiation. These are high energy particles that stream through the universe at nearly the speed of light, according to Dimitra Atri of the Mars Research Group at New York University Abu Dhabi's Center for Astrophysics and Space Science. They originate from events like supernova explosions and solar flares and travel through space, constantly bombarding Earth in all directions and entering its atmosphere. (How cosmic rays helped find a tunnel in Egypt's Great Pyramid.) In Marvel Studios’ The Fantastic Four: First Steps, now playing only in theaters, the Fantastic 4 receive their powers after being exposed to cosmic radiation that alters their DNA on a fundamental level. While these high-speed particles unfortunately won’t give you superpowers in real life, they can permeate the human body. At high doses, cosmic rays can tear through DNA molecules and damage biological tissue. Prolonged exposure to cosmic radiation can increase the risk of cancer, cataracts, and reproductive problems. It can also hinder neurogenesis, the process of generating new cells in the brain.  (Radiation in Japan Seas: Risk of Animal Death, Mutation?) But just how much the human body is exposed to this type of radiation and how it’ll influence our health will vary depending on altitude and what measures are taken to protect us from them. Here’s what you should know. Here on Earth, we have a natural defense system against cosmic radiation that safeguards life on the planet: Earth’s atmosphere and magnetic field. The atmosphere absorbs most of the energy from cosmic radiation, letting only a small fraction reach the Earth’s surface. Our planet’s magnetic field, produced by electric currents in the Earth’s core, shields the planet from most harmful space radiation. (Cosmic-ray hotspot discovered, offering clues on deep space mystery.) On average, people on Earth’s surface are exposed to around three millisieverts of radiation per year. (Sieverts, frequently expressed in millisieverts, are a unit used to measure the dose of radiation that affects the human body.) However, elevation matters. “As you move up, the thickness of the atmosphere lessens and you're exposed to more radiation,” Atri says. The higher a person is in altitude, the less atmospheric protection they will receive from cosmic particles. People in high-altitude locations, such as Denver—known as \"The Mile High City\"—experience slightly elevated cosmic radiation levels than those who are at sea level in places like Miami. When air travel takes us to higher altitudes, it also brings us closer to the highly energetic particles emanating from outer space. While a plane passenger is exposed to elevated levels of cosmic radiation, the radiation they receive in one flight is insignificant. For example, a coast-to-coast round-trip flight is about equal to the radiation dose of a single chest X-ray. Pilots, flight attendants, and frequent flyers, however, face increased exposure to cosmic radiation because of how often they’re in the sky. One Harvard University study concluded that radiation exposure contributed to occupational health issues within flight crews and job-related cancer risks. Other research found that aircrew typically receive more radiation exposure than workers at nuclear facilities. “Still, it is not enough to cause that much damage because you are still within the magnetic field of the Earth, and there is still an atmosphere,” Atri adds. After venturing beyond Earth’s protective atmosphere, spacefaring humans face significant radiation exposure levels. The human body in space would be constantly pelted with high-energy particles. (What toll does spaceflight take on astronauts? Here’s what we know.) Astronauts aboard the International Space Station (ISS), which orbits the Earth at an altitude of 400 kilometers, or 260 miles, are exposed to much higher levels of radiation than those on Earth’s surface. In just one week aboard the ISS, astronauts are exposed to the same cosmic radiation the average human would receive at sea level on Earth in a year. Astronauts traveling to farther reaches of the cosmos—on missions to the Moon, Mars, and beyond—would be exposed to even more cosmic rays during transit and arrival to their destination. Because of this, many space agencies have proposed career-long radiation dose limits on how much radiation spacefaring astronauts can be exposed to. An instrument aboard the Curiosity Mars rover during its 253-day trek to Mars revealed that the radiation dose received by an astronaut on a trip there and back alone would be about 0.66 sieverts—the equivalent of 660 chest X-rays. And while Earth’s atmosphere protects it from most of the cosmos’ barrage of radiation, Mars’ slight atmosphere—about 100 times thinner than Earth's—allows a lot of that radiation in. Using the Curiosity rover’s measurements, researchers estimate a 500-day mission on the Red Planet’s surface would bring the total exposure to around one sieverts; that’s about 10 times the radiation dose an astronaut receives during a six-month mission on the ISS. Researchers have proposed a number of spacecraft designs with shields made of water, hydrogen-rich materials, or planetary material that can offer a potentially safer trip through the cosmos by absorbing radiation. (8 night sky events to see in August, from a ‘sturgeon moon’ to a stunning 6-planet lineup) There is also ongoing research surrounding shelter designs that, once astronauts have reached their destination, can be buried or shielded to reduce radiation exposure.  “When you're on the surface, you can use Mars’ soil to build habitats,” Atri says. “You can build somethingunderground that gives you natural shielding. That should be sufficient to basically get rid of the most extreme component of damaging radiation.” Cosmic radiation is a major challenge for interplanetary travel, prompting medical experts to also consider medications that can lessen its impact on the human body. “It’s a very interdisciplinary field,” Atri says. “We have medical professionals, physicists, engineers, psychologists—everyone has to be on board.” Despite our growing knowledge of these mysterious charged particles, Atri says we need more data to fully understand how to protect humans from exposure if we want to explore the far reaches of the cosmos. Unless you have any space travel planned in the near future, you can rest assured you won’t be feeling many negative health effects—or experiencing superpowers—from cosmic radiation.  Copyright © 1996-2015 National Geographic SocietyCopyright © 2015-2025 National Geographic Partners, LLC. All rights reserved",
        "onlytext": "It has long been a concern for astronauts. But how much does cosmic radiation affect us on Earth? When we gaze up at the night sky, we often marvel at the twinkling stars, distant planets, and expansive galaxies. Yet, beyond the visible spectrum lies a more mysterious aspect of the cosmos—cosmic radiation. These are high energy particles that stream through the universe at nearly the speed of light, according to Dimitra Atri of the Mars Research Group at New York University Abu Dhabi's Center for Astrophysics and Space Science. They originate from events like supernova explosions and solar flares and travel through space, constantly bombarding Earth in all directions and entering its atmosphere.\n\nIn Marvel Studios’ The Fantastic Four: First Steps, now playing only in theaters, the Fantastic 4 receive their powers after being exposed to cosmic radiation that alters their DNA on a fundamental level. While these high-speed particles unfortunately won’t give you superpowers in real life, they can permeate the human body. At high doses, cosmic rays can tear through DNA molecules and damage biological tissue. Prolonged exposure to cosmic radiation can increase the risk of cancer, cataracts, and reproductive problems. It can also hinder neurogenesis, the process of generating new cells in the brain.\n\nBut just how much the human body is exposed to this type of radiation and how it’ll influence our health will vary depending on altitude and what measures are taken to protect us from them. Here’s what you should know.\n\nHere on Earth, we have a natural defense system against cosmic radiation that safeguards life on the planet: Earth’s atmosphere and magnetic field. The atmosphere absorbs most of the energy from cosmic radiation, letting only a small fraction reach the Earth’s surface. Our planet’s magnetic field, produced by electric currents in the Earth’s core, shields the planet from most harmful space radiation.\n\nOn average, people on Earth’s surface are exposed to around three millisieverts of radiation per year. However, elevation matters. “As you move up, the thickness of the atmosphere lessens and you're exposed to more radiation,” Atri says. The higher a person is in altitude, the less atmospheric protection they will receive from cosmic particles. People in high-altitude locations, such as Denver—known as \"The Mile High City\"—experience slightly elevated cosmic radiation levels than those who are at sea level in places like Miami.\n\nWhen air travel takes us to higher altitudes, it also brings us closer to the highly energetic particles emanating from outer space. While a plane passenger is exposed to elevated levels of cosmic radiation, the radiation they receive in one flight is insignificant. For example, a coast-to-coast round-trip flight is about equal to the radiation dose of a single chest X-ray. Pilots, flight attendants, and frequent flyers, however, face increased exposure to cosmic radiation because of how often they’re in the sky. One Harvard University study concluded that radiation exposure contributed to occupational health issues within flight crews and job-related cancer risks. Other research found that aircrew typically receive more radiation exposure than workers at nuclear facilities.\n\n“Still, it is not enough to cause that much damage because you are still within the magnetic field of the Earth, and there is still an atmosphere,” Atri adds.\n\nAfter venturing beyond Earth’s protective atmosphere, spacefaring humans face significant radiation exposure levels. The human body in space would be constantly pelted with high-energy particles. Astronauts aboard the International Space Station (ISS), which orbits the Earth at an altitude of 400 kilometers, or 260 miles, are exposed to much higher levels of radiation than those on Earth’s surface. In just one week aboard the ISS, astronauts are exposed to the same cosmic radiation the average human would receive at sea level on Earth in a year. Astronauts traveling to farther reaches of the cosmos—on missions to the Moon, Mars, and beyond—would be exposed to even more cosmic rays during transit and arrival to their destination.\n\nBecause of this, many space agencies have proposed career-long radiation dose limits on how much radiation spacefaring astronauts can be exposed to. An instrument aboard the Curiosity Mars rover during its 253-day trek to Mars revealed that the radiation dose received by an astronaut on a trip there and back alone would be about 0.66 sieverts—the equivalent of 660 chest X-rays. And while Earth’s atmosphere protects it from most of the cosmos’ barrage of radiation, Mars’ slight atmosphere—about 100 times thinner than Earth's—allows a lot of that radiation in. Using the Curiosity rover’s measurements, researchers estimate a 500-day mission on the Red Planet’s surface would bring the total exposure to around one sieverts; that’s about 10 times the radiation dose an astronaut receives during a six-month mission on the ISS.\n\nResearchers have proposed a number of spacecraft designs with shields made of water, hydrogen-rich materials, or planetary material that can offer a potentially safer trip through the cosmos by absorbing radiation. There is also ongoing research surrounding shelter designs that, once astronauts have reached their destination, can be buried or shielded to reduce radiation exposure.\n\n“When you're on the surface, you can use Mars’ soil to build habitats,” Atri says. “You can build somethingunderground that gives you natural shielding. That should be sufficient to basically get rid of the most extreme component of damaging radiation.”\n\nCosmic radiation is a major challenge for interplanetary travel, prompting medical experts to also consider medications that can lessen its impact on the human body. “It’s a very interdisciplinary field,” Atri says. “We have medical professionals, physicists, engineers, psychologists—everyone has to be on board.” \n\nDespite our growing knowledge of these mysterious charged particles, Atri says we need more data to fully understand how to protect humans from exposure if we want to explore the far reaches of the cosmos. Unless you have any space travel planned in the near future, you can rest assured you won’t be feeling many negative health effects—or experiencing superpowers—from cosmic radiation."
    },
    {
        "url": "https://www.nationalgeographic.com/science/article/astronauts-stranded-space-starliner-history",
        "text": "As Sunita Williams and Barry Wilmore finally return home, here's where they fit in a long list of astronauts who got to spend more time on the ISS than expected. Thanks to technical problems with their Boeing Starliner spacecraft, Sunita Williams and Barry \"Butch\" Wilmore have spent a lot more time on the International Space Station than originally planned. After around nine months in space, they returned to Earth aboard a SpaceX capsule with two other astronauts. The spacecraft journeyed 17 hours and splashed down at 5:57pm EDT on March 18. But these astronauts were not the first spacefarers to get “stranded” in space, and they probably won’t be the last. (From Space, Astronaut Snaps Photo of Planets Aligned) Dealing with such difficulties is an essential task for an astronaut—and Williams and Wilmore might have been secretly pleased with the situation. “Astronauts consider themselves ‘stranded’ on Earth, so this is a huge gift,” says Chris Hadfield, a former NASA astronaut, space shuttle pilot and long-term crew commander on the ISS. “It’s the purpose of our profession.” Call it a “stranding”, a delay, or an extended mission, astronauts spend extra time in space every now and then. Reasons vary from the geopolitical to natural hazards of space travel. But whatever the cause, astronauts and space agencies prepare for these scenarios. Williams and Wilmore were scheduled to spend eight days on the ISS, after arriving there on the Starliner’s first flight in June of 2024. But even before the launch and during the journey to the ISS, Starliner was plagued by leaks of the helium gas used to push fuel into its thrusters. NASA and Boeing tried to work through the problems, but in August, NASA announced their plan for Williams and Wilmore to return on the next SpaceX Dragon spacecraft scheduled to bring a crew back to Earth. Starliner made its own automated descent, landing in New Mexico in September 2024. But Williams and Wilmore had to wait for a SpaceX relief team to reach the ISS. That crew finally arrived on Sunday, after dealing with their own battery and hydraulics issues. That means the two astronauts spent roughly 286 days on the ISS—where missions typically only run as long as six months—and orbited Earth 4,576 times. Hadfield, formerly a U.S. Air Force and U.S. Navy test pilot, says issues are expected during the first flight of any spacecraft; but he adds that he understands why NASA has acted to ensure the crew’s safety. For astronauts, the more time in space, the better. “You train for decades so that you have an opportunity to spend extended periods of time in space—and this turned their short duration flight into a long-duration flight,” he notes.   As experienced astronauts, Williams and Wilmore probably would have responded positively to the delay, speculates Hadfield, who now writes thrillers set on Earth and in space. “At the end of each of my spaceflights, if someone had said ‘you get another three months up there’—that would have been the best thing,” he says. NASA insists that Williams and Wilmore were technically not “stranded” in space, arguing that they expected the first Starliner flight to expose such problems. But there’s a long history of spacefarers who have spent more time than expected in space because of barriers in bringing them back to Earth. (Here’s a brief overview of human spaceflight since 1961.) The most famous case is that of Sergei Krikalev, a cosmonaut on board the Mir space station during the dissolution of the Soviet Union. Krikalev had launched on May 18, 1991, from Baikonur in the Soviet Socialist Republic of Kazakhstan and planned to spend about 150 days on Mir. But the Soviet Union fell apart during his mission, and issues about who would pay for his return kept him in orbit for 311 days—a world record at the time. Sometimes spacecraft encounter problems while docked at the ISS, as was the case for another extended ISS occupant, American astronaut Frank Rubio. Rubio and two Russian cosmonauts arrived at the ISS on a Russian Soyuz spacecraft in September 2022, and they were scheduled to take the same spacecraft back in March 2023. But the Soyuz developed problems after it was struck by a micrometeoroid—a speck of dust or rock traveling extremely fast—and so Rubio had to catch a ride on a different Soyuz in September later that year. As a result, Rubio set a new record for the most time continuously spent in space by a NASA astronaut, of 371 days. However, that’s still shy of the overall record: Russian cosmonaut Valeri Polyakov spent 437 consecutive days on board the Mir space station in 1994 and 1995. (Read about how time in space changes astronauts.) Two American astronauts and a Russian cosmonaut were stuck on board the ISS in February 2003, after the space shuttle Columbia disintegrated during its reentry to the atmosphere, killing all seven astronauts on board. After the disaster, NASA suspended all space shuttle flights until they could be made safe — a grounding that lasted more than two years—and so the shuttle Atlantis scheduled to bring the crew back to Earth in March couldn’t fly. Eventually all three spent an extra three months on the ISS, returning in May 2003 on a Russian Soyuz spacecraft. For a time, Russia’s Soyuz was the only spacecraft that could dock with the ISS, and problems with it sometimes caused delays in changing the space station’s crews. This includes the MS-10 mission, which was aborted shortly after its launch in October 2018. Such delays, however, are seldom detailed by the Russian space agency Roscosmos; and Hadfield says changes in space flight schedules happen all the time. The most dramatic case of a space stranding was during the Apollo 13 mission in 1970, after an oxygen tank on the command module exploded just three days into its six-day journey to the moon and back. The disaster threatened the lives of the three astronauts on board. But they were able to use their lunar lander as a lifeboat, ditching it for the heat-shielded command module just before reentry; and they splashed-down safely on 17 April, 1970—roughly 14 hours later than originally expected. University of Southern California astronautics professor Mike Gruntman says it is not usually an issue when astronauts have to spend more time in space than they expected. But staying in space for extended periods might be more dangerous for older astronauts, who could suffer from the effects of muscle-loss and bone-loss in a microgravity environment, he says. Now that Williams and Wimore have returned to Earth, NASA will collect data on how their bodies changed during their extended stay. (Here’s what we know about how spaceflight impacts the human body.) If anything, the Starliner situation exemplifies the ability of career astronauts to deal with problems. But it also shows how geopolitical issues can have an impact on spaceflight, he says, noting that the current conflict between Russia and Ukraine makes it “unrealistic” to consider an emergency Soyuz launch to bring the astronauts back. Purdue University astronautical engineer Barrett Caldwell also sees the Starliner situation as a testament to the resilience of human spaceflight. “The spaceflight community puts a tremendous additional burden of safety and management of risk on any vehicle which is designed for humans,” he says. “The recent decisions show how much we are living that reality.” In addition, exposing and overcoming technical issues is part of the plan for new spacecraft: “Since this is a first mission of an experimental vehicle, all of this is part of the development and evaluation process,” says Caldwell. As for how the Starliner crew spent their extra time in space, Williams and Willmore effectively joined the existing space station crew, living and working in space. This included 150 science experiments, ranging from plant growth to stem cell studies. Former NASA astronaut Tom Jones speculated that NASA and its partners were probably \"glad to have the extra productivity onboard, with two extra brains and pairs of hands with which to conduct research and maintain the very complex ISS.” The ISS already stocks four months of emergency supplies for seven people, he noted. Several robot cargo ships also docked at the station during their time aboard, bringing extra supplies and personal items for the extra astronauts. Any discomfort from an extended stay comes with the territory for career astronauts. “You don’t become an astronaut for comfort,” says Hadfield. And Williams and Wilmore spent much of their lives training for just this sort of eventuality: “This is like the best Christmas present they both could have,” he says. “I would trade places with them in a heartbeat.”  Copyright © 1996-2015 National Geographic SocietyCopyright © 2015-2025 National Geographic Partners, LLC. All rights reserved",
        "onlytext": "Thanks to technical problems with their Boeing Starliner spacecraft, Sunita Williams and Barry \"Butch\" Wilmore have spent a lot more time on the International Space Station than originally planned. After around nine months in space, they returned to Earth aboard a SpaceX capsule with two other astronauts. The spacecraft journeyed 17 hours and splashed down at 5:57pm EDT on March 18. But these astronauts were not the first spacefarers to get “stranded” in space, and they probably won’t be the last.\n\nDealing with such difficulties is an essential task for an astronaut—and Williams and Wilmore might have been secretly pleased with the situation. “Astronauts consider themselves ‘stranded’ on Earth, so this is a huge gift,” says Chris Hadfield, a former NASA astronaut, space shuttle pilot and long-term crew commander on the ISS. “It’s the purpose of our profession.”\n\nCall it a “stranding”, a delay, or an extended mission, astronauts spend extra time in space every now and then. Reasons vary from the geopolitical to natural hazards of space travel. But whatever the cause, astronauts and space agencies prepare for these scenarios.\n\nWilliams and Wilmore were scheduled to spend eight days on the ISS, after arriving there on the Starliner’s first flight in June of 2024. But even before the launch and during the journey to the ISS, Starliner was plagued by leaks of the helium gas used to push fuel into its thrusters. NASA and Boeing tried to work through the problems, but in August, NASA announced their plan for Williams and Wilmore to return on the next SpaceX Dragon spacecraft scheduled to bring a crew back to Earth. Starliner made its own automated descent, landing in New Mexico in September 2024. But Williams and Wilmore had to wait for a SpaceX relief team to reach the ISS. That crew finally arrived on Sunday, after dealing with their own battery and hydraulics issues. That means the two astronauts spent roughly 286 days on the ISS—where missions typically only run as long as six months—and orbited Earth 4,576 times.\n\nHadfield, formerly a U.S. Air Force and U.S. Navy test pilot, says issues are expected during the first flight of any spacecraft; but he adds that he understands why NASA has acted to ensure the crew’s safety. For astronauts, the more time in space, the better. “You train for decades so that you have an opportunity to spend extended periods of time in space—and this turned their short duration flight into a long-duration flight,” he notes.\n\nAs experienced astronauts, Williams and Wilmore probably would have responded positively to the delay, speculates Hadfield, who now writes thrillers set on Earth and in space. “At the end of each of my spaceflights, if someone had said ‘you get another three months up there’—that would have been the best thing,” he says.\n\nNASA insists that Williams and Wilmore were technically not “stranded” in space, arguing that they expected the first Starliner flight to expose such problems. But there’s a long history of spacefarers who have spent more time than expected in space because of barriers in bringing them back to Earth.\n\nThe most famous case is that of Sergei Krikalev, a cosmonaut on board the Mir space station during the dissolution of the Soviet Union. Krikalev had launched on May 18, 1991, from Baikonur in the Soviet Socialist Republic of Kazakhstan and planned to spend about 150 days on Mir. But the Soviet Union fell apart during his mission, and issues about who would pay for his return kept him in orbit for 311 days—a world record at the time.\n\nSometimes spacecraft encounter problems while docked at the ISS, as was the case for another extended ISS occupant, American astronaut Frank Rubio. Rubio and two Russian cosmonauts arrived at the ISS on a Russian Soyuz spacecraft in September 2022, and they were scheduled to take the same spacecraft back in March 2023. But the Soyuz developed problems after it was struck by a micrometeoroid—a speck of dust or rock traveling extremely fast—and so Rubio had to catch a ride on a different Soyuz in September later that year. As a result, Rubio set a new record for the most time continuously spent in space by a NASA astronaut, of 371 days.\n\nHowever, that’s still shy of the overall record: Russian cosmonaut Valeri Polyakov spent 437 consecutive days on board the Mir space station in 1994 and 1995.\n\nTwo American astronauts and a Russian cosmonaut were stuck on board the ISS in February 2003, after the space shuttle Columbia disintegrated during its reentry to the atmosphere, killing all seven astronauts on board. After the disaster, NASA suspended all space shuttle flights until they could be made safe — a grounding that lasted more than two years—and so the shuttle Atlantis scheduled to bring the crew back to Earth in March couldn’t fly. Eventually all three spent an extra three months on the ISS, returning in May 2003 on a Russian Soyuz spacecraft.\n\nFor a time, Russia’s Soyuz was the only spacecraft that could dock with the ISS, and problems with it sometimes caused delays in changing the space station’s crews. This includes the MS-10 mission, which was aborted shortly after its launch in October 2018.\n\nThe most dramatic case of a space stranding was during the Apollo 13 mission in 1970, after an oxygen tank on the command module exploded just three days into its six-day journey to the moon and back. The disaster threatened the lives of the three astronauts on board. But they were able to use their lunar lander as a lifeboat, ditching it for the heat-shielded command module just before reentry; and they splashed-down safely on 17 April, 1970—roughly 14 hours later than originally expected.\n\nUniversity of Southern California astronautics professor Mike Gruntman says it is not usually an issue when astronauts have to spend more time in space than they expected. But staying in space for extended periods might be more dangerous for older astronauts, who could suffer from the effects of muscle-loss and bone-loss in a microgravity environment, he says.\n\nNow that Williams and Wimore have returned to Earth, NASA will collect data on how their bodies changed during their extended stay.\n\nIf anything, the Starliner situation exemplifies the ability of career astronauts to deal with problems. But it also shows how geopolitical issues can have an impact on spaceflight, he says, noting that the current conflict between Russia and Ukraine makes it “unrealistic” to consider an emergency Soyuz launch to bring the astronauts back.\n\nPurdue University astronautical engineer Barrett Caldwell also sees the Starliner situation as a testament to the resilience of human spaceflight. “The spaceflight community puts a tremendous additional burden of safety and management of risk on any vehicle which is designed for humans,” he says. “The recent decisions show how much we are living that reality.”\n\nIn addition, exposing and overcoming technical issues is part of the plan for new spacecraft: “Since this is a first mission of an experimental vehicle, all of this is part of the development and evaluation process,” says Caldwell.\n\nAs for how the Starliner crew spent their extra time in space, Williams and Willmore effectively joined the existing space station crew, living and working in space. This included 150 science experiments, ranging from plant growth to stem cell studies.\n\nFormer NASA astronaut Tom Jones speculated that NASA and its partners were probably \"glad to have the extra productivity onboard, with two extra brains and pairs of hands with which to conduct research and maintain the very complex ISS.” The ISS already stocks four months of emergency supplies for seven people, he noted. Several robot cargo ships also docked at the station during their time aboard, bringing extra supplies and personal items for the extra astronauts.\n\nAny discomfort from an extended stay comes with the territory for career astronauts. “You don’t become an astronaut for comfort,” says Hadfield. And Williams and Wilmore spent much of their lives training for just this sort of eventuality: “This is like the best Christmas present they both could have,” he says. “I would trade places with them in a heartbeat.”"
    },
    {
        "url": "https://www.nationalgeographic.com/culture/article/inside-irelands-gate-to-hell-that-birthed-halloween",
        "text": "Go in search of the ancient royal capital that spawned our favorite night of the dead. In the middle of a field in a lesser known part of Ireland is a large mound where sheep wander and graze freely. Had they been in that same location centuries ago, these animals might have been stiff with terror, held aloft by chanting, costumed celebrants while being sacrificed to demonic spirits that were said to inhabit nearby Oweynagat cave.  This monumental mound lay at the heart of Rathcroghan, the hub of the ancient Irish kingdom of Connaught. The former Iron Age center is now largely buried beneath the farmland of County Roscommon. In 2021, Ireland applied for UNESCO World Heritage status for Rathcroghan (Rath-craw-hin). It remains on the organization's tentative list. Spread across more than two square miles of rich agricultural land, Rathcroghan encompasses 240 archaeological sites, dating back 5,500 years. They include burial mounds, ring forts (settlement sites), standing stones, linear earthworks, an Iron Age ritual sanctuary—and Oweynagat, the so-called gate to hell. More than 2,000 years ago, when Ireland’s communities seem to have worshipped nature and the land itself, it was here at Rathcroghan that the Irish New Year festival of Samhain (SOW-in) was born, says archaeologist and Rathcroghan expert Daniel Curley. In the 1800s, the Samhain tradition was brought by Irish immigrants to the United States, where it morphed into the sugar overload that is American Halloween. Dorothy Ann Bray, a retired associate professor at McGill University and an expert in Irish folklore, explains that pre-Christian Irish divided each year into summer and winter. Within that framework were four festivities. Imbolc, on February 1, was a festival that coincided with lambing season. Bealtaine, on May 1, marked the end of winter and involved customs like washing one’s face in dew, plucking the first blooming flowers, and dancing around a decorated tree. August 1 heralded Lughnasadh, a harvest festival dedicated to the god Lugh and presided over by Irish kings. Then on October 31 came Samhain, when one pastoral year ended and another began. Rathcroghan was not a town, as Connaught had no proper urban centers and consisted of scattered rural properties. Instead, it was a royal settlement and a key venue for these festivals. During Samhain, in particular, Rathcroghan was a hive of activity focused on its elevated temple, which was surrounded by burial grounds for the Connachta elite. Those same privileged people may have lived at Rathcroghan. The remaining, lower-class Connachta communities resided in dispersed farms and descended on the site only for festivals. At those lively events they traded, feasted, exchanged gifts, played games, arranged marriages, and announced declarations of war or peace.  (See how people dressed up for All Hallow’s Eve a century ago.) Festivalgoers also may have made ritual offerings, possibly directed to the spirits of Ireland’s otherworld. That murky, subterranean dimension, also known as Tír na nÓg (Teer-na-nohg), was inhabited by Ireland’s immortals, as well as a myriad of beasts, demons, and monsters. During Samhain, some of these creatures escaped via Oweynagat cave (pronounced “Oen-na-gat” and meaning “cave of the cats”). “Samhain was when the invisible wall between the living world and the otherworld disappeared,” says Mike McCarthy, a Rathcroghan tour guide and researcher who has co-authored several publications on the site. “A whole host of fearsome otherworldly beasts emerged to ravage the surrounding landscape and make it ready for winter.” Thankful for the agricultural efforts of these spirits but wary of falling victim to their fury, the people protected themselves from physical harm by lighting ritual fires on hilltops and in fields. They disguised themselves as fellow ghouls, McCarthy says, so as not to be dragged into the otherworld via the cave.  (These imaginary beasts fueled nightmares around the world.) Despite these engaging legends—and the extensive archaeological site in which they dwell—one easily could drive past Rathcroghan and spot nothing but paddocks. Inhabited for more than 10,000 years, Ireland is so dense with historical remains that many are either largely or entirely unnoticed. Some are hidden beneath the ground, having been abandoned centuries ago and then slowly consumed by nature. That includes Rathcroghan, which some experts say may be Europe’s largest unexcavated royal complex. Not only has it never been dug up, but it also predates Ireland’s written history. That means scientists must piece together its tale using non-invasive technology and artifacts found in its vicinity. While Irish people for centuries knew this site was home to Rathcroghan, it wasn’t until the 1990s that a team of Irish researchers used remote sensing technology to reveal its archaeological secrets beneath the ground. “The beauty of the approach to date at Rathcroghan is that so much has been uncovered without the destruction that comes with excavating upstanding earthwork monuments,” Curley says. “[Now] targeted excavation can be engaged with, which will answer our research questions while limiting the damage inherent with excavation.” This policy of preserving Rathcroghan’s integrity and authenticity extends to tourism. Despite its significance, Rathcroghan is one of Ireland’s less frequented attractions, drawing some 22,000 visitors a year compared with more than a million at the Cliffs of Moher. That may not be the case had it long ago been heavily marketed as the “Birthplace of Halloween,” Curley says. But there is no Halloween signage at Rathcroghan or in Tulsk, the nearest town. Rathcroghan’s renown should soar, however, if Ireland is successful in its push to make it a UNESCO World Heritage site. The Irish Government has included Rathcroghan as part of the “Royal Sites of Ireland,” which is on its newest list of locations to be considered for prized World Heritage status. The global exposure potentially offered by UNESCO branding would likely attract many more visitors to Rathcroghan. But it seems unlikely this historic jewel will be re-packaged as a kitschy Halloween tourist attraction. “If Rathcroghan got a UNESCO listing and that attracted more attention here that would be great, because it might result in more funding to look after the site,” Curley says. “But we want sustainable tourism, not a rush of gimmicky Halloween tourism.” Those travelers who do seek out Rathcroghan might have trouble finding Oweynagat cave. Oweynagat is elusive—despite being the birthplace of Medb, perhaps the most famous queen in Irish history, 2,000 years ago. Barely signposted, it’s hidden beneath trees in a paddock at the end of a one-way, dead-end farm track, about a thousand yards south of the much more accessible temple mound. Visitors are free to hop a fence, walk through a field, and peer into the narrow passage of Oweynagat. In Ireland’s Iron Age, such behavior would have been enormously risky during Samhain, when even wearing a ghastly disguise might not have spared the wrath of a malevolent creature. (The twisted transatlantic tale of American jack-o’-lanterns.) Two millennia later, most costumed trick-or-treaters on Halloween won’t realize they’re mimicking a prehistoric tradition—one with much higher stakes than the pursuit of candy.  Ronan O’Connell is an Australian freelance journalist and photographer based between Ireland and Thailand. Copyright © 1996-2015 National Geographic SocietyCopyright © 2015-2025 National Geographic Partners, LLC. All rights reserved",
        "onlytext": "In the middle of a field in a lesser known part of Ireland is a large mound where sheep wander and graze freely. Had they been in that same location centuries ago, these animals might have been stiff with terror, held aloft by chanting, costumed celebrants while being sacrificed to demonic spirits that were said to inhabit nearby Oweynagat cave.  This monumental mound lay at the heart of Rathcroghan, the hub of the ancient Irish kingdom of Connaught. The former Iron Age center is now largely buried beneath the farmland of County Roscommon. In 2021, Ireland applied for UNESCO World Heritage status for Rathcroghan (Rath-craw-hin). It remains on the organization's tentative list. Spread across more than two square miles of rich agricultural land, Rathcroghan encompasses 240 archaeological sites, dating back 5,500 years. They include burial mounds, ring forts (settlement sites), standing stones, linear earthworks, an Iron Age ritual sanctuary—and Oweynagat, the so-called gate to hell. More than 2,000 years ago, when Ireland’s communities seem to have worshipped nature and the land itself, it was here at Rathcroghan that the Irish New Year festival of Samhain (SOW-in) was born, says archaeologist and Rathcroghan expert Daniel Curley. In the 1800s, the Samhain tradition was brought by Irish immigrants to the United States, where it morphed into the sugar overload that is American Halloween. Dorothy Ann Bray, a retired associate professor at McGill University and an expert in Irish folklore, explains that pre-Christian Irish divided each year into summer and winter. Within that framework were four festivities. Imbolc, on February 1, was a festival that coincided with lambing season. Bealtaine, on May 1, marked the end of winter and involved customs like washing one’s face in dew, plucking the first blooming flowers, and dancing around a decorated tree. August 1 heralded Lughnasadh, a harvest festival dedicated to the god Lugh and presided over by Irish kings. Then on October 31 came Samhain, when one pastoral year ended and another began. Rathcroghan was not a town, as Connaught had no proper urban centers and consisted of scattered rural properties. Instead, it was a royal settlement and a key venue for these festivals. During Samhain, in particular, Rathcroghan was a hive of activity focused on its elevated temple, which was surrounded by burial grounds for the Connachta elite. Those same privileged people may have lived at Rathcroghan. The remaining, lower-class Connachta communities resided in dispersed farms and descended on the site only for festivals. At those lively events they traded, feasted, exchanged gifts, played games, arranged marriages, and announced declarations of war or peace.  Festivalgoers also may have made ritual offerings, possibly directed to the spirits of Ireland’s otherworld. That murky, subterranean dimension, also known as Tír na nÓg (Teer-na-nohg), was inhabited by Ireland’s immortals, as well as a myriad of beasts, demons, and monsters. During Samhain, some of these creatures escaped via Oweynagat cave (pronounced “Oen-na-gat” and meaning “cave of the cats”). “Samhain was when the invisible wall between the living world and the otherworld disappeared,” says Mike McCarthy, a Rathcroghan tour guide and researcher who has co-authored several publications on the site. “A whole host of fearsome otherworldly beasts emerged to ravage the surrounding landscape and make it ready for winter.” Thankful for the agricultural efforts of these spirits but wary of falling victim to their fury, the people protected themselves from physical harm by lighting ritual fires on hilltops and in fields. They disguised themselves as fellow ghouls, McCarthy says, so as not to be dragged into the otherworld via the cave.  Despite these engaging legends—and the extensive archaeological site in which they dwell—one easily could drive past Rathcroghan and spot nothing but paddocks. Inhabited for more than 10,000 years, Ireland is so dense with historical remains that many are either largely or entirely unnoticed. Some are hidden beneath the ground, having been abandoned centuries ago and then slowly consumed by nature. That includes Rathcroghan, which some experts say may be Europe’s largest unexcavated royal complex. Not only has it never been dug up, but it also predates Ireland’s written history. That means scientists must piece together its tale using non-invasive technology and artifacts found in its vicinity. While Irish people for centuries knew this site was home to Rathcroghan, it wasn’t until the 1990s that a team of Irish researchers used remote sensing technology to reveal its archaeological secrets beneath the ground. “The beauty of the approach to date at Rathcroghan is that so much has been uncovered without the destruction that comes with excavating upstanding earthwork monuments,” Curley says. “[Now] targeted excavation can be engaged with, which will answer our research questions while limiting the damage inherent with excavation.” This policy of preserving Rathcroghan’s integrity and authenticity extends to tourism. Despite its significance, Rathcroghan is one of Ireland’s less frequented attractions, drawing some 22,000 visitors a year compared with more than a million at the Cliffs of Moher. That may not be the case had it long ago been heavily marketed as the “Birthplace of Halloween,” Curley says. But there is no Halloween signage at Rathcroghan or in Tulsk, the nearest town. Rathcroghan’s renown should soar, however, if Ireland is successful in its push to make it a UNESCO World Heritage site. The Irish Government has included Rathcroghan as part of the “Royal Sites of Ireland,” which is on its newest list of locations to be considered for prized World Heritage status. The global exposure potentially offered by UNESCO branding would likely attract many more visitors to Rathcroghan. But it seems unlikely this historic jewel will be re-packaged as a kitschy Halloween tourist attraction. “If Rathcroghan got a UNESCO listing and that attracted more attention here that would be great, because it might result in more funding to look after the site,” Curley says. “But we want sustainable tourism, not a rush of gimmicky Halloween tourism.” Those travelers who do seek out Rathcroghan might have trouble finding Oweynagat cave. Oweynagat is elusive—despite being the birthplace of Medb, perhaps the most famous queen in Irish history, 2,000 years ago. Barely signposted, it’s hidden beneath trees in a paddock at the end of a one-way, dead-end farm track, about a thousand yards south of the much more accessible temple mound. Visitors are free to hop a fence, walk through a field, and peer into the narrow passage of Oweynagat. In Ireland’s Iron Age, such behavior would have been enormously risky during Samhain, when even wearing a ghastly disguise might not have spared the wrath of a malevolent creature. Two millennia later, most costumed trick-or-treaters on Halloween won’t realize they’re mimicking a prehistoric tradition—one with much higher stakes than the pursuit of candy."
    },
    {
        "url": "https://www.nationalgeographic.com/travel/world-heritage/article/how-to-see-stonehenge-everything-you-need-to-know",
        "text": "Here’s how to experience the mystery of this megalithic monument. Around 4,500 years ago, a structure of enormous stones aligned with solar patterns was erected in England’s Salisbury Plain by a civilization without metal tools, horsepower, or wheels. Many of the mysteries of how and why the megalithic structure of Stonehenge was built remain unanswered. But recent discoveries via new technology are providing fresh clues to these enduring riddles, even as the site itself, located about 90 miles west of central London, faces the looming threat of modern development. In 1922, National Geographic published its first photograph of Stonehenge, a black-and-white aerial image of the site with the cutting-edge technology of that time—the airplane. For a century, we have covered the prehistoric site, reporting on evolving research about its age, formation, and use. Our coverage continues to be groundbreaking. National Geographic’s August 2022 cover features the site and stories within the issue push boundaries. National Geographic Explorer and photographer Martin Edström created an immersive 3D model of the site using photogrammetry. Using a drone, he and his team took 7,000 images of the site from all angles and processed them into a high-resolution digital replica. You can find a Stonehenge AR experience here. Stonehenge is composed of blocks that weigh more than 45 tons and tower up to 24 feet high. The monument is not only notable for its size, but for its ceremonial design—the first 1,600 feet of the avenue from Stonehenge is built on the axis of the summer solstice sunrise and winter solstice sunset. Whether this alignment was constructed for sun worship, calendar keeping, or other purposes remains a mystery. Over time, Stonehenge has been attributed to Druids, Romans, Vikings, Saxons, and even King Arthur’s court magician, Merlin. But the people who actually constructed the site left no written language or legend—only bones, potsherds, stone, and antler tools. According to a 12th-century legend from chronicler and cleric Geoffrey of Monmouth, Stonehenge’s monoliths were taken from a stone circle in Ireland after a great battle and transported by magic and by boat to where they stand today. He was correct in a way—we now know that, of the hundreds of stone circles in Britain, Stonehenge is the only one whose stones, averaging two tons each, were brought from a great distance, according to National Geographic Explorer and archaeologist Mike Parker Pearson. New tools including x-ray fluorescence spectrometry and ICP-MS laser ablation have helped geologists Richard Bevins and Rob Ixer identify four outcroppings in Preseli Hills, Wales, where the monoliths in Stonehenge originated from. That means the stones traveled some 175 miles to where they stand today. In Belgium, researcher Christophe Snoeck pioneered a technique to retrieve isotopes from cremated remains to reveal where an individual lived in their last decade of life—revealing more than ever before about those buried at Stonehenge. He learned nearly half of those buried in the structure’s early days lived miles from the site, and was even able to determine the kind of wood burned for cremation—trees not found near Stonehenge. Experts say Stonehenge required an enormous amount of timber for its construction—not just for palisades of trunks driven into the ground, but also for builders to drag 20-ton stones on wooden sledges, on possibly miles of wooden tracks, as well as giant scaffolds to erect the stones on site. The ten square mile area of Stonehenge includes avenues, settlements, some 350 burial grounds, and healing centers. This symbol of prehistory stands in stark contrast to its modern neighbor, the A303 highway, which most of the million annual visitors take to see the structure. Notoriously fettered with heavy traffic and spotted with potholes, the narrow road hosts rumbling trucks that can disturb the peace at the Stonehenge site. To ease these issues, a two-mile-long, four-lane tunnel was proposed to bypass Stonehenge, drawing fire from archaeologists and sparking protests. For now, the $2.2 billion project is on hold after a ruling from Britain’s High Court last year. Summer solstice is the most popular time to visit Stonehenge. During the summer solstice, the sun rises behind the Heel Stone, and its first rays shine into the heart of Stonehenge. Archaeological excavations have found it may have once had a partner stone, the two stones framing the sunrise. This is one of the few occasions the inner circle is open to the public. Turning 180 degrees to face southwest, during the winter solstice, the sun would originally have set between the two uprights of the tallest trilithon, but the effect has been lost since half the trilithon fell at some point in the millennia since its construction. Stonehenge is open year-round, and timed tickets for Stonehenge can be booked in advance for guaranteed entry. A walkway surrounds the famed circle, but due to conservation concerns, the public is typically not allowed inside the ring. However, many compensations await. The landmark is surrounded by a vast expanse of fields, perfect walking country dotted with associated earthworks, burial grounds, and other monuments. There is regular train service from London, Bristol/Bath, and Southampton to Salisbury, located 12 miles from Stonehenge. Bus service is also available via Salisbury Reds. From there, take a taxi or hop on the wheelchair-accessible bus to the Stonehenge Visitor Centre. Finally, a 1.5-mile (25-minute) walk leads to the circle. For those who are unable to walk, a free bus service operates between the disabled access parking lot and Stonehenge. Click here for more information on transportation. A variety of hotels and guesthouses are available in Wiltshire, and several campsites are located within 10 miles of Stonehenge. Copyright © 1996-2015 National Geographic SocietyCopyright © 2015-2025 National Geographic Partners, LLC. All rights reserved",
        "onlytext": "Around 4,500 years ago, a structure of enormous stones aligned with solar patterns was erected in England’s Salisbury Plain by a civilization without metal tools, horsepower, or wheels. Many of the mysteries of how and why the megalithic structure of Stonehenge was built remain unanswered. But recent discoveries via new technology are providing fresh clues to these enduring riddles, even as the site itself, located about 90 miles west of central London, faces the looming threat of modern development.\n\nIn 1922, National Geographic published its first photograph of Stonehenge, a black-and-white aerial image of the site with the cutting-edge technology of that time—the airplane. For a century, we have covered the prehistoric site, reporting on evolving research about its age, formation, and use. Our coverage continues to be groundbreaking. National Geographic’s August 2022 cover features the site and stories within the issue push boundaries.\n\nNational Geographic Explorer and photographer Martin Edström created an immersive 3D model of the site using photogrammetry. Using a drone, he and his team took 7,000 images of the site from all angles and processed them into a high-resolution digital replica. You can find a Stonehenge AR experience here.\n\nStonehenge is composed of blocks that weigh more than 45 tons and tower up to 24 feet high. The monument is not only notable for its size, but for its ceremonial design—the first 1,600 feet of the avenue from Stonehenge is built on the axis of the summer solstice sunrise and winter solstice sunset. Whether this alignment was constructed for sun worship, calendar keeping, or other purposes remains a mystery.\n\nOver time, Stonehenge has been attributed to Druids, Romans, Vikings, Saxons, and even King Arthur’s court magician, Merlin. But the people who actually constructed the site left no written language or legend—only bones, potsherds, stone, and antler tools.\n\nAccording to a 12th-century legend from chronicler and cleric Geoffrey of Monmouth, Stonehenge’s monoliths were taken from a stone circle in Ireland after a great battle and transported by magic and by boat to where they stand today. He was correct in a way—we now know that, of the hundreds of stone circles in Britain, Stonehenge is the only one whose stones, averaging two tons each, were brought from a great distance, according to National Geographic Explorer and archaeologist Mike Parker Pearson.\n\nNew tools including x-ray fluorescence spectrometry and ICP-MS laser ablation have helped geologists Richard Bevins and Rob Ixer identify four outcroppings in Preseli Hills, Wales, where the monoliths in Stonehenge originated from. That means the stones traveled some 175 miles to where they stand today.\n\nIn Belgium, researcher Christophe Snoeck pioneered a technique to retrieve isotopes from cremated remains to reveal where an individual lived in their last decade of life—revealing more than ever before about those buried at Stonehenge. He learned nearly half of those buried in the structure’s early days lived miles from the site, and was even able to determine the kind of wood burned for cremation—trees not found near Stonehenge.\n\nExperts say Stonehenge required an enormous amount of timber for its construction—not just for palisades of trunks driven into the ground, but also for builders to drag 20-ton stones on wooden sledges, on possibly miles of wooden tracks, as well as giant scaffolds to erect the stones on site.\n\nThe ten square mile area of Stonehenge includes avenues, settlements, some 350 burial grounds, and healing centers. This symbol of prehistory stands in stark contrast to its modern neighbor, the A303 highway, which most of the million annual visitors take to see the structure. Notoriously fettered with heavy traffic and spotted with potholes, the narrow road hosts rumbling trucks that can disturb the peace at the Stonehenge site. To ease these issues, a two-mile-long, four-lane tunnel was proposed to bypass Stonehenge, drawing fire from archaeologists and sparking protests. For now, the $2.2 billion project is on hold after a ruling from Britain’s High Court last year.\n\nSummer solstice is the most popular time to visit Stonehenge. During the summer solstice, the sun rises behind the Heel Stone, and its first rays shine into the heart of Stonehenge. Archaeological excavations have found it may have once had a partner stone, the two stones framing the sunrise. This is one of the few occasions the inner circle is open to the public.\n\nTurning 180 degrees to face southwest, during the winter solstice, the sun would originally have set between the two uprights of the tallest trilithon, but the effect has been lost since half the trilithon fell at some point in the millennia since its construction.\n\nStonehenge is open year-round, and timed tickets for Stonehenge can be booked in advance for guaranteed entry. A walkway surrounds the famed circle, but due to conservation concerns, the public is typically not allowed inside the ring. However, many compensations await. The landmark is surrounded by a vast expanse of fields, perfect walking country dotted with associated earthworks, burial grounds, and other monuments.\n\nThere is regular train service from London, Bristol/Bath, and Southampton to Salisbury, located 12 miles from Stonehenge. Bus service is also available via Salisbury Reds. From there, take a taxi or hop on the wheelchair-accessible bus to the Stonehenge Visitor Centre. Finally, a 1.5-mile (25-minute) walk leads to the circle. For those who are unable to walk, a free bus service operates between the disabled access parking lot and Stonehenge.\n\nA variety of hotels and guesthouses are available in Wiltshire, and several campsites are located within 10 miles of Stonehenge."
    },
    {
        "url": "https://www.nationalgeographic.com/science/article/screen-time-children-family",
        "text": "Not all screen time is created equally. Experts explain why quality matters more than quantity and how parents can rethink their rules for devices. Parenting in the digital age can feel like navigating a maze of opinions and advice. Strict device limits or outright bans often dominate conversations about screen time, leaving families overwhelmed by conflicting guidelines on what’s appropriate for children of different ages. But here’s the thing: the issue isn’t as simple as setting a timer and walking away. Studies reveal that the quality of what kids watch, play, and interact with often matters more than the quantity of their screen time. A 2022 article published in Frontiers in Psychology found that watching screens can have detrimental or beneficial effects on development, depending on content and context, underscoring that not all screen time is equal. So, could a more nuanced approach to screen time—focusing on quality over quantity and mentoring over monitoring— better serve families? Here’s what the experts say. The traditional focus on duration often misleads families into thinking screen time management is about limiting minutes. “By zeroing in on duration, families are led to believe that managing screen use is a simple numbers game,” says Katie Davis, author of Technology’s Child: Digital Media’s Role in the Ages and Stages of Growing Up and co-director of the UW Digital Youth Lab. Instead, she encourages parents to “consider what children are doing on their screens, who they are interacting with, and how the experience makes them feel.” Keena McAvoy, a digital wellness educator and co-founder of DMV Unplugged, adds, “Boundaries can be missing with simplistic screen time guidelines. For example, a 5-year-old watching a 45-minute long-form story on their iPad in the living room while their parent cooks dinner nearby is a much-preferred experience for a growing brain than a 5-year-old accessing YouTube shorts alone in their bedroom for 45 minutes.” This numbers-focused mindset can also unfairly vilify technology, says Regan Vidiksis, a senior researcher at EDC’s Center for Children and Technology. She adds that the conversation often overlooks the many positive uses of screens and media, including opportunities for creativity, learning, and connection. Parents and caregivers can make screen time a tool for growth and connection by thoughtfully integrating it into family life. Rather than imposing rigid rules, Davis suggests families “weave screens into the fabric of daily routines in ways that enhance, rather than detract from, shared experiences and individual growth.” For instance, a 2022 study highlights how video games, in the right context, can improve problem-solving skills, hand-eye coordination, and cognitive development in children.  With proper guidance, children can use technology to explore their interests, connect with diverse communities, and advocate for causes—all without becoming tethered to screens. “Technology is just another medium in their lives and something that they need to learn to develop healthy habits around, with the support of their families and other media mentors,” Vidiksis says. A 2023 Common Sense Media survey showed this when children ages 11-17 described ways of adding friction to their phones to try to use them more intentionally. “Conversations about digital habits, co-engagement in activities, and providing context for what children see and do online are critical aspects of fostering healthy technology use,” Davis says. Creating a balanced digital life starts with curating quality content and setting realistic expectations. McAvoy recommends that parents “delay their children’s access to algorithmically controlled technology products (social media, YouTube shorts, TikTok) for as long as possible” to help their developing brains. Instead, look for apps, games, and shows that encourage creativity, critical thinking, and active engagement over passive consumption. Several experts say Common Sense Media is a great resource for seeing how different digital media options measure up. Beyond curating content, parents should also look at creating a balanced lifestyle for their families. Screen time should complement, rather than replace, childhood activities like outdoor time, physical activity, in-person social interaction, hobbies, and sports as much as is practical. Parents may consider creating certain times of day or situations where screen time is off-limits to find a balance that works for their family. Or create public digital spaces around the home. Whatever you decide, the rules and expectations should be clear and explicit, says Vidiksis. She adds that the rules should also be reasonable and easily modified according to life’s circumstances. After all, screen time itself is not implicitly bad, and not all screen time is equal. Ultimately, the goal is not to eliminate screens but to guide children toward a positive relationship with technology. “Parents should ask themselves: Is my child’s technology use self-directed and fostering growth? These qualities matter far more than the number of minutes logged on a device,” Davis says. Copyright © 1996-2015 National Geographic SocietyCopyright © 2015-2025 National Geographic Partners, LLC. All rights reserved",
        "onlytext": "Not all screen time is created equally. Experts explain why quality matters more than quantity and how parents can rethink their rules for devices. Parenting in the digital age can feel like navigating a maze of opinions and advice. Strict device limits or outright bans often dominate conversations about screen time, leaving families overwhelmed by conflicting guidelines on what’s appropriate for children of different ages. But here’s the thing: the issue isn’t as simple as setting a timer and walking away. Studies reveal that the quality of what kids watch, play, and interact with often matters more than the quantity of their screen time. A 2022 article published in Frontiers in Psychology found that watching screens can have detrimental or beneficial effects on development, depending on content and context, underscoring that not all screen time is equal. So, could a more nuanced approach to screen time—focusing on quality over quantity and mentoring over monitoring— better serve families? Here’s what the experts say.\n\nThe traditional focus on duration often misleads families into thinking screen time management is about limiting minutes. “By zeroing in on duration, families are led to believe that managing screen use is a simple numbers game,” says Katie Davis, author of Technology’s Child: Digital Media’s Role in the Ages and Stages of Growing Up and co-director of the UW Digital Youth Lab. Instead, she encourages parents to “consider what children are doing on their screens, who they are interacting with, and how the experience makes them feel.” \n\nKeena McAvoy, a digital wellness educator and co-founder of DMV Unplugged, adds, “Boundaries can be missing with simplistic screen time guidelines. For example, a 5-year-old watching a 45-minute long-form story on their iPad in the living room while their parent cooks dinner nearby is a much-preferred experience for a growing brain than a 5-year-old accessing YouTube shorts alone in their bedroom for 45 minutes.” \n\nThis numbers-focused mindset can also unfairly vilify technology, says Regan Vidiksis, a senior researcher at EDC’s Center for Children and Technology. She adds that the conversation often overlooks the many positive uses of screens and media, including opportunities for creativity, learning, and connection.\n\nParents and caregivers can make screen time a tool for growth and connection by thoughtfully integrating it into family life. Rather than imposing rigid rules, Davis suggests families “weave screens into the fabric of daily routines in ways that enhance, rather than detract from, shared experiences and individual growth.” For instance, a 2022 study highlights how video games, in the right context, can improve problem-solving skills, hand-eye coordination, and cognitive development in children. \n\nWith proper guidance, children can use technology to explore their interests, connect with diverse communities, and advocate for causes—all without becoming tethered to screens. “Technology is just another medium in their lives and something that they need to learn to develop healthy habits around, with the support of their families and other media mentors,” Vidiksis says.\n\nA 2023 Common Sense Media survey showed this when children ages 11-17 described ways of adding friction to their phones to try to use them more intentionally. “Conversations about digital habits, co-engagement in activities, and providing context for what children see and do online are critical aspects of fostering healthy technology use,” Davis says.\n\nCreating a balanced digital life starts with curating quality content and setting realistic expectations. McAvoy recommends that parents “delay their children’s access to algorithmically controlled technology products (social media, YouTube shorts, TikTok) for as long as possible” to help their developing brains. Instead, look for apps, games, and shows that encourage creativity, critical thinking, and active engagement over passive consumption. Several experts say Common Sense Media is a great resource for seeing how different digital media options measure up.\n\nBeyond curating content, parents should also look at creating a balanced lifestyle for their families. Screen time should complement, rather than replace, childhood activities like outdoor time, physical activity, in-person social interaction, hobbies, and sports as much as is practical. Parents may consider creating certain times of day or situations where screen time is off-limits to find a balance that works for their family. Or create public digital spaces around the home. Whatever you decide, the rules and expectations should be clear and explicit, says Vidiksis. She adds that the rules should also be reasonable and easily modified according to life’s circumstances.\n\nAfter all, screen time itself is not implicitly bad, and not all screen time is equal. Ultimately, the goal is not to eliminate screens but to guide children toward a positive relationship with technology. “Parents should ask themselves: Is my child’s technology use self-directed and fostering growth? These qualities matter far more than the number of minutes logged on a device,” Davis says."
    },
    {
        "url": "https://www.nationalgeographic.com/science/article/sixth-taste-food-science",
        "text": "Could “fatty” or “starchy” one day become accepted as the sixth basic taste alongside the likes of sweet and salty? In 1908, Japanese chemist Kikunae Ikeda was studying the active substances in kombu (seaweed), when he made a discovery that would change food science forever. Ikeda realized that a compound called glutamic acid was what gave seaweed such a distinct savory and meaty taste—nothing like the existing basic tastes of sweet, salty, sour, and bitter. He dubbed that taste umami. Yet even though people have been eating seaweed, mushrooms, miso, aged cheeses, and fermented foods for ages, it took nearly a century of scientific debate for umami to be recognized worldwide as the fifth basic taste. It wasn’t until the 1990s that it gained universal recognition—and only after extensive psychophysical, electrophysiological, and biochemical studies that confirmed it was unlike any other existing taste.  Is it time to add a sixth taste to the roster? Some scientists argue that the answer is yes. They have put forward several new contenders. Should it be fatty? Starchy? Kokumi? Salty licorice? To varying degrees, researchers have made a case for all these in recent years. It isn’t that scientists have been intentionally searching for a sixth taste. Rather, “there’s a fundamental science-based search for different ways we can experience the world in terms of taste and how it’s detected,” explains neuroscientist Emily Liman, the Harold Dornsife Chair in Neuroscience at the University of Southern California who specializes in sensory biology. Breakthroughs in understanding the science of taste would allow food scientists to create more delicious foods—and could also help doctors diagnose and treat related disorders that involve a loss of your sense of taste.  But getting the scientific community to agree on a sixth taste is no easy feat. To qualify as a basic taste, any contender must be proven to be as distinct as the sweetness of berries or the bitterness of kale. First, it’s important to note that “there’s a difference between taste and flavor,” says Marisa Moore, an Atlanta-based registered dietitian in culinary nutrition. “Taste begins on the tongue when the food molecules we eat mix with saliva and activate our tastebuds, which then send signals to the brain and trigger the perception of distinct tastes.” Contrary to popular belief, the tongue does not have specific areas that detect each type of taste. Instead, basic tastes can be picked up by taste buds on various parts of the tongue, though different cells may have varying levels of sensitivity to sweet, sour, salty, bitter, and umami.     While taste begins in the mouth, flavor is what registers in the brain when it combines the entire experience of taste, texture, temperature, and aroma. “Taste and smell do interact in the brain to put together flavor,” says Richard Mattes, distinguished professor of nutrition science at Purdue University, “but that’s different from taste.” The process of getting a new taste sensation accepted by the scientific community is complicated. “There is no widely agreed upon set of seven criteria and if you tick each box, you’ve got it,” Mattes says. “It’s just scientific consensus.” Even so, various criteria have been proposed over the years. For starters, a taste needs to have a “chemical signature”—a set of chemical stimuli that are distinct from that of other substances. Then, Mattes says, you need to have a receptor anywhere on the tongue, palate, or throat where the chemical stimulus can interact with a cell in a taste bud. That cell in the taste bud must then have a mechanism “to change the stimuli’s chemical signal to an electrical signal that is subsequently transmitted through nerves to the brain,” Mattes adds.  The electrical signal carried by taste nerves must activate regions in the brain that are associated with taste, and it needs to generate a unique taste perception in the brain that’s independent from other tastes. “It can’t overlap with another taste,” Lim says. Finally, there needs to be a response in the body that results from these effects. “There should be a physiological response to activation of that sensory signal,” Mattes says. For example, he notes, the perception of sweetness triggers the release of insulin throughout the body. A major challenge to getting the scientific community to accept a new taste has to do with the complexity of the research that needs to be conducted. “You have to go through a lot to prove a taste exists,” says Dawn Jackson Blatner, a registered dietitian nutritionist in Chicago and recipe creator for national brands. None of the candidates for a sixth basic taste have passed this gauntlet of criteria yet—but some of them are closer to that benchmark than others. The unlikely Scientists aren’t quite convinced about kokumi—a flavor-enhancing sensation naturally present in fermented foods, aged foods, and slow-cooked foods like stews. Discovered by Japanese scientists in the late 1980s, kokumi has been identified in an amino acid that interacts with the tongue’s calcium receptors—which some argue make it a candidate as a taste. But kokumi is a bit idiosyncratic because it’s as much a feeling or impression as a taste. Indeed because kokumi is a savory sensation that enhances other flavors, rather than providing a distinct one on its own, “it’s not clear whether it’s really taste or related to texture,” says Juyun Lim, a principal investigator at the Monell Chemical Senses Center in Philadelphia. “A lot of the taste world is problematic with words.” Also in the murky area: salty licorice. In 2023, Liman and her colleagues published a study in Nature Communications, announcing they had found evidence that taste cells respond to ammonium chloride—which is used in salty licorice (often present in Scandinavian candies)—through the same protein receptor that signals sour taste. This research was touted and widely reported on as a potential sixth taste. But even Liman wouldn’t go as far as to describe it as a basic taste, and she points out that identifying a sixth taste wasn’t the purpose of her research, either. “We were trying to understand how ammonium chloride is detected,” says Liman. “It’s a strong activator for the taste system and a common stimulus in the niche of taste research.” She adds: \"Our research raises the question of whether ammonium or sour is a fundamental taste as both involve the same receptor and neither involves a single pure population of taste cells.” The possible Then there’s the middle ground of what might be accepted as a sixth taste—if more research can back it up. A small study published in the journal Chemical Senses in 2016 suggested that starchy qualifies as a basic taste because humans can identify the taste of starch without any sensorial clues and saliva has enzymes that can break down starch. “Every culture eats carbohydrates, often with every meal—it makes sense that we should be able to taste them,” says Lim, a coauthor of the study. While these criteria may have been proven, Lim’s team is still looking for specific starch receptors in the mouth that would further support designating starchy as an official taste. The best bet Only one recent candidate has come close to meeting the criteria for acceptance. In 2015, a team of researchers at Purdue University made a case for designating fat (as in: fatty acids) as the sixth basic taste and proposed naming it oleogustus. (The moniker stems from the Latin words for fat—“oleo”—and taste—“gustus.”) Unlike the pleasant mouth-feel many people associate with the taste of fat, oleogustus tastes “like rancid oil—more like bitter than sweet or salty,” says Mattes. So far most of the criteria have been met: The fat taste has been shown to stimulate taste receptors on the tongue and it has a taste that’s distinct from the other basic tastes. In addition, researchers have shown that specific receptors are activated by medium and long chain fatty acids. And it’s been shown that exposure to fat stimulates increases in blood concentrations of triacylglycerols (a type of lipid that stores energy in the body).  Yet, even when many of the criteria have been met with scientific evidence, that doesn’t mean something is automatically accepted as an official taste. “The closer you get to changing long-held, deeply held views of how science works, the more resistance is encountered,” Mattes says. “It takes a long time to get a scientific consensus.” Despite all these hurdles, scientists continue to study taste—and debate what the next big one might be—because taste is important. For starters, “it’s about what we crave, how we cook, and how satisfied we feel after eating,” says Blatner, author of The Superfood Swap. The basic tastes also can influence people’s eating habits. For example, some researchers have linked an insensitivity to fatty acid tastes with overweight or obesity. On the other hand, research has found that sipping broth with an umami taste right before visiting a buffet can prevent people from gorging excessively.   Specific tastes are also used “ to make meals satisfying, which can help us avoid overeating,” Blatner says. The basic tastes play other roles, too. “Some tastes have an evolutionary function—for bitter, it’s [often] avoidance of plant toxins,” Liman notes. “For sweet, it’s for the caloric properties of carbohydrates.” Indeed, some tastes can warn us not to eat foods that may be harmful, notes Blatner. “It can help us figure out how fresh something is or whether something is spoiled.” Mattes’ research, for example, found that oleogustus isn’t appealing. “It’s very unpleasant,” he says. “It’s a warning signal telling you this is not the best thing to eat.”  But the question of whether oleogustus counts as a sixth taste is ultimately still a matter of debate. Perhaps we’ll have an answer before the next century.  Copyright © 1996-2015 National Geographic SocietyCopyright © 2015-2025 National Geographic Partners, LLC. All rights reserved",
        "onlytext": "Could “fatty” or “starchy” one day become accepted as the sixth basic taste alongside the likes of sweet and salty? In 1908, Japanese chemist Kikunae Ikeda was studying the active substances in kombu (seaweed), when he made a discovery that would change food science forever. Ikeda realized that a compound called glutamic acid was what gave seaweed such a distinct savory and meaty taste—nothing like the existing basic tastes of sweet, salty, sour, and bitter. He dubbed that taste umami. Yet even though people have been eating seaweed, mushrooms, miso, aged cheeses, and fermented foods for ages, it took nearly a century of scientific debate for umami to be recognized worldwide as the fifth basic taste. It wasn’t until the 1990s that it gained universal recognition—and only after extensive psychophysical, electrophysiological, and biochemical studies that confirmed it was unlike any other existing taste.\n\nIs it time to add a sixth taste to the roster? Some scientists argue that the answer is yes. They have put forward several new contenders. Should it be fatty? Starchy? Kokumi? Salty licorice? To varying degrees, researchers have made a case for all these in recent years. It isn’t that scientists have been intentionally searching for a sixth taste. Rather, “there’s a fundamental science-based search for different ways we can experience the world in terms of taste and how it’s detected,” explains neuroscientist Emily Liman, the Harold Dornsife Chair in Neuroscience at the University of Southern California who specializes in sensory biology. Breakthroughs in understanding the science of taste would allow food scientists to create more delicious foods—and could also help doctors diagnose and treat related disorders that involve a loss of your sense of taste.\n\nBut getting the scientific community to agree on a sixth taste is no easy feat. To qualify as a basic taste, any contender must be proven to be as distinct as the sweetness of berries or the bitterness of kale. First, it’s important to note that “there’s a difference between taste and flavor,” says Marisa Moore, an Atlanta-based registered dietitian in culinary nutrition. “Taste begins on the tongue when the food molecules we eat mix with saliva and activate our tastebuds, which then send signals to the brain and trigger the perception of distinct tastes.” Contrary to popular belief, the tongue does not have specific areas that detect each type of taste. Instead, basic tastes can be picked up by taste buds on various parts of the tongue, though different cells may have varying levels of sensitivity to sweet, sour, salty, bitter, and umami.\n\nWhile taste begins in the mouth, flavor is what registers in the brain when it combines the entire experience of taste, texture, temperature, and aroma. “Taste and smell do interact in the brain to put together flavor,” says Richard Mattes, distinguished professor of nutrition science at Purdue University, “but that’s different from taste.” The process of getting a new taste sensation accepted by the scientific community is complicated. “There is no widely agreed upon set of seven criteria and if you tick each box, you’ve got it,” Mattes says. “It’s just scientific consensus.” Even so, various criteria have been proposed over the years. For starters, a taste needs to have a “chemical signature”—a set of chemical stimuli that are distinct from that of other substances. Then, Mattes says, you need to have a receptor anywhere on the tongue, palate, or throat where the chemical stimulus can interact with a cell in a taste bud. That cell in the taste bud must then have a mechanism “to change the stimuli’s chemical signal to an electrical signal that is subsequently transmitted through nerves to the brain,” Mattes adds.\n\nThe electrical signal carried by taste nerves must activate regions in the brain that are associated with taste, and it needs to generate a unique taste perception in the brain that’s independent from other tastes. “It can’t overlap with another taste,” Lim says. Finally, there needs to be a response in the body that results from these effects. “There should be a physiological response to activation of that sensory signal,” Mattes says. For example, he notes, the perception of sweetness triggers the release of insulin throughout the body.\n\nA major challenge to getting the scientific community to accept a new taste has to do with the complexity of the research that needs to be conducted. “You have to go through a lot to prove a taste exists,” says Dawn Jackson Blatner, a registered dietitian nutritionist in Chicago and recipe creator for national brands. None of the candidates for a sixth basic taste have passed this gauntlet of criteria yet—but some of them are closer to that benchmark than others.\n\nThe unlikely Scientists aren’t quite convinced about kokumi—a flavor-enhancing sensation naturally present in fermented foods, aged foods, and slow-cooked foods like stews. Discovered by Japanese scientists in the late 1980s, kokumi has been identified in an amino acid that interacts with the tongue’s calcium receptors—which some argue make it a candidate as a taste. But kokumi is a bit idiosyncratic because it’s as much a feeling or impression as a taste. Indeed because kokumi is a savory sensation that enhances other flavors, rather than providing a distinct one on its own, “it’s not clear whether it’s really taste or related to texture,” says Juyun Lim, a principal investigator at the Monell Chemical Senses Center in Philadelphia. “A lot of the taste world is problematic with words.”\n\nAlso in the murky area: salty licorice. In 2023, Liman and her colleagues published a study in Nature Communications, announcing they had found evidence that taste cells respond to ammonium chloride—which is used in salty licorice (often present in Scandinavian candies)—through the same protein receptor that signals sour taste. This research was touted and widely reported on as a potential sixth taste. But even Liman wouldn’t go as far as to describe it as a basic taste, and she points out that identifying a sixth taste wasn’t the purpose of her research, either. “We were trying to understand how ammonium chloride is detected,” says Liman. “It’s a strong activator for the taste system and a common stimulus in the niche of taste research.” She adds: \"Our research raises the question of whether ammonium or sour is a fundamental taste as both involve the same receptor and neither involves a single pure population of taste cells.”\n\nThe possible Then there’s the middle ground of what might be accepted as a sixth taste—if more research can back it up. A small study published in the journal Chemical Senses in 2016 suggested that starchy qualifies as a basic taste because humans can identify the taste of starch without any sensorial clues and saliva has enzymes that can break down starch. “Every culture eats carbohydrates, often with every meal—it makes sense that we should be able to taste them,” says Lim, a coauthor of the study. While these criteria may have been proven, Lim’s team is still looking for specific starch receptors in the mouth that would further support designating starchy as an official taste.\n\nThe best bet Only one recent candidate has come close to meeting the criteria for acceptance. In 2015, a team of researchers at Purdue University made a case for designating fat (as in: fatty acids) as the sixth basic taste and proposed naming it oleogustus. (The moniker stems from the Latin words for fat—“oleo”—and taste—“gustus.”) Unlike the pleasant mouth-feel many people associate with the taste of fat, oleogustus tastes “like rancid oil—more like bitter than sweet or salty,” says Mattes. So far most of the criteria have been met: The fat taste has been shown to stimulate taste receptors on the tongue and it has a taste that’s distinct from the other basic tastes. In addition, researchers have shown that specific receptors are activated by medium and long chain fatty acids. And it’s been shown that exposure to fat stimulates increases in blood concentrations of triacylglycerols (a type of lipid that stores energy in the body).\n\nYet, even when many of the criteria have been met with scientific evidence, that doesn’t mean something is automatically accepted as an official taste. “The closer you get to changing long-held, deeply held views of how science works, the more resistance is encountered,” Mattes says. “It takes a long time to get a scientific consensus.” Despite all these hurdles, scientists continue to study taste—and debate what the next big one might be—because taste is important. For starters, “it’s about what we crave, how we cook, and how satisfied we feel after eating,” says Blatner, author of The Superfood Swap.\n\nThe basic tastes also can influence people’s eating habits. For example, some researchers have linked an insensitivity to fatty acid tastes with overweight or obesity. On the other hand, research has found that sipping broth with an umami taste right before visiting a buffet can prevent people from gorging excessively.\n\nSpecific tastes are also used “ to make meals satisfying, which can help us avoid overeating,” Blatner says. The basic tastes play other roles, too. “Some tastes have an evolutionary function—for bitter, it’s [often] avoidance of plant toxins,” Liman notes. “For sweet, it’s for the caloric properties of carbohydrates.” Indeed, some tastes can warn us not to eat foods that may be harmful, notes Blatner. “It can help us figure out how fresh something is or whether something is spoiled.” Mattes’ research, for example, found that oleogustus isn’t appealing. “It’s very unpleasant,” he says. “It’s a warning signal telling you this is not the best thing to eat.”\n\nBut the question of whether oleogustus counts as a sixth taste is ultimately still a matter of debate. Perhaps we’ll have an answer before the next century."
    }
]